\documentclass[12pt]{amsart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage[ansinew]{inputenc}
\usepackage{amssymb,amsmath,amsthm,hyperref,mathrsfs,array,graphicx,bbm,enumerate,wasysym,dsfont, tocloft}
%\usepackage{eucal}
\usepackage[all]{xy}
%\usepackage{bbm}
\usepackage{fullpage}
\usepackage{comment}
\usepackage[usenames, dvipsnames]{color}

\usepackage{titlesec}
\titleformat{\section}[display]
{\normalfont\Large\bfseries\center}{\textsc{Section}\,\thesection}{.5em}{}
\titleformat{\subsection}
{\normalfont\Large\center}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\large}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}[runin]
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titleformat{\subparagraph}[runin]
{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}
\newcommand{\sectionbreak}{\clearpage}
\setlength{\cftsecindent}{3em}

%\usepackage{subcaption}
%%%%%%%%%%%
%\textwidth 173mm \textheight 235mm \topmargin -50pt \oddsidemargin -0.45cm \evensidemargin -0.45cm

%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{exo}{Exercise}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{rem}[thm]{Remark}
\newtheorem{defn}[thm]{Definition}

\newtheorem{eg}[thm]{Example}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{con}[thm]{Conjecture}
\newtheorem{qst}[thm]{Question}

\numberwithin{equation}{section}
\DeclareMathOperator{\Tr}{Tr}
	
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\renewcommand{\H}{\mathbb H}
\newcommand{\Hh}{\mathcal H}
\newcommand{\Dd}{\mathcal D}
\newcommand{\HD}{\mathcal H_0^1([0,1])}
\newcommand{\N}{\mathbb N}
\newcommand{\Nn}{\mathcal N}
\newcommand{\D}{\mathbb D}
\newcommand{\E}{\mathbb E}
\newcommand{\F}{\mathcal F}
\newcommand{\G}{\mathcal G}
\newcommand{\CC}{\mathcal C}
\newcommand{\Po}{\mathcal P}
\newcommand{\I}[1]{\mathbf{1}_{\left \{#1\right \}}}
\renewcommand{\P}{\mathbb P}
\renewcommand{\1}{\mathbf 1}
\newcommand{\ip}[1]{\langle #1 \rangle}

\newcommand{\ol}{\overline}
\newcommand{\A}{\mathds A}
\newcommand{\ABP}[2]{\rotatebox[origin=c]{180}{$#1\A$}}
\newcommand{\AB}{{\mathpalette\ABP\relax}}
\newcommand{\B}{\mathcal B}
\newcommand{\ED}{\operatorname{EL}}
\newcommand{\M}{\operatorname{M}}

\newcommand{\eps}{\epsilon}
\newcommand{\crad}{\text{crad}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\intt}{\text{Int}}
%\newcommand{\cll}{\text{Cl}}
%\newcommand{\sgn}{\text{sign}}
\newcommand{\CLE}{\text{CLE}}
\newcommand{\ALE}{\text{ALE}}
\renewcommand{\d}{{d}}
\newcommand{\question}[1]{{\color{blue}  #1}}
%\newcommand{\comment}[1]{{\color{red}  #1}}
%\setlength\parindent{0pt}

\title{Basic probability theory 2024}
\newcommand{\juhan}[1]{{\color{ForestGreen} #1}} 
\newcommand\nmD[1]{\left\lVert#1\right\rVert_{\nabla}}
\newcommand\nmL[1]{\left\lVert#1\right\rVert_{L^2}}
\newcommand\nmH[2]{\left\lVert#1\right\rVert_{\Hh^{#2}}}
\newcommand\nm[1]{\left\lVert#1\right\rVert}


\author{Juhan Aru}
\begin{document}
\maketitle

\footnote{Version of 2024. All kinds of feedback, including smaller or bigger typos, is appreciated - juhan.aru@epfl.ch. This is a third version of the notes. In writing previous version of these notes I have consulted notes of I. Manolescu (Fribourg), Y. Velenik (Geneva), A. Eberle (Bonn) (all on their websites) and the book by R. Dalang \& D. Conus published by EPFL press.} 

\newpage
\setcounter{tocdepth}{2}
\setcounter{section}{-1}
\tableofcontents 
\makeatletter
\let\toc@pre\relax
\let\toc@post\relax
\makeatother

\section{Introduction}

This course is about probability theory: the mathematical framework for formalising our questions about random phenomena, and their mathematical study. 

When we want to describe a random phenomena in the real world, we build a mathematical model. Choosing a useful model - i.e. a model that actually tells us something about the world - involves lots of well-chosen simplifications and righteous choices. For example, to model a coin toss, we usually discard the possibility of it landing on the edge, or without further knowledge we consider the heads and tails equiprobable, although that may not be the case for example already because of different weight distributions. But choosing the model that best corresponds to the observed world is not the central topic of this course.

In this course we will more concentrate on setting up the general mathematical framework for studying random phenomena, the formulation of probabilistic models and then discuss the mathematical tools necessary and useful to study such models. Hopefully we also have some time to discuss some interesting and relevant models.

\section{Basic framework}

\noindent In this chapter we discuss some basic but important notions of probability theory:
\begin{itemize}
\item Probability space
\item Random variables
\item Independence
\end{itemize}

\subsection{Probability space}
Our first aim is to motivate the notion of a probability space or a probabilistic model. To do this let us consider two examples:

\begin{enumerate}
\item A random number with values in $\{1, 2, \dots, 12\}$ e.g. something that comes from a lottery. 
\item Describing the weather in Lausanne the day after.
\end{enumerate}
In describing these two random phenomena we will still use everyday vocabulary / intuitions. Thereafter we will give the mathematical definitions that will fix the vocabulary for the rest of the course.
~\\
\paragraph{(1) Random number.} To describe a random number mathematically, we basically need three inputs: 
\begin{itemize}
    \item The set of all possible outcomes: in this case $\Omega = \{1, 2, 3, \dots, 12\}$
    \item The collection of yes / no questions that we can answer about the actual outcome, i.e. this random number. For example:
    \begin{itemize}
    \item Is this number equal to $3$?
    \item Is this number even?
    \item Is this number smaller than $4$?
    \end{itemize}
    To each of these questions we put in correspondence the subset of outcomes that corresponds to the answer yes: $\{3\}$, $\{2,4,6,8,10,12\}$ or $\{1,2,3\}$ respectively. We call each such subset an event.
    \item Finally, to each event $E \subseteq \Omega$ we want to assign a numerical value $\P(E) \in [0,1]$ that we call the probability. This should correspond to the fraction of times an event happens if the random number is given to us many times, e.g. if the lottery is played many times. \footnote{In fact, one uses probabilistic models also to model phenomena that only happens once. In that case probability measures somehow our degree of belief.}
\end{itemize}

Here the set of possible outcomes was easy and directly given by the problem. Also it is natural to assume that each subset $E \subseteq \Omega$ is an event - or in other words that for each $E$ we can ask the question: is the number in $E$? This means that the we can take the collection of events to correspond to all subsets of $\Omega$. 

Determining the probability really depends on what we want to model - e.g. if we are trying to model the lottery, we may assume that all numbers are equally likely and then we rediscover the model from high-school: we set $\P(E) = |E|/|\Omega|$. However, if we wanted to describe the sum of two dice, we would need to choose the numbers $\P(E)$ very differently! \footnote{See Exercise sheet 1.}

Now, if we want our model to correspond to the intuitive notion of probability and to predict the fraction of repeated experiments, then these choices are not quite free - we need to add some constraints. E.g. we cannot put in an arbitrary function $\P$: indeed, if we have two events $E_1 \subseteq E_2$ then we should have $\P(E_1) \leq \P(E_2)$ as every time $E_1$ happens, also $E_2$ happens. We should also have $\P(\Omega) = 1$ as something always happens and $\P(E \cup F) = \P(E) + \P(F)$ if $E$ and $F$ are disjoint (why?). Of course not all these constraints are distinct - some might imply others and when giving the definition of a probability space below we will purify and choose only some conditions that will then mathematically imply all the others. 
\\
\paragraph{(2) Weather in Lausanne the day after.} We would again want to make the three decisions, but here the task is already harder at the very first step. What should be the state space? A natural state space could probably be all possible microscopic states of the atmosphere up to 20km of height over Lausanne...but here we of course have many arbitrary choices - why 20 km, how wide should we look over Leman etc? And in any case, any natural state would be impossibly complicated!

Luckily, we do not actually need to worry about it - we only have to assign probabilities to all the events in the collection of events. And we have some freedom in choosing this collection events - it could  be determined by our possibility to measure the states, e.g. we are able to measure the temperature up to some precision, or the density of $CO_2$ or water molecules to some precision and this determines some subsets of the state space. %We could of course just choose our state space to be exactly so small that we can distinguish each state with our measurement devices, but allowing the state space to be larger and just restricting the set of yes and no questions / events has an advantage - if we get better at observing and measuring we can keep the same sample space and just change the collection of events. In particular, if we have fixed $\P$, we would need to only extend it and not rebuild the whole model.

However, as with the probability function, also for the collection of events there are some natural consistency conditions: we would assume that if one can observe if event $E$ happened, we should be also able to measure if its complement $E^c$ happened. Or if we are able to say if $E$ happened or if $F$ happened, we should be able to say if one of the two happened - i.e. $E \cup F$ should also be an event. And in fact it comes out that this is all we need!

Naturally, setting up probabilities for this model is also horribly complicated - there are no natural symmetry assumptions like the one we used for the uniform distribution. Also, even the best physicist in the world will not be able to describe the natural probability distribution of all microscopic states of the atmosphere, especially as it will heavily depend on what is happening just before! Thus, our only choice basically is to try to somehow use the combination of our knowledge about atmospheric processes together with our observations from history to set up some estimates for the model; and then naturally we will try to improve it with every next day. Luckily, this difficult task is not up to us but rather the office of meteo and the statisticians! 

\begin{rem}
Finally, before giving the mathematical definitions, let us stress again that all three components of the model - the sample space, the set of events and their probabilities - are inputs that we choose to build our model. When trying to model a real world phenomena we usually make simplifications for each of these choices. For example, for the coin toss we use only two outcomes: heads and tails, although theoretically edge is also possible. Also, we usually set probabilities to be a half, although that is not exactly true either.
\end{rem}

\subsection{Mathematical definition of a probability space}
We are now ready to use our mathematical filter and give a mathematical definition of a probability space. In fact, we first use the mathematical purifier to come up with a definition in the restricted setting where $\Omega$ is a finite set, and then generalize it further.

Indeed, the discussions above lead us directly to:
\begin{defn}[Elementary probability space, Kolmogorov 1933 ]
An elementary probability space is a triple $(\Omega, \F, \P)$, where
\begin{itemize}
	\item $\Omega$ is a finite set, called the state or sample space or the universe.
	\item $\F$ is a set of subsets of $\Omega$, satisfying:
	\begin{itemize}
		\item $\emptyset\in \F$;
		\item if $A \in \F$, then also $A^c \in \F$;  
		\item If $A_1, A_2, \in \F$, then also $A_1 \cup A_2 \in \F$.
	\end{itemize}
$\F$ is called the collection of events and any $A \in \F$ is called an event.
	\item And finally, we have a function $\P : \F \to [0, 1]$ satisfying $\P(\Omega) = 1$ and additivity for disjoint sets: if $A_1, A_2 \in \F$ are pairwise disjoint, then $$\P(A_1 \cup A_2) = \P(A_1) + \P(A_2).$$ This function $\P$ is called the probability
\end{itemize} 
\end{defn}
Notice that some properties discussed above, like the fact that for events $E_1 \subseteq E_2$, we have $\P(E_1) \leq \P(E_2)$, follow directly from the definition.\footnote{See Exercise sheet 1.}

Now, most phenomena in the real world can be described by finite sets just because we are able to measure things only to a finite level of precision. However, like the notion of a continuous or differentiable function helps to simplify our mathematical descriptions of reality and thus improve our understanding, continuous probability spaces also make the mathematical descriptions neater, simpler and thereby also make it easier to understand and study the underlying random phenomena. 

Some natural examples where infinite sample spaces come in: an uniform point on a line segment e.g. stemming from breaking a stick into several pieces; the position on the street where the first raindrop of the day falls; or the space of all infinite sequences of coin tosses. In all these cases the mathematically natural state space is even uncountable. Countably infinite state spaces can also come up: for example if we want to model the first moment that a repeated coin toss comes up heads, the value might be $1, 2, 3$ or with very very small probability also $10^{10}$, so a natural state space would contain all natural numbers.

So let us state the general definition:
\begin{defn}[Probability space, Kolmogorov 1933 ]
A probability space is a triple $(\Omega, \F, \P)$, where
\begin{itemize}
	\item $\Omega$ is a set, called the state or sample space or the universe.
	\item $\F$ is a set of subsets of $\Omega$, satisfying:
	\begin{itemize}
		\item $\emptyset\in \F$;
		\item if $A \in \F$, then also $A^c \in \F$;  
		\item If $A_1, A_2, \dots \in \F$, then also $\bigcup_{n \geq 1} A_n \in \F$.
	\end{itemize}
$\F$ is called the collection of events or a $\sigma$-algebra and any $A \in \F$ is called an event.
	\item And finally, we have a function $\P : \F \to [0, 1]$ satisfying $\P(\Omega) = 1$ and additivity for disjoint sets: if $A_1, A_2, \dots \in \F$ are pairwise disjoint, $$\P(\bigcup_{n \geq 1}A_n) = \sum_{n \geq 1}\P(A_n).$$ This function $\P$ is called the probability
\end{itemize} 
\end{defn}
Notice the only differences are 1) we do not assume $\Omega$ to be finite 2) we assume that the set of events is stable under countable unions 3) we assume also the additivity of the probability under countable unions. 

\begin{exo}
Show that each elementary probability space is a probability space.
\end{exo}

In fact probability spaces are an example of a general notion of measure spaces - probability spaces are just measure spaces with total mass equal to $1$.

\begin{defn}[Measure space, Borel 1898, Lebesgue 1901-1903]
A measure space is a triple $(\Omega, \F, \mu)$, where
\begin{itemize}
	\item $\Omega$ is a set, called the sample space or the universe.
	\item $\F$ is a set of subsets of $\Omega$, satisfying:
	\begin{itemize}
		\item $\emptyset\in \F$;
		\item if $A \in \F$, then also $A^c \in \F$;  
		\item If $A_1, A_2, \dots \in \F$, then also $\bigcup_{n \geq 1} A_n \in \F$.
	\end{itemize}
$\F$ is called a $\sigma$-algebra and any $A \in \F$ is called a measurable set.
	\item And finally, we have a function $\mu : \F \to [0, \infty]$ satisfying $\mu(\emptyset) = 0$ and countable additivity for disjoint sets: if $A_1, A_2, \dots \in \F$ are pairwise disjoint, $$\mu(\bigcup_{n \geq 1}A_n) = \sum_{n \geq 1}\mu(A_n).$$ This function $\mu$ is called a measure. If $\mu(\Omega)  < \infty$, we call $\mu$ a finite measure.
\end{itemize} 
\end{defn}
\noindent Geometrically we interpret:
\begin{itemize}
    \item $\Omega$ as our space of points
    \item $\F$ as the collection of subsets for which our notion of volume can be defined
    \item $\mu$ our notion of volume: it gives each measurable set its volume.
\end{itemize}
It is important to make this link to measure theory as many properties of probability spaces directly come from there. Yet it is also good to keep in mind that probability theory is not just measure theory - as M. Kac has put it well, 'Probability is measure theory with a soul' and we adhere to this philosophical remark.

\begin{rem}
You should compare the definition of a probability space / measure space with the definition of a topological space: there also we use a collection of subsets with certain properties to attach structure to the set. A question you should ask is: why do we use exactly countable unions and intersections for the events, and not finite or arbitrary?
\end{rem}

\subsection{Some basic properties of probability spaces}

We start by a few small remarks about the definition of a probability space:

\begin{rem}
It is worth considering why ask for countable stability of the $\sigma$-algebra or countable additivity of the probability measure. Whereas this is more a meta-mathematical question, it is good to keep it in mind throughout the course. Let us here just offer two simple observations.

First, countable sums naturally come up when we take limits of finite sums. In fact, countable additivity can be seen to be equivalent to certain form of continuity for the probability measure (see below).

Second, allowing for arbitrary unions leads easily to  power-sets, and sums of uncountably many positive terms cannot be finite (see the exercise sheet).
\end{rem}

\begin{exo}
Show that the countable additivity in the axioms of a probability space can be replaced with finite additivity plus the following statement: for any decreasing sequence of events $E_1 \supseteq E_2 \supseteq E_3 \dots$ with $\cap_{i\geq 1} E_i=\emptyset$ we have that $\P(\cap_{i=1}^n E_i) \to 0$ as $n \to \infty$.

\noindent $\star$ Does this hold in a general measure space?
\end{exo}

Also we would like to remark another setting that explains well the usefulness of $\sigma$-algebras:

\begin{rem}
Often in real life we only obtain information about the world step by step, and thus if we want to keep on working on the same probability space (which is helpful as then $\P$ will only need to be extended not redefined), we can consider a sequence of $\sigma$-algebras $\F_1 \subseteq \F_2 \subseteq \F_3 \dots$ called a filtration - each day we can ask some more yes/no questions because we already for example know what happened on the previous day and maybe also have learned something new. All possible information is contained in the power set $\Po(\Omega)$.
\end{rem}

Probability spaces are usually classified in two types:

\begin{defn}[Discrete and continuous probability spaces]
Probability spaces $(\Omega, \F, \P)$ with a countable sample space $\Omega$ are called discrete probability spaces and those with an uncountable sample space are called continuous probability spaces.
\end{defn}

In this course we will mainly work with discrete probability spaces, as they are technically easier to deal with. However, continuous probability spaces come up naturally and we won't be able to fully avoid them either.

Their technical difference can be summoned in the following proposition, whose non-examinable proof will be left for enthusiasts.

\begin{prop}\label{prop:disatom}
Let $\Omega$ be countable and $\F$ a $\sigma-$algebra on $\Omega$. Then one can find disjoint events $E_1, E_2, \dots \in \F$ such that for every $E \in \F$ we can express $E = \cup_{i \in I_E} E_i$.
\end{prop}

Essentially, this says that for every discrete probability space it suffices to determine $\P(E_i)$ for a countable collection of disjoint sets $E_i$, and thereafter for every other set $E$ we can use countable additivity to extend $\P$. Notice that this means it is first easy to check whether a given $\P$ satisfies all the axioms and even more importantly it is easy to check when two probability measures are equal.

For continuous probability spaces this does not necessarily hold - the useful $\sigma$-algebras are usually more complicated. To examplify why one doesn't want to necessarily use the power-set consider the following proposition, whose proof is in the appendix and relies on the axiom of choice: 

\begin{prop}\label{noshiftonS}
There is no probability measure $\P$ on $([0,1], \Po([0,1]))$ that is invariant under shifts, i.e. such that for any $A \in \Po([0,1]), \alpha \in [0, 1)$, we have that $\P(A+\alpha \mod 1) = \P(A)$, where here we denote $A+ \alpha \mod 1 := \{a+\alpha \mod 1: a \in A\}$, the set obtained by shifting $A$ by $\alpha$, modulo $1$.
\end{prop} 

In fact, it comes out that the only way to remedy this situation is to make the relevant $\sigma-$algebra smaller. We would still want to be able to answer yes or no to questions like: is my random number equal to $\{x\}$ or is it in an interval $(a,b)$? Thanks to the fact that we have only countable additivity, this does not imply that our $\sigma$-algebra would need to be the power-set. And thanks to the properties of the $\sigma-$algebras, we can always construct at least some $\sigma-$algebra containing all our favourite sets - see the exercise sheet.

Let us now state some immediate consequences of the definitions about the $\sigma-$algebras and the probability measures:

\begin{lemma}[Stability of the $\sigma-algebra$]\label{lem:meassets}
	Consider a set $\Omega$ with a $\sigma$-algebra $\F$. 
	\begin{enumerate}
		\item If $A_1, A_2, \dots, \in \F$, then also $\bigcap_{n \geq 1} A_n \in \F$. 
		\item Then also $\Omega \in \F$ and if $A, B \in \F$, then also $A \setminus B \in \F$.
		\item For any $n \geq 1$, if $A_1, \dots, A_n \in \F$, then also $A_1 \cup \dots \cup A_n \in \F$ and $A_1 \cap \dots \cap A_n \in \F$.
	\end{enumerate}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:meassets}]
	By de Morgan's laws for any sets $(A_i)_{i \in I}$, we have that 
	$$\bigcap_{i \in I} A_i = (\bigcup_{i \in I} A_i^c)^c.$$
	Property (1) follows from this, as if $A_1, A_2, \dots \in \F$, then by the definition of a $\sigma$-algebra also $A_1^c, A_2^c, \dots \in \F$ and hence
	$$(\bigcup_{i \geq 1} A_i^c)^c \in \F.$$
	For (3), again by de Morgan laws, it suffices to show that $A_1 \cup \dots \cup A_n \in \F$. But this follows from the definition of a $\sigma$-algebra, as 
	$A_1 \cup \dots \cup A_n = \bigcup_{i \geq 1} A_i$ with $A_k = \emptyset$ for $k \geq n+1$.

 Point (2) is left as an exercise.
\end{proof}

In a similar vein, the basic conditions on the measure give rise to several natural properties:

\begin{prop}[Basic properties of a  probability measure]\label{prop:propmeas}
	Consider a probability space $(\Omega, \F, \P)$. Let $A_1, A_2, \dots \in \F$. Then
	\begin{enumerate}
 		\item For any $A \in \F$, we have that $\P(A^c) = 1 - \P(A)$.
		\item For any $n \geq 1$, and $A_1, \dots, A_n$ disjoint, we have finite additivity $$\P(A_1) + \dots + \P(A_n) = \P(A_1 \cup \dots \cup A_n).$$
		In particular if $A_1 \subseteq A_2$ then $\P(A_1) \leq \P(A_2)$.
		\item If for all $n \geq 1$, we have $A_n \subseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\P(A_n) \to \P(\bigcup_{k \geq 1} A_k)$. 
		\item We have countable subadditivity (also called the union bound): $\P(\bigcup_{n \geq 1}A_n) \leq \sum_{n \geq 1} \P(A_n)$.
		\item If for all $n \geq 1$, we have $A_n \supseteq  A_{n+1}$, then as $n \to \infty$, it holds that $\P(A_n) \to \P(\bigcap_{k \geq 1} A_k)$. 
	\end{enumerate}
	
\end{prop}
\begin{proof}
Properties (1), (4) and second part of (2) were included in the Exercise sheet 1. The first part of property (2) follows like in the lemma above by taking $A_{n+1} = A_{n+2} = \dots = \emptyset$ and using countable additivity.

So let us prove property (3): Write $B_1 = A_1$ and for $n \geq 2$, $B_n = A_n\setminus A_{n-1}$. Then $B_n$ are disjoint, $\bigcup_{n = 1}^N B_n = A_N$ and $\bigcup_{n \geq 1} B_n = \bigcup_{n \geq 1} A_n$. 

Thus by countable additivity 
$$\P(\bigcup_{i \geq 1} A_i ) = \P(\bigcup_{i \geq 1} B_i ) = \sum_{i \geq 1}\P(B_i)$$
But $\P$ is non-negative, so
$$\sum_{i \geq 1}\P(B_i) = \lim_{n \to \infty}\sum_{i = 1}^n \P(B_i)$$
By countable additivity again
$$\sum_{i = 1}^n \P(B_i) = \P(\bigcup_{i = 1}^n B_n) = \P(A_n)$$ 
and (3) follows.

\end{proof}

\subsection{Random variables}

In fact when studying a random phenomena we certainly don't want to restrict ourselves to yes and no questions. For example, in our model of a random number among $\{1, 2, \dots, 12\}$ the natural question is not 'Is this number equal to $5$?' but rather 'What number is it?'. 
Similarly in our example of discussing the weather, it is more natural to ask 'What is the temperature?', 'How much rain will there be in the afternoon?'?

Such numerical observations about our random phenomena will be formalised under the name of random variables. In essence they give a number for each state and thus as such are just functions $X:\Omega \to \R$ from the state-space to real numbers. However, we may not want to include all such functions for consistency reasons. Indeed, we want to be able to ask yes / no questions about our random numbers, e.g. Is the random number equal to $3$? Is the temperature more than $18$? But again the answer yes / no corresponds to certain subsets of states in the universe and as such should be events in our model. Thus there is a link between the collection of events, and the collection of functions that can act as random variables. Let us without further give the general definition:

\begin{defn}[Random variable]
Let $(\Omega, \F, \P)$ be a probability space. We call a function $X:\Omega \to \R$ a random variable if for every interval $(a,b)$ the set $X^{-1}((a,b)) := \{\omega \in \Omega: X(\omega) \in (a,b)\}$ is an event on the original probabiliuty space, i.e. belongs to $\F$.
\end{defn}

There is a simplification in the case of discrete probability spaces:

\begin{lemma}[Random variables on discrete probability spaces]
Let $(\Omega, \F, \P)$ be a discrete probability space. Then $X: \Omega \to \R$ is a random variable if and only if for every $y \in \R$ we have that $X^{-1}(\{y\}) \in \F$.
\end{lemma}

\begin{proof}
This can be verified carefully from the definitions and will be on the exercise sheet.
\end{proof}

For the structurally minded the definition of a random variable might look somewhat arbitrary. And indeed, I have been hiding one piece of information - the natural collection of events on $\R$ that we alluded to a little bit already in the previous subsection. We will directly state it on $\R^n$.

\begin{defn}[Borel $\sigma$-algebra]
The smallest $\sigma$-algebra on $\R^n$ that contains all open boxes of the form $(a_1, b_1) \times \dots \times (a_n, b_n)$ is called the Borel $\sigma$-algebra. We denote it by $\F_B$
\end{defn}

\begin{rem}
In fact this definition is even more general: given any topological space $(X, \tau)$, the smallest $\sigma$-algebra containing all open sets is called the Borel $\sigma$-algebra. You will see on the exercise sheet that this more general definition reduces to the previous one in the case of $\R^n$ with its Euclidean topology.
\end{rem}

Based on this an equivalent, possibly more structural definition of a random variable is as follows: a function $X: \Omega \to \R$ is a random variable if the preimage of every set in the Borel $\sigma-$algebra under $X$ is an event. \footnote{In measure theory such functions would be called measurable functions from $(\Omega, \F)$ to $(\R, \F_B)$; notice the similarity with the definition of continuous functions in your topology course.}

An important notion that comes with random variables is its law:

\begin{lemma}[The law of a random variable]
Let $(\Omega, \F, \P)$ be a probability space and $X:\Omega \to \R$ a random variable. 

Then there is a probability measure $\P_X$ induced on $(\R, \F_B)$ by defining $\P_X(F) := \P(X^{-1}(F)$ for every $F \in \F_B$. This probability measure $\P_X$ is called the law (or distribution) of a random variable $X$.
\end{lemma}

This is a lemma and not a definition as it needs to be proved that indeed $\P_X$ is a probability measure on $(\R, \F_B)$.

\begin{proof}[Proof of Lemma]
We need to verify the axioms on a probability measure for a probability space:
\begin{itemize}
\item We have $\P_X(\R) = \P(\Omega) = 1$
\item Similarly $\P_X(F) = \P(X^{-1}(F)) \in [0,1]$ for all $F \in \F_B$
\item Finally it remains to check countable additivity: let $F_1, F_2, \dots$ be disjoint sets in $\F_B$. Then 
$$\P_X(\bigcup_{i \geq 1}F_i) = \P(X^{-1}(\bigcup_{i \geq 1}F_i)) = \P(\bigcup_{i \geq 1}X^{-1}(F_i)) = \sum_{i \geq 1}\P(X^{-1}(F_i)) = \sum_{i \geq 1}\P_X(F_i).$$
Here we used the definition in the first and last equality, the properties of preimages in the second equality and the fact that $X^{-1}(F_i)$ are disjoint together with countable additivity in the third equality.
\end{itemize}
\end{proof}

In words we showed that each random variable $X$ induces a probability measure on the real numbers by just forgetting about the whole context and just concentrating on the number we see. For example in the case of weather in Lausanne, the temperature will give us a random variable and by just looking at its value and nothing else we have just a random real-valued number. Or more simply, if if we throw two fair coins and count the nunmber of heads, their sum will be a random variable that takes values in the set $\{0,1,2\}$.  Thus the notion of the law of random variable gives us a way to compare random quantities arising in very different contexts.

\begin{defn}[Equality in law]
Let $X, Y$ be two random variables defined possibly on different probability spaces. We say that $X$ and $Y$ are equal in law or equal in distribution, denoted $X \sim Y$ if for every $E \in \F_B$ we have that $\P_X(E) = \P_Y(E)$.
\end{defn}

We stress that when looking at the law of random variable the context gets forgotten - we only concentrate on the numerical value and the initial probability space $(\Omega, \F, \P)$ only helps to determine $\P_X$ but plays no role thereafter. This means that we can nicely connect different random phenomena between each other. For example the indicator functions of all events that have probability $p$, independently on which probability space they have been defined, have  the same law. Or more concretely, for example
the following random variables have the same law:
\begin{itemize}
\item Number of heads in two independent tosses
\item Number of prime factors when we choose uniformly a number among $\{1,2,3,4\}$.
\end{itemize}

In some sense a large part of this course will be about studying and describing probability laws of random variables. %Let us here see how they help us describe a slightly more complicated probabilistic model than coin tosses.

\begin{comment}
\subsubsection{The model of uniform random graphs}


First, recall the notion of a graph:

\begin{defn}[Graph]
	Let $n \in \N$. A simple graph is a set of vertices $V = \{v_1, \dots, v_n\}$ together with an edge set $E$, that is some subset of $\{\{v_i, v_j\} : (v_i, v_j) \in V \times V, v_i \neq v_j\}$. 
 \end{defn}

You can imagine the graph as drawing all the $n$ points $v_1, \dots, v_n$ on the plane and then drawing a line between $v_i$ and $v_j$ to say they are connected if and only if $\{v_i, v_j\} \in E$. You can think of it also as of a social network: each vertex corresponds to an individual, an each between two vertices denotes that these people know each other.

Maybe the easiest model of a random network or graph is the one where you consider each graph with the same vertex set as equally likely. Such a graph is called a uniform random graph

\begin{eg}[Uniform random graph]\label{eg:rgraph}
	The probability model for a uniform random graph is defined as follows: we let $\Omega$ be the set of all simple graphs $G$ with vertex set $V$, set $\F = \Po(\Omega)$ and define $\P$ such that $\P(\{G\}) = |\Omega|^{-1}$ for each graph $G \in \Omega$.
	\end{eg} 

We would like to describe how this random graph that describes some sort of random network looks like. For example, we could ask
\begin{itemize}
    \item Is the graph connected? 
    \item Are vertices $x$ and $y$ connected? 
    \item How many neighbours does a vertex typically have?
    \item What is the shortest distance between two vertices?
    \item How many edges are there in total?
    \item How many different connected components are there?
\end{itemize}

All these questions correspond to  either events or random variables and have nice interpretations. For example in social networks the number of neighbours corresponds to the number of friends of someone, the shortest distance to the shortest communication path between two people, the number of edges to the number of friendships in total etc...

\begin{exo}[Uniform random graphs]
Consider the probability model for uniform random graphs.
	\begin{itemize}
		\item What is the size of the sample space $\Omega$, i.e. how many simple graphs are there on $n$ vertices?
		\item What is the probability that there are exactly $3$ edges in the graph?
        \item What is the probability law of the number of neighbours of a given vertex $v$?
        
		\item Show that the probability of the event that there is an isolated vertex, i.e. a vertex that is not connected to anyone else, goes to zero as $n \to \infty$.
	\end{itemize}
\end{exo}

\end{comment}
\section{Conditional probability and independence}

In general, if we learn something new about our random phenomena, this knowledge influences and often changes our predictions for the rest of the model.

\begin{itemize}
    \item For example in the case of a uniform random number between $1$ and $12$, if someone tells you that this number is even, then the probability of seeing $1$ will suddenly be $0$, but the probability of seeing $2$ will rise from $1/12$ to $1/6$.
    \item In the case of weather in Lausanne, if someone tells us that it rains the whole day, then it is less likely to also be above $35$ degrees.
\end{itemize} 

The aim of this section is to set up the vocabulary to talk about how the knowledge about some event or random variable influences the probabilities we should assign to other events. This leads us to talk about conditional probabilities and to discuss the case where events do not influence each other, giving rise to an important notion of probability theory called independence. 

\subsection{Conditional probability}

We have already considered (in the course and on the example sheets) many unpredictable situations where several events naturally occur either at the same time or consecutively: a sequence of coin tosses or successive steps in a random walk, or different links or edges in a random graph. In all these cases, the fact that one event has happened could easily influence the others. For example, if you want to model the financial markets tomorrow, it seems rather advisable to take into account what happened today. To talk about the change of probabilities when we have observed something, we introduce the notion of conditional probability:

\begin{defn}[Conditional probability]
	Let $(\Omega, \F, \P)$ be a probability space and $E \in \F$ with $\P(E) > 0$. 
	Then for any $F \in \F$, we define the conditional probability of the event $F$ given $E$ (i.e. given that the event $E$ happens), by
	\[\P(F|E) := \frac{\P(E \cap F)}{\P(E)}.\]
\end{defn}

Recall that $E \cap F$ is the event that both $E$ and $F$ happen. Hence, as the denominator is always given by $\P(E)$, the conditional probability given $E$ is proportional to $\P(E \cap F)$ for any event $F$. Here is the justification for dividing by $\P(E)$:

\begin{lemma}
	Let $(\Omega, \F, \P)$ be a probability space and $E \in \F$ with $\P(E) > 0$. 
	Then $P(\cdot |E)$ defines a probability measure on $(\Omega, \F)$, called the conditional probability measure given $E$.
\end{lemma}

\begin{proof}
	First, notice that $\P$ is indeed defined for every $F \in \F$. Next, $\P(\emptyset|E) = \P(\emptyset)/\P(E) = 0$ and $\P(\Omega|E) = \P(E)/\P(E) = 1$. So it remains to check countable additivity. 
	
	So let $F_1, F_2, \dots \F$ be disjoint.
	Then also $E \cap F_1, E \cap F_2, \dots $ are disjoint. Hence
	$$\P(\bigcup_{ i\geq 1} F_i|E) = \frac{\P((\bigcup_{ i\geq 1} F_i) \cap E)}{\P(E)} =\frac{\P(\bigcup_{ i\geq 1} (F_i \cap E))}{\P(E)} = \sum_{i \geq 1}\frac{\P(F_i \cap E)}{\P(E)} = \sum_{i \geq 1}\P(F_i|E),$$
	and countable additivity follows.
	
\end{proof}

It should be remarked that conditional probability of an event might sometimes be similar to the initial probability (we will see more about this very soon), but it might also be drastically different. A somewhat silly but instructive example is the following: 
\begin{itemize}
    \item Conditional probability of the event $E^c$, conditioned on $E$ is always zero, no matter what the original probability was;
    \item similarly the conditional probability of $E$, conditioned on $E$ is always $1$. 
\end{itemize} 

Or for a more senseful exercise consider the following:

\begin{exo}[Random walk and conditional probabilities]
Consider the simple random walk of length $n$. 
\begin{itemize}
    \item What is the probability that the walk ends up at the point $n$ at time $n$? Now, suppose that the first step was $-1$. What is the probability that the walk ends up at the point $n$ at time $n$ now?
    \item Suppose that $n$ is even. What is the probability that the walk ends up at the point $0$ at time $n$? Now, suppose that the first step was $-1$. What is the probability that the walk ends up at the point $0$ at time $n$ now?
\end{itemize}
\end{exo}
%%%%%%%%%%%%%%%%%%%%%%% THIS IS IN THE EXERCISES, BUT OUT WITH IT FROM NOTES!
%\begin{exo}
%	The French, Swiss and German decide to elect the greatest mathematician of all time. The French propose Poincar\'e, the Swiss propose Euler and the German Gauss. Each country has one vote, and the candidate with most votes wins. In case of equal votes, the winner is chosen uniformly randomly.
%	Now Mathematico, an organization that predicts elections, forecasts that 
%	\begin{itemize}
%		\item the French will give their vote with probability $1/2$ to Poincar\'e and equally with probability $1/4$ to Euler or Gauss;
%		\item the Swiss will give their vote with probability $1/2$ to Euler and equally with probability $1/4$ to Poincar\'e or Gauss;
%		\item the German will give their vote with probability $1/2$ to Gauss and equally with probability $1/4$ to Poincar\'e or Euler.
%	\end{itemize}
%	Moreover, Mathematico thinks that none of the countries cares about the opinion of the others.
	
%	Build a probabilistic model to be able to predict the winner. What assumptions are you making? In this model, what is the probability that Euler wins? What is the probability that Euler gets at least $2$ votes?
%	Now, surprisingly it comes out that the Swiss have elected Gauss instead of Euler. How would you now estimate the probability that Euler still wins the election?
%\end{exo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One also has to be very careful about the exact conditioning, as two similarly sounding conditionings can induce very different conditional probabilities. In general, we need to know something extra about the relation of two events to know how the probability of one changes when conditioned on the other.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO BE WORKED
%\begin{exo}[Uniform random graphs and conditional probabilities]
%Consider the uniform random graph on $n \geq 3$ vertices as defined above.
%\begin{itemize}
 %   \item What is the probability that the graph is connected given each vertex is connected to exactly one edge?
  %  \item What is the probability that the graph is connected given that each vertex but one is connected to exactly one edge?
%\end{itemize}
%\end{exo}

%%%%%%%%%%%%%%%%%%%%%%% THIS IS IN THE EXERCISES, BUT OUT WITH IT FROM NOTES!
%\begin{exo}
%	Roger Federer is now $70$ years old, but is still playing. He is a bit tired of running and has limited his strategy in his serve game: he either serves an ace with probability $1/2$ and obtains a point, or with the same probability makes a double fault and his opponent gains a point.
%	The game has also been simplified and the player who first obtains 3 points wins. 
%	Build a probabilistic model (or several) to answer the following questions and answer them:
%	\begin{itemize}
%		\item What is the probability that Roger wins his serve game?
%		\item What is the probability that Roger won his serve game, given that he hit at least two aces?
%		\item What is the probability that he will win his serve game, given that he started by hitting two aces?
%	\end{itemize}
%\end{exo}
There are some cases where these relations and thus conditional probabilities are easy:
\begin{itemize}
\item When $E \subseteq F$, then the conditional probability of $F$ given $E$ is just $1$.
\item When $F \subseteq E^c$, then the conditional probability of $F$ given $E$ is just $0$. 
\item The third case is when $F$ and $E$ are so called independent: in that case $\P(F|E) = \P(E)$ basically by definition (we will come back to that).
\end{itemize}
In general, there are not many tools to calculate conditional probabilities, but there is one very useful tool called the Bayes' formula or the Bayes' rule:

\subsubsection{Bayes' rule}

\begin{prop}[Bayes' rule]
	Let $(\Omega, \F, \P)$ be a probability space and $E, F$ two events of positive probability. Then
	\[\P(E|F) = \frac{\P(F|E)\P(E)}{\P(F)}\]
\end{prop}

%We will discuss this formula, conditional probabilities and independence at a greater length next week.
It's not only that the statement looks innocent, but also the proof is a one-liner - by definition of conditional probability, we can write
\[ \P(E|F)\P(F) = \P(E\cap F) = \P(F|E)\P(E).\]
Still, it is a very nice observation that allows us not only to calculate, but also is behind the framework of Bayesian statistics / Bayesian thinking about probability.

Let us here analyse a simple example. 

\begin{eg}
Consider the situation with three different coins: one has heads on both sides, one has tails on both sides, and one is a fair coin. Now someone picked using some procedure one of the three types of coins, told you that she tossed a coin and heads came up. Which coin did she toss?

The relevant probability space that contains the three coins and three tosses is as follows. First, the state space is pairs the product space $\{C_h, C_t, C_f\} \times \{H, T\}$ - the first coordinate describes the type of the coin, the second the result of the toss. As a $\sigma-$algebra we take the whole $\sigma-$algebra as we can ask both about what came up on top, and then which coin it was.

We know that to define $\P$ on a finite set with the power-set it suffices to define $\P$ for every element of the state-space. From the assumptions $\P(\{C_h, T\}) = \P(\{C_t, H\}) = 0$ and $\P(\{C_f,T\}) = \P(\{C_f, H\})$. If we further set $p_f = \P(\{coin = C_f\})$, $p_h = \P(\{coin = C_h\})$, $p_t = \P(\{coin = C_t\})$ it also has to hold that $p_f + p_t + p_h = 1$, leaving two free parameters altogether.

Let us now calculate the probabilities that we were interested in. Clearly, 
$$\P(\{coin = C_t\}|\{toss = H\}) = 0$$
as the coin with two tails sides could not have produced heads. For the other combinations it is easiest to use Bayes' formula to calculate 
$$\P(\{coin = C_h\}|\{toss = H\}) = \frac{\P(\{toss = H\}|\{coin = C_h\})\P(\{coin =C_h\})}{\P(\{toss = H\})} = \frac{\P(\{coin =C_h\})}{\P(\{toss = H\})}$$
and
$$\P(\{coin = C_f\}|\{toss = H\}) = \frac{\P(\{toss = H\}|\{coin = C_f\})\P(\{coin =C_f\})}{\P(\{toss = H\})} = \frac{\P(\{coin =C_f\})}{2\P(\{toss = H\})}.$$
Thus we see that 
$$\frac{\P(\{coin = C_h\}|\{toss = H\})}{\P(\{coin = C_f\}|\{toss = H\})} = \frac{2\P(\{coin = C_h\})}{\P(\{coin = C_f\})} = 2p_h/p_f$$
and given that 
$$\P(\{coin = C_h\}|\{toss = H\})+\P(\{coin = C_f\}|\{toss = H\}) = 1$$
we conclude our estimates $$\P(\{coin = C_f\}|\{toss = H\}) = \frac{p_f}{p_f + 2 p_h}$$ and $$\P(\{coin = C_h\}|\{toss = H\}) = \frac{2p_h}{p_f + 2 p_h}.$$
What can we conclude? The first thing is maybe that without having any knowledge of how likely each coin was to begin with, we cannot say much about the final answer, as it contains that information! What we assume about the initial probability of each coin matters a lot: if we estimate that the coin with two heads was very unlikely compared to the fair coin, say $p_h = 0.000001p_f$, then after seeing heads our estimate gives $\P(\{coin = C_f\}|\{toss = H\}) = 0.999999$. If however we have no reason to believe that any one coin was more likely to be taken than any other, for example because the person tossing the coin just picked it randomly among the three possibilities, then we have $p_f = p_h = p_t = 1/3$ and our formula gives 
$\P(\{coin = C_f\}|\{toss = H\}) = 1/3$ and $\P(\{coin = C_h\}|\{toss = H\}) = 2/3$.

However, an important point is that independently of the initial probabilities, we can say how the probabilities or rather the rations of probabilities changed - our guess that it was the coin was heads/heads went up two times w.r.t. to the fair coin. An in fact, as you will see on the exercise sheet if we could follow more tosses we would become more and more knowledgeable which coin it was, independently of our possibly bad initial estimate. This is also the idea behind Bayesian approach to probability models - we may not know all the parameters to begin with, but we can then just fill them with guesses and as we observe more and more about the world, we can a posteriori improve on these guesses and make our models better.
\end{eg}
%%%%%%%%%%
%%%%%
% HERE DESCRIPTION OF CASES WHEN IT IS POSSIBILITY 
%%
%%
%%%%%%%%


\subsubsection{Law of total probability}

Although conditional probabilities are often tricky, they are necessary to deal with and even useful. For example, they help to decompose the probability space. Indeed, the following result is a generalization of the following intuitive result: if you know that exactly one of three events $E_1, E_2, E_3$ always happens, then to understand the probability of any other event $F$, it suffices to understand the conditional probabilities of this event, conditioned on each of $E_i$, i.e. the probabilities $\P(F|E_i)$.

%Before stating the result about decomposing the probability space let us introduce one more bit of vocabulary. Namely, we saw that when conditioning on some event $E$ of non-trivial probability $0 < \P(E) < 1$, there will be necessarily events whose probability is zero under the conditional measure like for example $E^c$. So whenever we work with probability measures we have to be ready that some events (not only the empty set) have zero probability. 

\begin{prop}[Law of total probability]
	Let $(\Omega, \F, \P)$ be a probability space. Further, let $I$ be countable and $(E_i)_{i \in I}$ be disjoint events with positive probability  $\Omega = \bigcup_{i \in I}E_i$. Then for any $F \in \F$, we can write
	\[\P(F) = \sum_{i \in I}\P(F|E_i)\P(E_i).\]	
\end{prop}

\begin{proof}
	%We can write $F$ as a disjoint union
	%â‚¬$F = \left(F \cap (\bigcup_{i \in I} E_i)\right) \cup \left(F \cap (\Omega \setminus (\bigcup_{i \in I}E_i))\right)$$
	%and as $\P\left(F \cap (\Omega \setminus (\bigcup_{i \in I}E_i))\right) = 0$ by assumption, we see by additivity of $\P$ under disjoint unions that
As $\Omega = \bigcup_{i \in I}E_i$ we have	
 $\P(F) = \P\left(F \cap (\bigcup_{i \in I} E_i)\right).$
	
	Now rewrite $F \cap (\bigcup_{i \in I} E_i) = \bigcup_{i \in I} (F \cap E_i) $.
	Because $(E_i)_{i \in I}$ are disjoint, so are $(F \cap E_i)_{i \in I}$. Hence again by countable additivity for disjoint sets 
	$$\P(F) = \P\left(\bigcup_{i \in I} (F \cap E_i)\right) = \sum_{i \in I} \P(F \cap E_i).$$
	Now, by definition $\P(F \cap E_i) = \P(F|E_i) \P(E_i)$ and the proposition follows.
	
\end{proof}

\begin{rem}
In fact pretty much the same proof works if $E_i$ don't cover the full space, but we only know that $\P(\Omega \setminus (\bigcup_i E_i)) = 0$. This generalisation is left as an exercise.
\end{rem}

\subsection{Independence of events}

Conditional probabilities are of course not at all difficult when the probability of an event does not change under conditioning - i.e. when $\P(E|F) = \P(E)$. Such pairs of events are called independent. In fact the rigorous definition is slightly different:

\begin{defn}[Independence for two events]
	Let $(\Omega, \F, \P)$ be a probability space. We say that two events $E, F$ are independent if $\P(E \cap F) = \P(E)\P(F)$.
\end{defn} 
Observe that when $\P(F) > 0$, then we get back to the intuitive statement of independence, i.e.that $\P(E|F) = \P(E)$. Indeed, if $E$ and $F$ are independent we can write \[\P(E|F) = \frac{\P(E \cap F)}{\P(F)} = \frac{\P(E)\P(F)}{\P(F)} = \P(E).\]
We have chosen the other definition, as then we automatically also include the case where possibly $\P(F) = 0$. 

\begin{eg}
Consider our model of a uniform random number among $\{1,2,3,\dots,12\}$ and the events $E_1 := \{\text{the number is equal to }1\}$, $E_2 := \{\text{the number is divisible by }2\}$, $E_3 := \{\text{the number is divisible by }3\}$. Which of these are independent?

From a direct calculation, we have $\P(E_1 ) = 1/12$, $\P(E_2) = 1/2$ and $\P(E_3) = 1/3$. But also we can directly calculate that $\P(E_1 \cap E_2) = \P(E_1 \cap E_3) = 0$ and $\P(E_2 \cap E_3) = \P(\{\text{the number is divisible by }6\} = 1/6$. We conclude that $E_2, E_3$ are independent, but $E_1$ and $E_2$ are not, neither are $E_1,E_3$.
\end{eg}

Already in this examples we actually had three events and one could also ask if there is some sort of notion of joint independence that generalises to more events. And indeed there are two different ways to generalize independence to several events:
\begin{itemize}
	\item mutual or joint independence
	\item and pairwise independence 
\end{itemize}
The stronger and more important notion is that of mutual independence.

\begin{defn}[Mutual independence]
	Let $(\Omega, \F, \P)$ be a probability space and let $I$ be an index set. Then the events $(E_i)_{i \in I}$ are called mutually independent if for any finite subsets $I_1 \subseteq I$ we have that
	\[\P\left(\bigcap_{i \in I_1}E_i \right) = \Pi_{i \in I_1} \P(E_i).\]
\end{defn}


Sometimes one does not have the full mutual independence or at least does not know it holds, and just pairwise independence can be asserted. There are similar notions of $k-$wise independence too.

\begin{defn}[Pairwise independence]
	Let $(\Omega, \F, \P)$ be a probability space and let $I$ be an index set. Then the events $(E_i)_{i \in I}$ are called pairwise independent if for any $i \neq j \in I$ the events $E_i$ and $E_j$ are independent.
\end{defn}

It is important to notice that, whereas mutual independence clearly implies pairwise independence, the opposite is not true in general:

\begin{exo}[Pairwise independent but not mutually independent]\label{exo:ind}
	Consider the probability space for two independent coin tosses. Let $E_1$ denote the event that the first coin comes up heads, $E_2$ the event that the second coin comes up heads and $E_3$ the event
	that both coin come up on the same side. Show that $E_1, E_2, E_3$ are pairwise independent but not mutually independent.
\end{exo}

Finally, one can also talk about independence of collections of events. This will be important when we try to generalize the notion of independence from events to random variables

\begin{defn}[Mutual independence of collections of events]
 Consider two collections events $(E_i)_{i \in I}$ and $(F_j)_{j \in J}$ all defined on the same probability space. We say that they are independent if for all $i \in I, j\in J$:
	\[\P(E_i \cap F_j) = \P(E_i)\P(F_j).\]
In case of several different collections of events $(E_{j,i})_{i \in I_j}$ for $j = 1 \dots$, we say that these collections are mutually independent if for any finite subset $J_1 \subseteq J$ and any events $E_{j, i_j}$ with $j \in J_1$, it holds that
	\[\P\left(\bigcap_{j \in J_1}E_{j, i_j} \right) = \Pi_{j \in J_1} \P(E_{j, i_j}).\]
Equivalently, we ask any subset of events $E_{j, i_j}$ from different collection to be mutually independent.
\end{defn}

Before going to the independence of random variables, here are some basic properties of independence for events:

\begin{lemma}[Basic properties]
	Let $(\Omega, \F, \P)$ be a probability space. 
	\begin{itemize}
		\item If $E$ is an event with $\P(E) = 1$ then it is independent of all other events.
		\item If $E, F$ are independent, then also $E^c$ and $F$ are independent. In particular every event with $\P(E) = 0$ is independent of all other events.
		\item Finally, if an event is independent of itself, then $\P(E) \in \{0,1\}$.
	\end{itemize}
\end{lemma}

\begin{proof}
This is on the example sheet.

 %Let $E, F \in \F$. By inclusion-exclusion formula
%	$$\P(E \cup F) = \P(E) + \P(F) - \P(E \cap F).$$
	
%	Now, if $\P(E) =1$ then also $\P(E \cup F) \geq \P(E) = 1$ and hence this gives
%	$\P(E \cap F) = \P(F) = \P(F)\P(E)$ and hence $E$ and $F$ are independent.
	
%	For the second property, we can write by law of total probability
%	$$\P(E^c \cap F) + \P(E \cap F) = \P(F).$$
%	By independence of $E, F$ we have $\P(E \cap F) = \P(E)\P(F)$ and thus it follows that
%	$$\P(E^c \cap F) = \P(F)(1-\P(E)) = \P(F)\P(E^c)$$
%	as desired. The second part then follows from the points 1) and 2).
	
%	Finally, if $E$ is independent of itself then $\P(E) = \P(E \cap E) = \P(E)^2$. Hence $\P(E)(1-\P(E)) = 0$, implying that $\P(E) \in \{0,1\}$.

\end{proof}


\subsection{Independence of random variables}

We now formalise the notion of independence for random quantities, i.e. random variables. Recall that (the law of) a random variable $X$ is characterized by all events $\{X \in (a,b)\}$ for intervals $(a,b)$. The mutual independence of random variables is then defined as mutual independence of these sets of events. More precisely,

\begin{defn}[Mutually independent random variables]
Let $I$ be an index set and $(X_i)_{i \in I}$ a family of random variables defined on the same probability space $(\Omega, \F, \P)$. We say that these random variables are mutually independent if for every finite set $J \subseteq I$ and all collections of intervals $((a_j,b_j))_{j \in J}$ we have that
$$\P(\bigcap_{j \in J} \{X_j \in (a_j,b_j)\}) = \Pi_{j \in J}\P\left(X_j \in (a_j,b_j)\right).$$
\end{defn}

\begin{rem}
The more structurally sound definition would use instead as the collection all Borel sets $E_j \in F_\B$. However, that is impractical, and in fact turns out (via some non-trivial measure theory) to be equivalent to the condition above.
\end{rem}

There are naturally more equivalent conditions. For example, a useful one as we see later is the following:

\begin{exo}
Consider random variables $X_1, X_2, \dots$ defined on the same probability space $(\Omega, \F, \P)$. Then $X_1, X_2, \dots$ are mutually independent if and only if for every $m \geq 2$ and all pairs $a_j \in \R$ we have that 
		$$\P(\bigcap_{1 \leq j \leq m} \{X_j \leq a_j\}) = \Pi_{1 \leq j \leq m}\P(X_j \leq a_j).$$
\end{exo}


Further, we again have a very nice and simple condition for random variables defined on discrete probability spaces.

\begin{lemma}[Independence on the discrete probability space]\label{lem:inddiscr}
Let $X_1, \dots, X_n$ be defined on a discrete probability space. Then $X_1, \dots, X_n$ are mutually independent if and only if for every $s_1 \ \dots, s_n \in \R$, we have that $$\P(\bigcap_{i=1}^n \{X_i = s_i\}) = \Pi_{i = 1}^n \P(X_i = s_i).$$
The same holds more generally if $X_1, \dots, X_n$ are defined on any probability space but each take only a discrete number of values with full probability, i.e. for each of them there is some countable set $S_i$ such that $\P(X_i \in S_i) = 1$. \footnote{Such random variables are called discrete random variables, as we will see soon.}
\end{lemma}

\begin{proof}
This is left as an exercise.
\end{proof}

As a sanity check it is now simple to see that for discrete probability spaces (though this is true in general as well!) the indicator events $E, F$ of two events are independent if and only if $E, F$ are independent as events: indeed $\P\left(\{1_E = x\}\cap\{1_F = y\}\right)$ is equal to
$$1_{x = 1}1_{y=1}\P(E)\P(F) + 1_{x = 1}1_{y=0}\P(E)\P(F^c) + 1_{x = 0}1_{y=1}\P(E^c)\P(F) + 1_{x = 0}1_{y=0}\P(E^c)\P(F^c)$$
which in turn can be rewritten as 
$$(1_{x = 1}\P(E) + 1_{x = 0}\P(E^c))(1_{y = 1}\P(F) + 1_{y = 0}\P(F^c)) = \P(\{1_E = x\}) \P(\{1_F = y\}.$$


\begin{exo}[Simple symmetric random walk]
Prove that for a simple random walk of length $n$ all the increments of the walk, i.e. $\Delta_i = S_{i} - S_{i-1}$ for $i = 1 \dots n$, are mutually independent random variables.

%Define the symmetric simple random walk on a product probability space.
\end{exo}

The notion of independent random variables is very important and widely used - often also just because otherwise it is very difficult to do any calculations! 

\begin{rem}[i.i.d. random variables]
Often one talks about collection of i.i.d. random variables $(X_j)_{j \in J}$ - this means that $(X_j)_{j \in J}$ are mutually independent (first 'i') and all have the same probability law, i.e. are identically distributed (the 'i.d.'). Intuitively, this corresponds to repeating the very same random situation or experiment over and over again.
\end{rem}

Now, we started the course by constructing probability spaces and then defining  random variables on it. However, there are natural cases where one would like to go in the opposite direction - we know from observation or experience that we would like to study a bunch of independent random variables and our question is how to construct a probability space where they live? This might sound somewhat silly, but in fact mathematically it is not an easy question! We will partly deal with this question in the next subsection.


\subsection{Independence and product probability spaces}

Whereas independence is a probabilistic concept, it comes out that it is related also to a structure in measure spaces.

Let us consider an example to see this.

\begin{eg}[The space for $n$ fair coin tosses]
We have seen that the probability space for $n$ fair coin tosses can be modelled by taking the state space $\Omega$ to be the set of all $n$-tuples $\{x_1, \dots, x_n\}$ of length $n$ with each $x_i \in \{H,T\}$, then taking $\F$ to be the power set and finally setting the probability of each singleton, i.e. each $n$-tuple, to be $2^{-n}$.

Now, let us look at this as follows:
\begin{itemize}
\item Each $n$-tuples is just an element of the product space $\{H, T\} \times \cdots \{H,T\}$, so we can use as $\Omega$ the product space. Let's denote also by $\Omega_0 = \{H, T\}$ the state spaces for the coordinates.
\item The power-set is the smallest $\sigma-$algebra containing all sets of the form $E_1 \times \cdots \times E_n$ with each $E_i$ in the power-set of a single coordinate $\{H,T\}$
\item The uniform probability measure on $\Omega$ satisfies by definition
$$\P(E_1 \times \cdots E_n) = \P_0(E_1) \cdots \P_0(E_n),$$
where $\P_0$ is the uniform probability measure on the space of a single toss.
\item Finally the fact that the tosses are independent comes down to the following: all events $F_1, \dots, F_n$ of the form $F_i = \Omega_0 \times \Omega_0 \times \dots E_i \times \dots \times \Omega_0$ with $E_i \in \F_i$ are mutually independent:
indeed for $i \neq j $ we have for example
$$\P(\Omega_0 \times \Omega_0 \times \dots E_i \times \dots \times \Omega_0 \cap \Omega_0 \times \Omega_0 \times \dots E_j \times \dots \times \Omega_0) = \P(\Omega_0 \times \Omega_0 \times E_i \times \Omega_0 \dots \times E_j \times \Omega_0 \dots \times \Omega_0)$$
which by above equals $\P_0(E_i)\times \P_0(E_j)$ which again by above is equal to the product of $\P(\Omega_0 \times \Omega_0 \times \dots E_i \times \dots \times \Omega_0)$ and $\P(\Omega_0 \times \Omega_0 \times \dots E_j \times \dots \times \Omega_0$.
\end{itemize}
\end{eg}

So we see that in some sense the product structure goes in hand with independence. And indeed, this is the general rule - mutual independence of random variables is naturally linked to products of probability spaces. 

Let us follows this through mathematically, by first discussing product spaces in general and then looking at the construction of probability spaces for independent random variables.

\subsubsection{Construction of product spaces}

So let us have a brief look at the construction of product spaces. Consider probability spaces $(\Omega_i, \F_i, \P_i)$ for $i = 1,2 \dots$. Then to construct the product probability space we need a product $\sigma-$algebra and a product measure.
\begin{enumerate}
    \item The product $\sigma-$algebra $\F_\Pi$ is simple and natural: it is the smallest $\sigma-$algebra containing all $E_{i_1} \times \cdots \times E_{i_n}$ with $E_{i_j} \in \F_{i_j}$ for all $j = 1 \dots n$ and $\{i_j\}_{j = 1 \dots n}$ a finite subset of $\N$.
    \item The product probability measure $\P_\Pi$ of $\P_1, \P_2, \dots$ on $(\Pi_{i \geq  1} \Omega_i, \F_\Pi)$ also sounds simple: it is the only probability measure such that $$\P(E_{i_1} \times \cdots \times E_{i_n}) = \Pi_{j = 1}^n\P_i(E_{i_j})$$ for all $E_{i_1} \times \cdot \times E_{i_n}$ with $E_{i_j} \in \F_{i_j}$ for $j = 1 \dots n$.   However, its construction and uniqueness even in the case of finite products is technical for general probability spaces and out of the scope of this course.
\end{enumerate}

Thus we will state the following theorem without proof, which you will see in the measure theory or the third year probability course:

\begin{thm}[Product measure // admitted]
For $i \in \N$, let $(\Omega_i, \F_i, \P_i)$ be probability spaces. Then there exists a unique probability measure $\P_\Pi$ on $(\Pi_{i \in \N}\Omega_i, \F_{\Pi})$ such that
for any finite subset $J \subset \N$ and any event $E$ of the form $E = \Pi_{i \in \N} F_i$ with $F_i = \Omega_i$ for $i \notin J$ and $F_i = E_i \in \F_i$ for $i \in J$, we have that 
\begin{equation}\label{eq:prod}
\P_\Pi(E) = \Pi_{i \in J} \P_i(E_i).
\end{equation}
We call such a measure the product measure of the collection $((\Omega_i, \F_i, \P_i))_{i \geq 1}$.
\end{thm}

It is rather easy to see the existence and uniqueness in the case of a finite number of discrete probability spaces, so let us do that. Below, we state it in the case where the $\sigma-$algebras are equal to the power set, but as discussed before (see Proposition \ref{prop:disatom}, this essentially encompasses the case of general $\sigma-$algebras on discrete spaces.

\begin{lemma}[Discrete product spaces]\label{lem:discprod}
Let $(\Omega_i, \Po(\Omega_i), \P_i)$ for $i = 1 \dots n$ be discrete probability spaces.
Then the product probability $\P_\Pi$ measure on $(\Pi_{i = 1}^n \Omega_i, \F_\Pi)$ exists and is unique.  
\end{lemma}

\begin{proof}
On the example sheet
%Observe that $\F_\Pi = \Po(\Pi_{i = 1}^n \Omega_i)$: indeed, as each $\{\omega_i\} \in \F_i$, it follows that 
%$\{(\omega_1, \dots, \omega_n)\} \in \F_\Pi.$ But we saw that in case where $\Omega_i$ are discrete, the smallest $\sigma-$algebra containing all the singletons is the power-set.

%Now, we have that 
%$$E_1 \times \cdots \times E_n = \bigcup_{\forall i: \omega_i \in E_i }\{(\omega_1, \dots, \omega_n)\}.$$
%Moreover, for a finite product of discrete probability spaces this disjoint union is countable. It follows that
%$$\P_\Pi(E_1 \times \cdots \times E_n) = \sum_{\forall i: \omega_i \in E_i } \P_\Pi(\{(\omega_1, \dots, \omega_n)\}).$$
%As also
%$$\sum_{\forall i: \omega_i \in E_i } \Pi_{i = 1}^n\P_i(\{\omega_i\}) = \Pi_{i = 1}^n\P_i(E_i),$$
%the condition for being a product measure is equivalent to
%$$\P_\Pi(\{(\omega_1, \dots, \omega_n)\}) = \Pi_{i = 1}^n\P_i(\{\omega_i\})$$ for all $\omega_i \in \Omega_i$. But this condition uniquely defines $\P_\Pi$. 

%Indeed, we have already seen that on a discrete probability space with power-set $\sigma-$algebra we can uniquely define the probability measure by fixing it on singletons to some numbers in $[0,1]$ that sum up to $1$, as is the case here.

%Indeed, the right-hand side is a well defined number in $[0,1]$ and we know that to determine a probability measure on a discrete probability space with its power-set, it suffices to just to determine the probability on singletons. 

%The only question we might have, why does this define a probability measure, i.e. why are axioms satisfied by this definition? We leave this simple check of axioms to the reader.
\end{proof}

\subsubsection{Probability spaces for independent random variables}

We will now follow through the philosophy alluded to above: 
\begin{itemize}
\item if we are given some laws of random variables and we want to construct a common probability space on which all of these random variables are defined and are moreover mutually independent, then we should use product spaces.
\end{itemize}

We will again state this proposition in a larger generality than we prove it.

\begin{thm}[Existence of probability spaces with independent random variables // partly admitted]\label{thm:extprobspace}
Consider random variables $(X_i)_{i \geq 1}$. Then we can find a common probability space $(\Omega, \F, \P)$ and random variables $(\widetilde X_i)_{i\geq 1}$ defined on $(\Omega, \F, \P)$ such that 
	\begin{itemize}
		\item For all $i \geq 1$, $\widetilde X_i$ and has the law of $X_i$
		\item Moreover, the random variables $(\widetilde X_i)_{i \geq 1}$ are mutually independent.
	\end{itemize}
\end{thm}



\begin{eg} 
Suppose you have a coin that is not fair, but comes up heads with probability $p \in (0,1)$. How would you model the sequence of independent $n$ such tosses? 

The assumption of all sequences being equally likely does not make sense any longer (e.g. think of the case when $p$ is near $1$, then certainly the sequence of all zeros and all ones cannot have the same probabilities). However, the assumption of mutual independence and its relation to product measures are useful.

Indeed, we can define the probability space as follows:
\begin{itemize}
    \item we take the product space of $n$ copies of $(\{0,1\},\Po(\{0,1\}), \P_p)$ , where $\P_p$ such that it gives $1$ with probability $p$ and $0$ with probability $1-p$. 
\end{itemize}
Notice that in this probability space, the probability of a fixed sequence of $n$ tosses with $m$ heads and $n-m$ tails is exactly $p^m(1-p)^{n-m}$. If we further want to calculate the probability that we have exactly $m$ heads (regardless of the positions in which the heads appear) we have to sum over all sequences with $m$ heads and we get ${n \choose m}p^m(1-p)^{n-m}$. Check that $\sum_{m = 0}^n {n \choose m}p^m(1-p)^{n-m} = 1$!
\end{eg}

%We will again content ourselves with proving it in the case of discrete random variables and for finite products.

 %First we need a small lemma that will be on the exercise sheet:

 %\begin{lemma}
 %Let $X:\Omega \to \R$ be a random variable on some probability space $(\Omega, \F, \P)$. Suppose that there is some countable set $S$ such that $\P(X \in S) = 1$. Then one can find a countable set $\hat \Omega$, a probability measure $\hat \P$ on $(\hat \Omega, \Po(\hat \Omega)$ and a random variable $\hat X : \hat \Omega \to \R$ such that $\hat X$ is equal in law to $X$.
 %\end{lemma}

 Let us now give the proof of the theorem in the case when all the random variables are defined on discrete probability spaces. For a slightly more natural statement, see the exercise sheet

\begin{proof}[Proof of Theorem \ref{thm:extprobspace}, case of finite products of random variables on discrete spaces]

Suppose we have discrete probability spaces $(\Omega_i, \Po(\Omega_i), \P_i)$ and random variables $ X_i: \Omega_i \to \R$.

By the Lemma \ref{lem:discprod} above, we can construct the product probability space corresponding to these probability spaces, denoted $(\Omega_\Pi = \Pi_{i = 1}^n \Omega_i, \F_\Pi, \P_\Pi)$.

Now, define $\widetilde X_i(\omega_1, \dots, \omega_n) := X_i(\omega_i)$. One can check that $\widetilde X_i$ thus defined are all random variables and they are defined to have the same law as $X_i$. Indeed, by the definition of $\widetilde X_i$ and the product measure
$$\P_{\widetilde X_i}(E) = \P_\Pi(\Omega_1 \times \Omega_2 \dots \times X_i^{-1}(E) \times \dots \dots \times \Omega_n) =\P_{ X_i}(E).$$

Finally, we need to check that the random variables $(\widetilde X_i)_{i = 1 \dots n}$ are mutually independent on the space $(\Pi_{i = 1}^n \Omega_i, \F_\Pi, \P_\Pi)$. From the identity
$$\{\omega: \Omega_\Pi:\widetilde X_i(\omega) \in E_i\} = \{\Omega_1 \times \dots \times X_i^{-1}(E) \times \dots \times \Omega_n\}$$
we have that:
$$\P_\Pi(\bigcap_{i = 1 \dots n} \{\widetilde X_i \in E_i\}) = \P_\Pi( \Pi_{i = 1}^n X_i^{-1}(E_i)).$$
By the definition of product measure this equals
$\Pi_{1 =1}^n\P_{ X_i}(E_i),$
which in turn equals $\Pi_{i =1}^n\P_{\widetilde X_i}(E_i)$ by equality in law. The last expression is equal to $\Pi_{i =1}^n\P_\Pi(\widetilde X_i \in E_i)$ by definition and we conclude.
\end{proof}

Let us finish this section by playing with an important example. 

\subsubsection{Erd\"os-Renyi random graph}

Our aim in this section is to describe and study random graphs. Graphs are simple mathematical structures that help to describe networks like social networks, or logistic networks or why not the network of neurons in the brain.

\begin{defn}[Simple graph]
Let $n \in \N$. A simple graph is a pair $G = (V,E)$ where $V$ is a set of points $V = \{v_1, \dots, v_n\}$, called vertices, and $E$ is a subset of $\{\{v_i, v_j\} : (v_i, v_j) \in V \times V, v_i \neq v_j\}$, i.e. a set of unordered pairs of distinct vertices, called edges.
\end{defn}

You can imagine the graph as drawing all the $n$ points $v_1, \dots, v_n$ on the plane and then drawing a line between $v_i$ and $v_j$ to say they are connected if and only if $\{v_i, v_j\} \in E$. 

If the networks are very big, like the brain or the social network in Facebook, it is both impractical and unfeasible to describe them in all detail. Moreover, it comes out that usually they start resembling certain random networks. Thus in order to understand properties of these real world networks, one often studies the simplified models of random networks.

The easiest model of a random network, or in our mathematical language of a random graph, is the Erd\"os-Renyi random graph where we include each edge with probability $p > 0$.

\begin{eg}[Erd\"os-Renyi random graph]\label{eg:rgraph}
For $n \in \N$ consider a set of vertices $V$ of size $n$ and let $E$ be the set of all undirected edges between these vertices. 

The Erd\"os-Renyi random graph $G_{n,p}$ of size $n$ and edge parameter $p \in [0,1]$ is then defined by including each possible edge independently with probability $p$.

To define the relevant probability space we let
\begin{itemize}
    \item The state space should include all possible graphs with the vertex set $V$. We observe that this can be done by determining the edge set. So we let
    $\Omega = \{0,1\}^E$ be the set of all possible edge configurations on $n$ vertices - we interpret $1$ to mean that an edge is present.
    \item We assume that we can check for any edge if it is present or not, and thus set $\F = \Po(\Omega)$
    \item Finally, we set each edge to be present with probability $p$ independently of others. In other words for each $\omega \in \Omega$ we set 
    $$\P_p(\{\omega\}) := p^{|\omega|}(1-p)^{\vert E\vert - |\omega|},$$ where $\omega \in \Omega$ is an edge configuration and $|\omega|$ is the number of edges in this configuration. 
\end{itemize} Finally, we can identify each element $\omega$ also with the resulting graph $G_{n,p}(\omega) = (V, E(\omega))$.
	\end{eg} 

What are some questions that we would like to look at? Roughly we would like to answer how the graph look likes when $n$ is very large, i.e. tending to infinity. Of course sometimes one could be also interested in $n$ small, but then one could actually explicitly describe the probability of each possible graph and picture it. 

Now to describe how the graph looks like we could consider the following questions:
\begin{enumerate}
\item How many edges are present?
\item Is the graph connected, i.e. can one find for each $v, w \in V$ a set of edges $e_1, \dots, e_n$ such that each $e_i, e_{i-1}$ share a vertex and $e_1$ is connected to $v$ and $e_n$ connected to $w$? 
\item If yes, what is the maximal distance between two vertices?
\item If no, how many different connected components are there?
\item What is the biggest connected component?
\item ...
\end{enumerate}

Each of these questions is about a single graph, i.e. a single configuration $\omega$. Thus in the random graph model they correspond either to an event or random variable, whose probability or law we can study. 

For example, $N_E: \Omega \to \N$ given by $N_E(\omega) := |\omega|$ attaches to each $\omega$ its number of edges and thus corresponds to the first question. Similarly the event $F := \{\omega \text{ is connected}\}$ corresponds to the second question. Of course there are also more complex questions, which arise when one consideres several questions at the same time.

One is interested in both how the probability of these events behaves for $p \in [0,1]$ fixed and $n \to \infty$, but also how this behaviour changes when we change $p$. Notice that a priori $p$ does not need to be constant, we can also easily consider a sequence of graphs $G_{n, p(n)}$ where $p(n)$ is a function of $n$.

Studying the properties of Erd\"os-Renyi random graphs was and still is a very active research topic, with hundreds if not thousands of papers written about them. We will try to just get a very small taste of this research.

Let us concentrate on one notion, that of connectivity and look at some scenarios. Notice that when $p = 1$ then the graph is connected with probability $1$ and when $p= 0$ it is disconnected with probability $1$. We will try to get a grasp what happens with $p_n \in (0,1)$ possibly changing with $n$.

\begin{claim}
Let $p \in (0,1)$ be fixed. Then as $n \to \infty$ the probability of the graph being connected converges to $1$ almost surely i.e. with probability 1.
\end{claim}

This is maybe not so surprising as with fixed probability $p$ we will have lots of edges: indeed, if you think of edges as coin tosses, you would expect to have a proportion $p$ of all edges to be present, which makes $pn(n-1)/2$ edges!


\begin{proof}
We will prove that $\P_p(\{G_{n,p} \text{ is not connected}\}) \to 0$ as $n \to \infty$. First notice that 
$$\{G_{n,p} \text{ is not connected}\} = \cup_{v \neq w \in V}\{\ v, w\text{ not connected by a path}\}.$$
Thus by the union bound
$$\P_p(\{G_{n,p} \text{ is not connected}\}) \leq 1/2\sum_{v \neq w \in V}\P_p(\{\ v, w\text{ not connected by a path}\}),$$
where the $1/2$ comes from the fact that we count each edge twice in the sum. But because of symmetry of the model, each pair of edges is equivalent, so we can write the right hand side as $n(n-1)/2\cdot\P_p(\{\ v, w\text{ not connected by a path}\})$.

Thus we want to bound the probability that $v$ and $w$ are not connected by a path. First, just looking at the edge $\{v,w\}$  is not enough - this edge is absent with probability $1-p$, which doesn't go to zero. However, there are many other ways to connect these two vertices. 

One way is to use an intermediate vertex $z$: if $v$ and $w$ are not connected, then there is no vertex $z$ such that both $\{v,z\}$ and $\{z,w\}$ belong to the edge set. Thus we can write
    \begin{align*}
        \P_p (\{\ v, w\text{ not connected by a path}\})
        \leq \prod_{z \in V \setminus \{v,w\}}\P_p (\{\{v,z\} \notin E\}\cup\{\{z,w\} \notin E\}). 
    \end{align*}
    But now $\P_p (\{\{v,z\} \notin E\}\cup\{\{z,w\} \notin E\}) = 1 - \P_p(\{\{v,z\} \in E\}\cap\{\{z,w\} \in E\} = 1 - p^2$ and hence 
    $$\P_p (\{\ v, w\text{ not connected by a path}\}) \leq (1-p^2)^{n-2}.$$
    This clearly goes to zero as $n \to \infty$ and thus any two fixed vertices will be connected with probability going to $1$. 

    We now come back to our initial probability of all pairs being connected and bound:
    $$\P_p(\{G_{n,p} \text{ is not connected}\}) \leq n(n-1)/2\cdot(1-p^2)^{n-2}.$$
    This is also nicely goes to zero!

\end{proof}

In fact, if we look at the proof more carefully we see that the claim is true as long as $p = p(n)$ goes to zero with $n$ sufficiently slowly. In other words the exact same proof gives us

\begin{claim}
Let $(p_n)_{n \geq 1}$ be a sequence of numbers in $[0,1]$ satisfying $p_n \geq n^{-1/4}$ . Then as $n \to \infty$ the probability of the graph being connected converges to $1$ almost surely i.e. with probability 1.
\end{claim}

\begin{proof}
We follow the proof above and notice that for $1 \geq p_n \geq n^{-1/4}$ we still have that
$$n(n-1)/2\cdot(1-p^2)^{n-2} \to 0$$
as $n \to \infty$.
\end{proof}

On the other hand, we have that

\begin{claim}
Let $(p_n)_{n \geq 1}$ be a sequence of numbers in $[0,1]$ such that $p_n \leq n^{-2}$ . Then as $n \to \infty$ the probability of the graph being connected converges to $0$ almost surely i.e. with probability 1.
\end{claim}

This will be on the exercise sheet. But notice the interesting phenomena: there seems to be a sort of threshold effect. If $p_n$ decays very fast, the probability of connectedness goes to $0$; if decays slowly enough it goes to $1$. Why doesn't it go to some other number between $0$ and $1$? Where is the exact threshold? It is a non-trivial theorem that says this threshold is exactly at $p_n = \frac{\log n}{n}$!


\section{Random variables and random vectors}

In this chapter, we will look more closely into random variables and $n$-tuples of random variables, called random vectors. 

\subsection{The cumulative distribution function of a random variable}

Recall that we call two random variables equal in law, when the probability measures they induce on $(\R, \F_B)$ are equal - this allowed us to compare random variables defined on different probability spaces, coming up in different contexts.

Our first aim is to see how to classify and compare random variables more easily. Indeed, for now we saw that the law of each random variable is described by the probability over all possible events, but this is a description that is very difficult to deal with. 

It comes out that all the information about the law of a random variable can be uniquely encoded using what is called a cumulative distribution function. 

\begin{defn}[Cumulative distribution function]
We call a function $F:\R \to [0,1]$ a (cumulative) distribution function (abbreviated c.d.f.) if it satisfies the following conditions: 
\begin{enumerate}
	\item $F$ is non-decreasing;
	\item $F(x) \to 0$ as $x \to -\infty$ and $F(x) \to 1$ as $x \to \infty$;
	\item $F$ is right-continuous, i.e. for any $x \in \R$ and any sequence $(x_n)_{n \geq 1} \in [x,\infty)$ such that $x_n \to x$, we have that $F(x_n) \to F(x)$.
\end{enumerate}
\end{defn}

Given a random variable $X$, we define its cumulative distribution function as follows:

\begin{prop}[Cum.dist. function of a random variable]\label{prop:cumrv}
For each random variable $X$ (defined on some probability space $(\Omega, \F, \P)$), the function $F_X(x) := \P_X((-\infty,x])$ defines a cumulative distribution function (c.d.f). 
\end{prop}

\begin{proof}
Set $F_X(x) = \P(X \in (-\infty,x])$.
Then as $(-\infty,x] \subseteq (-\infty,y]$ for $x \leq y$, we have by (1) of Proposition \ref{prop:propmeas} that $F$ is non-decreasing.

Let us next check right-continuity of $F$. So let $(x_n)_{n \geq 1}$ be any sequence in $[x, \infty)$ converging to $x$. 
Then setting $A_n := \cap_{1 \leq k \leq n} (-\infty, x_k]$ we get that $\bigcap_{n\geq 1} A_n = (-\infty,x]$. 
By continuity of $\P$, i.e. (5) of Proposition \ref{prop:propmeas}, it follows that $\P_X(A_n) \to \P_X((-\infty,x])$. But now notice that as $x_n \to x$, we have that for any $n$ large enough $\{-\infty, x_n\} \subseteq A_{m_n}$ for some $m_n$ chosen such that $m_n \to \infty$ as $n \to \infty$. It follows that $F_X(x) \leq F_X(x_n) \leq \P_X(A_{m_n})$ and we conclude that $F_X(x_n) \to F_X(x)$ as $n \to \infty$.

The final two claims are on the example sheet.

%Now, if $(x_n)_{n \geq 1} \to -\infty$ we have that $\bigcap_{n \geq 1}(-\infty, x_n] = \emptyset$. Hence similarly to above (5) of Proposition \ref{prop:propmeas} implies that $F(x_n) \to 0$. Finally, if $(x_n)_{n \geq 1} \to \infty$, we have $\bigcup_{n \geq 1} (-\infty,x_n] \to \R$ and thus by (2) of the same proposition again $F(x_n) \to 1$.
\end{proof}

In fact, it comes out the conversely each cumulative distribution function gives rise to a unique law of a random variable.

\begin{thm}[Laws of random variable are uniquely determined by c.d.f. //  admitted]\label{prop:cumrv}
Each cumulative distribution function $F$ gives rise to a unique law of a random variable $X$ such that $F_X(x) = \P_X((-\infty,x])$. In other words c.d.f.s are in one to one correspondence with probability measures $\P$ on $(\R, \F_B)$.
\end{thm}

We admit this theorem in the general case, but will again prove the discrete case. Let us look at a simple example:

\begin{eg}

Let us calculate the c.d.f of the so called Bernoulli random variable $X$ that takes value $1$ with probability $p$ and $0$ with probability $1-p$. Notice that all indicator functions of events correspond to such random variables with $\P(E) = p$. 

We have $F_X(x) = (1-p)1_{x \geq 0} +p1_{x \geq 1}$. More generally for a random variable that takes only finite number of values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$, we have
$F_X(x) =\sum_{i = 1 \dots n}p_i 1_{x \geq x_i}$. (Why?)

\end{eg}

Thus we see that $F_X$ encodes the behaviour of $X$ rather naturally. Let us now look at this relation between the cumulative distribution function $F_X$ and the random variable $X$ more closely. By $F(x^-)$ we denote the limit of $F(x_n)$ with $(x_n)_{n \geq 1} \to x$ from below, i.e. by numbers $x_n < x$.

\begin{lemma}[C.d.f vs r.v.]\label{lem:cumvsrv}
	Let $X$ be a random variable on some probability space $(\P, \Omega, \F)$ and $F_X $ its cumulative distribution function. Then for any $x < y \in \R$
	\begin{enumerate}
		\item $\P(X < x) = F(x-)$ 
		\item $\P(X > x) = 1 - F(x)$ 
		\item $\P(X \in (x,y)) = F(y-) - F(x)$.
		\item $\P(X = x) = F(x) - F(x-)$.
	\end{enumerate}
\end{lemma}

\begin{proof}
This is on exercise sheet.
%First from $F(X) = \P(X \leq x)$ it follows directly that 
%$$1-F(X) = 1 - \P(X \leq x) = \P(X > x).$$ 
%Now, write $\{X < x\} = \cup_{n \geq 1}\{X < x - 1/n\}$. Then by Proposition \ref{prop:propmeas}, we conclude that
%$$F(x-) = \lim_{n \geq 1}F(x-1/n) = \lim_{n \geq 1} \P(X \leq x-1/n) = \P(X < x).$$
%To finish the first part, observe that by additivity of $\P$ under disjoint events
%$$\P(X \in (x,y)) + \P(X \leq x) = \P(X < y)$$
%and thus $\P(X \in (x,y)) = F(y-) - F(x).$
%For the second part, notice similarly that by additivity under disjoint events $$\P(X = x) + \P(X < x) = \P(X \geq x),$$
%from which it again follows that $\P(X = x) = F(x) - F(x-)$.
\end{proof}

\begin{eg}
Let us also exhibit the c.d.f. of the uniform random variable $U$ taking values uniformly in $[0,1]$. It is given by $F_U := x1_{x \in [0,1]} + 1_{x > 1}$. By the proposition above we can see that for any interval $(a,b) \subseteq [0,1]$, $\P(U \in (a,b)) = b-a$.
\end{eg}


From above we see that all jumps of $F_X$ correspond to points where $\P(X = x) > 0$. In fact there can be only countably many of them.

\begin{lemma}
A cumulative distribution function $F_X$ of a random variable $X$ has at most countably many jumps. 
\end{lemma}

\begin{proof}
Let $S_n$ be the set of jumps that are larger than $1/n$ and $\widehat S_n$ any finite subset of $S_n$. 
Then $\widehat S_n$ is measurable and $1 \geq \P(X \in S_n) \geq |\widehat S_n|n^{-1}$. Thus it follows that $|\widehat S_n| \leq n$.
As this holds for any finite subset of $S_n$, we deduce that $|S_n| \leq n$ and in particular $S_n$ is finite.

Now the set of all jumps can be written as a union $\bigcup_{n \geq 1} S_n$. Hence as each $S_n$ is finite and a countable union of finite sets is countable, we conclude.
\end{proof}

These jumps of a c.d.f. $F_X$ are sometimes called atoms of the law of $X$. More precisely, we call $s \in \R$ an atom for the law of $X$ if and only if $\P(X = s) > 0$.

In the extreme case $F_X$ increases only via jumps, i.e. is piece-wise constant changing value at most countable times.
%%%% BRING OUT OF THE TEXT
Precisely:
\begin{defn}[Piece-wise constant with at most countable jumps]
We say that $f: \R \to [0, \infty)$ is piece-wise constant with countably many jumps iff there is some countable set $S$ and some real numbers $c_s > 0$ for $s \in S$ such that $\sum_{s \in S} c_s < \infty$ and
$$ f(x) = \sum_{s \in S} c_s1_{x \geq s}.$$
\end{defn}
Notice that this set $S$ could be dense, like the set of rational numbers, making it hard to imagine as a staircase function!

In the other extreme $F_X$ could also be everywhere continuous. These observations help us separate out two classes of random variables.

\subsubsection{Classification of random variables}

\begin{defn}[Discrete vs continuous random variables]
A random variable is called discrete if its c.d.f. $F_X$ is piece-wise constant changing value at most countable many times. It is called continuous if its c.d.f. $F_X$ is continuous.
\end{defn}

These definitions look a bit abstract / non-telling from the probabilistic perspective and a priori differs from the definition we gave on the example sheet! But no need to worry, it does give the same object:

\begin{exo}[Discrete vs random variables ver 2]
Consider a random variable $X$. Prove that
\begin{itemize}
    \item $X$ is discrete, i.e. its cumulative distribution function $F_X$ is piece-wise constant, if and only if there is a countable set $S \subseteq \R$ with $\P(X \in S) = \P_X(S) = 1$.
    \item $X$ is continuous if and only if for every $y \in \R$, $\P(X = y ) = \P_X(\{y\}) = 0$.
\end{itemize}
\end{exo}


Notice that not every random variable is either discrete or continuous, there could be also mixtures of the two, e.g. one could imagine a c.d.f. given by $F(x) = 0.5\cdot 1_{x \geq 0} + 0.5\cdot x\cdot1_{x \in [0,1)} +0.5\cdot 1_{x \geq 1}$ (What does it correspond to?).

The following proposition says, the c.d.f. of any random variable can be written as a convex combination of c.d.f-s of a discrete and continuous random variable. 

\begin{prop}\label{prop:cdfsum}
Any cumulative distribution function $F$ can be written uniquely as convex combination of a continuous c.d.f $F_c$ and a piece-wise constant c.d.f. with countably many jumps $F_j$ i.e. for some $a \in [0,1]$ we have that  $F = a F_{j} + (1-a)F_{c}$.
\end{prop}

Moreover, in a later exercise sheet you will see how to interpret this as saying that each random variable can be written as a random sum of a continuous and discrete random variable.

\begin{proof}
	If $F$ is either continuous or piece-wise constant with countably many jumps, the existence of such writing is clear. So suppose that $F$ is neither. Write $S$ for the countable set of jumps of $F$. Define 
	
$$\widehat F_{j}(x) = \sum_{s \in S}1_{x \geq s}(F(s) - F(s-)),$$
which is piece-wise continuous with countably many jumps.

We claim that $\widehat F_{c} := F - \widehat F_{j}$ is continuous. Indeed, by definition both $F$ and $\widehat F_{j}$ both right-continuous, and thus is also their difference. Moreover, both are continuous at any continuity point $x$ of $F$, i.e. when $x \notin S$ as by definition then $F(x) = F(x^-)$ and one can check the same for $F_j$. Finally, when $s \in S$, then again by definition of $\widehat F_{j}$, we have that $$F(s) - F(s-) = 1_{s \geq s}(F(s) - F(s-)) = \widehat F_{j}(s) - \widehat F_{j}(s-)$$
and thus $\widehat F_{c}$ is continuous at such $s$ too.

Now, as $F$ is neither continuous nor piece-wise constant increasing with jumps, we have that $0 < \widehat F_{j}(\infty) < 1$ and $0 < \widehat F_{c}(\infty) < 1$. Hence, we can define 
$$F_{j}(x) := \frac{\widehat F_{j}(x)}{\widehat F_{j}(\infty)}$$
and
$$F_{c}(x) := \frac{\widehat F_{c}(x)}{\widehat F_{c}(\infty)}.$$
By definition both of those are non-decreasing, right-continuous satisfying the correct limits at $\pm \infty$ and hence are c.d.f-s for random variables. As $F_{j}$ increases only via jumps and $F_{c}$ is continuous, we have the desired writing with $a =  \widehat F_{j}(\infty)$ and $1- a = \widehat F_{c}(\infty)$.

Uniqueness is left as an exercise.
To see the uniqueness of the decomposition, suppose that one can write
$$F_X = a F_{Y_1} + (1-a)F_{Y_2} = b F_{Z_1} + (1-b)F_{Z_2},$$
where both $Y_1$ and $Z_1$ are discrete and $Y_2, Z_2$ continuous random variables. Then $a F_{Y_1}  - b F_{Z_1}$ has to be continuous, but also piecewise constant with countably many jumps. As $a F_{Y_1}(-\infty)  - b F_{Z_1}(-\infty) = 0$, the only possibility is that it is constantly zero. As $F_{Y_1}(\infty) = 1 = F_{Z_1}(\infty)$, it follows that $a = b$ and $F_{Y_1} = F_{Z_1}$. Thus also $F_{Y_2} = F_{Z_2}$ and the proposition follows. 

\end{proof}


\subsection{Examples of discrete random variables}

There are several families of laws of discrete random variables that come up again and again. As we will see, sometimes these laws also have very nice mathematical characterizations. 

Recall that to characterise the law of a random variable, we can either give the value of $\P_X(F)$ for a sufficiently large set of $F$ (e.g. all intervals) or give the c.d.f. For a discrete random variable $X$ it suffices to just determine the support $S$, i.e. the smallest set $S \subseteq \R$ such that $\P(X \in S) = 1$ and determine $\P(X = s)$ for each $s \in S$ (why?). \\



\noindent \textbf{Bernoulli random variable}\\
As mentioned already, a random variable that takes only values $\{0,1\}$, taking value $1$ with probability $p$ is called a Bernoulli random variable of parameter $p$. It is named after the Swiss mathematician Bernoulli, who also thought that all sciences need mathematics, but mathematics doesn't need any. Leaving you to judge, let us see that these examples come up very often.

Namely, on every probability space $(\Omega, \F, \P)$, every indicator function of an event, i.e. $1_E$ gives rise to a Bernoulli random variable and the parameter $p$ is equal to the probability of the event. Indeed for any event $E$ in a probability space $(\Omega, \F, \P)$ the indicator function $1_E: (\Omega, \F) \to (\R, \F)$ is measurable and hence  a random variable. Moreover, it is $\{0,1\}$ valued by definition and $\P(\{1_E = 1\}) = \P(E) = p$. 

Sometimes one talks about Bernoulli random variables more generally whenever there are two different outcomes, e.g. also when the values are $\{-1,1\}$. We then call it the Bernoulli random variable with values $\{-1,1\}$.\\

\noindent \textbf{Uniform random variable}\\
Any random variable that takes values in a finite set $S = \{x_1, \dots, x_n\}$, each with equal probability $1/n$ is called the uniform random variable on $S$. We call the law of this random variable the uniform law. Its c.d.f is given by simply $F_X(x) = n^{-1}\sum_{i = 1}^n 1_{x \geq x_i}$.

Examples are - a fair dice, the outcome of roulette, taking the card from the top of a well-mixed pack of cards etc...
For concreteness, a trivial example is that if we model a fair dice on $\Omega = \{1,2,3,4,5,6\}$, $\F = \Po(\Omega)$ and $\P({i}) = 1/6$, then the random variable $X(\omega) := \omega \in \R$ gives rise to a uniform random variable. 

We use this family of random variables every time we have no a priori reason to prefer one outcome over the other. A fancy mathematical way of saying this would be to say that the uniform law is the only probability law on a finite set that is invariant under permutations of this set. We will also see on the example sheet that this is the so called maximum entropy probability distribution with values in a finite set $S$.\\

\noindent \textbf{Binomial random variable}\\
A random variable that takes values in the set $\{0,1,\dots,n\}$, and takes each value $k$ with probability $$p^k(1-p)^{n-k}{n \choose k}$$ is called a binomial random variable of parameters $n \in \N$ and $0 \leq p \leq 1$ (why do the probabilities sum to one?). We denote the law of such a binomial random variable by $Bin(n,p)$. 

Notice that for $n = 1$, we have the Bernoulli random variable. Bernoulli random variable comes up naturally in models of independent coin tosses, random graphs, or models of random walks. The reason why it comes up so often is that it always describes the following situation - we have a sequence of independent indistinguishable events and we count the number of those who occur. Or in other words, the Binomial random variable $Bin(n,p)$ can be seen as a sum of $n$ independent $Ber(p)$ random variables. 

\begin{exo}[Binomial r.v. is the number of occurring events]
Suppose we have $n$ mutually independent events $E_1, \dots, E_k$ of probability $p$ on some probability space $(\Omega, \F, \P)$. Consider the random number of events that occurs: $X = \sum_{i = 1}^n 1_{E_i}$. Prove that $X$ is a random variable and has the law $Bin(n,p)$.
\end{exo}

For a concrete lively example, let's go back to the Erdos-Renyi random graph on $n$ vertices, where each edge is independently included with probability $p$. We can then fix some vertex $v$ and consider the random variable $M_v$ giving the number of vertices adjacent to $v$, i.e. linked to $v$ by an edge. The exercise above shows that this random variable has law $Bin(n-1,p)$.\\

%\begin{proof}
%	Notice that $X \in \{0, \dots, n\}$ and for every $k = 0 \dots n$, $\{X = k\}$ can be written as $$\{X = k\} = \bigcup_{I \subseteq \{1, \dots, n\}, |I| = k}\cap_{i \in I}E_i\cap_{i \notin I}E_i^c.$$ Hence, $X^{-1}((-\infty,x])$ is measurable for any $x$ and $X$ is a random variable.
	
%	But now we can write 
%	$$\P(X =k) = \P(\bigcup_{I \subseteq \{1, \dots, n\}, |I| = k}\cap_{i \in I}E_i\cap_{i \notin I}E_i^c).$$
%	Observe that all the events in the union are disjoint, and thus
%	$$\P(X =k) = \sum_{I \subseteq \{1, \dots, n\},  |I| = k}\P(\cap_{i \in I}E_i\cap_{i \notin I}E_i^c).$$
%	As there are exactly $\{n \choose k\}$ subsets of size $k$, and events $E_i$ are mutually independent, we deduce
%	$$\P(X = k) = {n \choose k}\Pi_{i \in I}\P(E_i)\Pi_{i \notin I}\P(E_i^c).$$
%	Plugging now in the fact that for all $E_i$ we have that $\P(E_i) = p$, the result follows.
%\end{proof}


\noindent \textbf{Geometric random variable}\\
A random variable that takes values in the set $\N$, each value $k$ with probability $p (1-p)^{k-1}$ for some $0 < p \leq 1$ is called a geometric random variable of parameter $p$. We denote the law of a geometric random variable by $Geo(p)$. One should again check that this even defines a random variable, by seeing that the probabilities do sum to one.

A geometric random variable describes the following situation: we have independent events $E_1, E_2, \dots $ each of success probability $p$ and we are asking for the smallest index $k$ such that the event $E_k$ happens. For example, $Geo(1/2)$ describes the number of tosses needed to get a first heads. This will be made precise on the exercise sheet.

There is also a nice property that characterizes the geometric r.v.:

\begin{lemma}[Geometric r.v. is the only memoryless random variable with values in $\mathbb{N}$]
We say that a random variable $X$ with values in $\N$ is memoryless if for every $k, l \in \N$ we have that $\P(X > k+l| X > k) = \P(X>l)$. Every geometric random variable is memoryless, and in fact these are the only examples of memoryless random variables on $\N$.
\end{lemma}

\begin{proof}
Let us start by proving that the geometric random variable satisfies the memoryless property. First, notice that if $\P(X = 1) = 1$, then $X$ is a degenerate geometric random variable with $p = 1$. So we can suppose that we work in the case $\P(X > 1) > 0$.

Let us check that a geometric r.v. is memoryless. First, it is easy to check that for a geometric random variable $X$, we have that $\P(X > l) = (1-p)^l$ for some $p \in (0,1]$. As by the definition of conditional probability $$\P(X > k+l|X > k) = \frac{\P(X > k+l)}{\P(X > k)},$$ 
it follows that $\P(X > k+l|X > k) = (1-p)^{k+l-k} = (1-p)^l = \P(X > l)$ as desired.

Now, let us show that each random variable satisfying the memoryless property has the law of a geometric random variable. Again if $\P(1) = 1$, we are done. Otherwise we can write
$$\P(X > 1+ l| X > 1)\P(X > 1) = \P(X > 1+l).$$
As for a memoryless random variable $\P(X > l) = \P(X > 1+ l| X > 1)$, we obtain
$$\P(X > l)\P(X>1) = \P(X > l+1).$$
Thus inductively $\P(X > l) = \P(X > 1)^l$ and hence $X$ is a geometric random variable of parameter $p = 1 - \P(X > 1)$.
\end{proof}~\\

\noindent \textbf{Poisson random variable}\\

Poisson was a French mathematician who has famously said that the life is good for only two things - mathematics and teaching mathematics. His random variables come up quite often. 

The Poisson random variable is a discrete random variable with values in $\{0\} \cup \N$ and taking the value $k$ with probability $$e^{-\lambda}\frac{\lambda^k}{k!}$$
for some $\lambda > 0$. We denote this distribution by $Poi(\lambda)$. Poisson random variables describe occurrences of rare events over some time period, where events happening in any two consecutive time periods are independent. For example, it has been used to model
\begin{itemize}
	\item The number of visitors at a small off-road museum.
	\item More widely, the number of stars in a unit of the space.
	\item Or more darkly, it was used to also model the number of soldiers killed by horse kicks in the Prussian army.
\end{itemize}

One way we see the Poisson r.v. appearing is via a limit of the Binomial distribution if the success probability $p$ scales like $1/n$:

\begin{lemma}[Poisson random variable as the limit of Binomials]
Consider the Binomial distribution $Bin(n,\lambda/n)$. Prove that as $n \to \infty$ it converges to $Poi(\lambda)$ in the sense that for every $k\in \{0\} \cup \N$, we have that $$\P(Bin(n,\lambda/n) = k) \to e^{-\lambda}\frac{\lambda^k}{k!}.$$
\end{lemma}

\begin{proof}
By definition, for any fixed $n \in \N$ and $k\in \{0\} \cup \N$, we have
$$\P(Bin(n,\lambda/n) = k) = {n \choose k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}.$$
Using
$${n \choose k} = \frac{n!}{(n-k)!k!} = \frac{n(n-1)\cdots (n-k+1)}{k!}.$$
we can write
$$\P(Bin(n,\lambda/n) = k) = \frac{\lambda^k}{k!}\left(1-\frac{\lambda}{n}\right)^n \frac{n(n-1)\cdots (n-k+1)}{n^k}\left(1-\frac{\lambda}{n}\right)^{-k}.$$
But now as $n \to \infty$
$$\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda}.$$
Moreover, for any fixed $t > 0$ also $\frac{n-t}{n} \to 1$ as $n \to \infty$ and hence
$$\frac{n(n-1)\cdots (n-k+1)}{n^k} \to 1$$
and
$$\left(1-\frac{\lambda}{n}\right)^{-k} = \left(\frac{n-\lambda}{n}\right)^{-k}\to 1,$$
proving the lemma.\end{proof}

To connect this to the occurrences of rare events described before, one could think as follows. Suppose we try to model the number of arrivals over time window $[0,1]$, say one year in a distant location. We then cut a time-window $[0,1]$ into $n$ equal time-segments of length $1/n$ with $n$ large, say into 365 days, so that we can suppose that at each time-segment, say each day, there is at most one arrival. In this case we can describe the arrival or non-arrival using $Ber(p)$ or $1_E$ for some event $E$. If we further suppose that all days are alike, we can take this parameter $p$ to be the same for all time-segments of the same length, e.g. for all days. Moreover, if we suppose that an arrival in one time-segment does not influence arrivals in other time-intervals, we can assume that all events $E$ corresponding to different time intervals are mutually independent. Hence the total number of arrivals is the number of independent events happening, when the event probability is $p$ - we saw above that this gives a $Bin(n,p)$ random variable. But now, if you check carefully the proof above, you see that if $p$ is not of the form $\lambda/n$ for some $\lambda > 0$, then in fact the number of events will either go to infinity or go to zero - i.e. to have a non-trivial random variable in the limit $n \to \infty$, we are forced to set $p = \lambda/n$.

Poisson random variables also behave very well under taking independent copies and taking random subsets of them:

\begin{exo}[Poisson random variables]
	Let $X_1 \sim Poi(\lambda_1)$ and $X_2 \sim Poi(\lambda_2)$ be two independent random variables defined on the same probability space. 
	\begin{itemize}
		\item Prove that then $X_1 + X_2$ is also a Poisson random variable with parameter $\lambda_1 + \lambda_2$. 
		\item Let now $Y_1, Y_2, \dots$ be independent $Ber(p)$ random variables defined on the same probability space. Prove that $X := \sum_{i = 1}^{X_1} Y_i$ also has the law of $Poi(p\lambda)$ and $X_1 - X$ has the law of $Poi((1-p)\lambda)$ and is independent of $X$.
	\end{itemize}
 %	Now, we consider what is called a Poisson point process on $\N$: This is a collection of i.i.d. random variables $(X_i)_{i \in \N}$ where each $X_i  \sim Poi(\lambda)$. For example you can think that some Newtonian apples fall on each integer. What is the law of the total number of apples on a finite set $S \subseteq \N$? Now colour every apple independently red with probability $p$ and green with probability $1-p$ - i.e. every apple is ripe with probability $p$. Prove that restricting to only ripe / green apples also gives a Poisson point process on $\N$ and that moreover these processes are independent.
	
%	Finally, let $i_1$ be the first index of $\N$, which contains at least one apples, let $i_2$ be the second index that contains at least one apple etc. What is the distribution of the vector $(i_1, i_2-i_1, i_3-i_2, \dots)$?

\end{exo}
\subsubsection{How to choose my distribution - the maximum entropy principle}

We have now seen several examples of discrete random variables with special properties, which may or may not have sounded relevant and were a bit different for each of the examples. A generic question is the following. Suppose we want to model some statistical phenomena using a random variable. From the experiments or theory we can deduce some weak constraints on the probability distribution of the variable - for example the support of the distribution, i.e. which values it takes, and maybe some other parameters obtained from repeated experiments like some sort of average. The question is: which probability distribution should we choose as our model under these constraints? 

Intuitively, we would like to choose a distribution that takes into account these constraints and nothing more. Already Laplace used such an argument: his principle of insufficient reason says that if we only know that we have $n$ outcomes, we should assign each the probability $1/n$. It comes out that somehow the right generalization of this principle of insufficient reason is the principle of maximum entropy - we should choose the probability distribution with maximal entropy, given the constraints. This feels at least intuitively natural, as we are then maximizing our surprise or uncertainty about what is happening. The principle of maximum entropy was introduced by E. T. Jaynes in the 1950s. 

To state this, let us first introduce the concept of entropy in the realm of discrete random variables. This rich concept is also interesting in itself and I believe you have already met it in your course of computer science as introduced by Shannon, although its origins go back much further in thermodynamics. In essence the entropy of a random variable is a way to formalise the notion of  information content that we learn by observing an outcome. 

\begin{defn}[Discrete (Shannon) entropy]
	For a discrete random variable $X$ with outcome set $S$ we define the entropy $$H(X) := -\sum_{s \in S} \P(X = s) \log \P(X = s).$$ We also call it the entropy of the probability distribution $\P$.
\end{defn}

The mentioned link to information content can be thought of in two steps:
\begin{itemize}
	\item First, to each event $E$, and in particular to each outcome $\{X = s\}$ we assign a measure of information content or surprise: $-\log \P(E)$. We like this precise measure more than just $\P(E)$ basically because $\log$ is additive under products and hence the information content of two independent events adds up.
 \item Then to measure the information content over all outcomes we take the weighted average of $-\log \P(X = s)$ as in the definition above. Such a weighted average is called the mathematical expectation, as we will shortly see.
\end{itemize}

\begin{rem}
Often in computer science / information theory one rather uses $\log_2$ instead of $\log$ - this in some sense is just a choice of units for the information content.
\end{rem}

\begin{eg}
One can directly check that for the uniform distribution on $n$ points the entropy is $H = \log n$. Indeed then $\P(X=s) = 1/n$ for any $s$ and the claim follows.
\end{eg}

Notice that although we defined the discrete entropy for a random variable, it does not depend on the exact values of the random variable - only on how many values with which probability it takes. In particular for example it is direct to see the following Lemma.

\begin{lemma}
	For any real-valued discrete random variable we have that $H(X) = H(aX +b)$.
\end{lemma}

Further, the reason of choosing the logarithm in the definition, boils down to the following facts:

\begin{lemma}
The entropy is non-negative: $H(X) \geq 0$. Moreover, for independent discrete random variables $X, Y$, we have that $H(X,Y) = H(X) + H(Y)$. 
\end{lemma}

\begin{proof}
The first part is evident from the fact that $- p \log p \geq 0$ for all $p \in [0,1]$ \footnote{If you wish we take the convention that $0 \log 0 = 0$, which of course also makes a lot of sense by taking $x > 0$ and letting it tend to $0$.}, The proof of the second part is a direct computation on the example sheet.
\end{proof}

There are many ways to give mathematical characterisations of entropy, i.e. to give a set of intuitive conditions for a measure of information content such that they uniquely characterise the entropy functional. We will not do this in our course, but rather explain a property that makes entropy appear - asymptotic equipartition property.

In this respect, consider i.i.d. non-trivial discrete random variables $X_1, X_2, \dots, X_n$ taking each of the values $s \in S$ with positive probability. Then for each sequence of outcomes $(s_1, s_2, \dots, s_n)$ we can calculate the probability $\P(X_1 = s_1, \dots, X_n = s_2)$. By independence, this is given by $\Pi_{i=1}^n \P(X_i = s_i)$. As each of $\P(X_1=s) \in (0,1)$, these probabilities decay exponentially with $n$, i.e. should be roughly of the form $\exp(cn)$. Is this the case, and how does this $c > 0$ behave? For a given sequence, this will clearly depend on the exact sequence $(s_1, s_2, \dots, s_n)$, so one cannot expect an answer in full generality. 

However, one can determine this exponent for a typical outcome sequence and it is given by the entropy of $X_1$:

\begin{thm}[Asymptotic equipartition property]
Let $X_1, X_2, \dots$ be  i.i.d. non-trivial discrete random variables, taking values $s \in S$ with positive probability. Denote by $p(s_1, s_2, \dots, s_n)$ the probability of the sequence of outcomes $(s_1, s_2, \dots, s_n)$. Then $\P(|-\frac{1}{n}\log p(X_1, \dots, X_n) - H(X_1)| > \eps)$ converges to $0$ as $n\to\infty$.
\end{thm}

We will be able to give a simple proof and even a stronger statement of this result towards the end of the course, but I encourage you already to try it out now - this maybe also helps to understand how the notion we are about to introduce simplify our thinking. \\

As said, entropy has many contexts and many uses, but for us the aim was to help select probability laws on discrete sets. Here are two results in this direction.

\begin{lemma}[Uniform distribution has maximum entropy]\label{lem:umax}
	Consider all probability distributions on $n$ points. Among such distributions, the uniform distribution is the unique maximum entropy distribution.
\end{lemma}

\begin{proof}
	Let $Q = (q_i)_{i = 1 \dots n}$ denote a probability distribution on $n$ points. We want to prove that 
	$$H(Q) = - \sum_{i=1}^n q_i \log q_i \leq \log n.$$
	We use the fact that $\log x$ is concave on $[0,1]$. Thus we have that 
	$$H(Q) = \sum_{i=1}^n (q_i \log \frac{n}{q_i}) - \log n \leq \log (\sum_{i=1}^n \frac{q_in}{q_i}) -\log n = \log n^2 - \log n = \log n,$$
	where we also used that $\sum_{i=1}^n q_i = 1$. Moreover, the equality holds only if $q_i = 1/n$ for all $i$, so in fact the uniform distribution is the unique maximum entropy distribution
\end{proof}

Similarly we can single out the geometric random variable among all distributions with outcomes in $\N$, however with one extra constraint. Namely, it is maximum entropy distribution on $\N$ among those distributions for which the so called mean or expectation is finite: $\sum_{n \geq 1} n \P(X = n) < \infty$.


%For the exos?
%\begin{lemma}
	%We have that $H(X,Y) = H(X) + H(Y|X)$, where $H(Y|X) := \sum_{\omega_x}p(\omega_x) H(Y|X = \omega_x)$ is the conditional entropy.
%\end{lemma}

%\begin{proof}
	%Let us check this computation. We have that
	%$$H(X,Y) = - \sum_{\omega_x, \omega_y} p(\omega_x, \omega_y) \log_2 p(\omega_x, \omega_y).$$
	%Denote by $p(\omega_y|\omega_x)$ the conditional probabilities. Then $p(\omega_x, \omega_y) = p(\omega_x)p(\omega_y|\omega_x)$ and we have that
	%$$H(X,Y) = -\sum_{\omega_x} (p(\omega_x) \log_2 p(\omega_x)) \sum_{\omega_y}p(\omega_y|\omega_x) - \sum_{\omega_x} %p(\omega_x)\sum_{\omega_y} p(\omega_y|\omega_x) \log_2 p(\omega_y|\omega_x).$$
	%But $\sum_{\omega_y}p(\omega_y|\omega_x) = 1$, and thus the claim follows.
%\end{proof}

%As a simple corollary we also get our first entropy inequality:

%\begin{cor}
	%We have that $H(X,Y) \leq H(X) + H(Y)$ with equality iff $X$ and $Y$ are independent.
%\end{cor}	

%\begin{proof}
	%We already saw that the independent case gives an inequality. So it suffices to prove a strict inequality for all other cases. From the lemma above, we have that $H(X,Y) = H(X) + H(Y|X)$. So it suffices to show that 
	%$H(Y|X) < H(Y)$ whenever $X, Y$ are not independent. To do this first rewrite
	%$$-\sum_{\omega_x}p(\omega_x)\sum_{\omega_y} p(\omega_y|\omega_x) \log_2 p(\omega_y|\omega_x) = \sum_{\omega_y}\sum_{\omega_x}p(\omega_x) (-p(\omega_y|\omega_x) \log_2 p(\omega_y|\omega_x)).$$
	%Now, notice that $-x \log x$ is strictly concave on $[0,1]$. Thus, as $\sum_{\omega_x} p(\omega_x) = 1$, we can bound
	%$$\sum_{\omega_x}p(\omega_x) \left[-p(\omega_y|\omega_x) \log_2 p(\omega_y|\omega_x)\right]$$
	%from above by
	%$$-\left[\sum_{\omega_x}p(\omega_x)p(\omega_y|\omega_x)\right] \log_2 \left[\sum_{\omega_x}p(\omega_x)p(\omega_y|\omega_x)\right] = - p(\omega_y) \log_2 p(\omega_y).$$
	%As the equality holds only if $p(\omega_y|\omega_x)$ is equal for all $y, x$, the corollary follows.
%\end{proof}


\subsection{Continuous random variables}

Recall that we called a random variable $X$ continuous if $F_X$ was continuous, i.e. without any jumps. From Lemma \ref{lem:cumvsrv} it follows that $\P(X = x) = 0$ for all $x \in \R$.
Most often continuous random variables arise via what is called a density function and this is also how we will usually construct them. 

%We have seen that Riemann integral does not go well with measure theory - for example the set $\Q$ is a Borel set in $\R$, however $1_\Q$ is not Riemann-integrable. Thus it will be much more convenient to use a different notion of integral, called the Lebesgue integral. We will introduce it in Section 3 to the extent we need it, though it will be properly introduced only in Analysis IV. For now, you should just know that for any function $f$ that is Riemann integrable, its Lebesgue integral and Riemann integral agree.

\begin{defn}[Continuous r.v. with density]
	Let $X$ be a random variable and $f_X: \R \to \R$ be a non-negative integrable function with $\int_\R f_X(x) dx = 1$. Then we say that a r.v. $X$ has density $f_X$ if for every $x \in \R$
	$$F_X(t) = \int_{-\infty}^t f_X(x)dx.$$\footnote{You might have already heard that there are several notion of an integral. Here the natural integral to use would be Lebesgue integral as then one can integrate over all Borel sets, which as you may have seen, is not possible for the Riemann integral. But in fact for all the examples here thinking of Riemann integral is quite sufficient.}

\end{defn}

\begin{rem}
We remark straight away that there are also continuous random variables without a density (see starred section of the exercises).
\end{rem}

%In particular, next to the Riemann integral stands the Lebesgue integral. So what do we mean by integrable?

%We have seen that Riemann integral does not go well with measure theory - for example the set $\Q$ is a Borel set in $\R$, however $1_\Q$ is not Riemann-integrable. So it would be much more convenient to use the notion called the Lebesgue integral that you meet fully in Analysis IV and partly later on in this course. However, for now, it is really no restriction for us if \textit{for the sake of precision we just consider Riemann integrals}. 
%In fact, all examples of densities we will see are Riemann integrable, so this is not a real restriction. Moreover, none of the results change become untrue when you come back and change Riemann integrals for Lebesgue integrals - in fact, as you will see next semester, for any function $f$ that is Riemann integrable, its Lebesgue integral and Riemann integral agree.}

Let us now look at the definition more closely. First, it is important to check the definition even makes sense, i.e. that the $F_X$ defined actually is a cumulative c.d.f.:

\begin{exo}
Consider a non-negative Riemann integrable function $f_X$ with $\int_\R f_X(x) dx = 1$. Define $F_X(x) :=\int_{-\infty}^x f_X(x)dx$. 
\begin{itemize}
\item Prove that $F_X$ is a cumulative distribution function. 
\item Prove that if two random variables have the same density function, they have the same law
\item Prove that given $F_X$, there is at most one continuous $f_X$ such that $F_X(t) :=\int_{-\infty}^t f_X(x)dx$. 
\item Give examples to show that $f_X$ is however not uniquely defined by $F_X$. 
\end{itemize}
\end{exo}

%\begin{rem}
%Given the exercise, one might think that $f_X$ is always unique. This is however not the case. For example, $1_{x \in [0,1]}$ is a density function, but so is $1_{x \in (0,1]}$ and both of them give rise to the same random variable. Even changing the value of $f_X$ at any finite number of points does not influence the Riemann integral, and hence also not the density. Still, this arbitrariness has no consequences - we have already seen that $F_X$ defines uniquely the law of the random variable. Moreover, most often we will consider r.v. with continuous density, and we saw that then we canonically fix a $f_X$. Thus we will still often talk of 'the' density, especially as the density functions of interest for us will be either continuous or piece-wise continuous with a finite number of jumps.
%\end{rem}

Further, let us look at an interpretation. Using Lemma \ref{lem:cumvsrv} and the remark above that $\P(X = x ) = 0$ for every $a < b$, we can also write 
$$\P(X \in (a,b)) = \P(X \in [a,b]) = \int_{a}^b f_X(x)dx.$$
it is important to notice that $f_X$ does not give you the probability of $\{X = x\}$ at each point - we already saw that for continuous random variables this probability is $0$ for all $x \in \R$.
However, taking $b = a+\eps$, we can still obtain an interpretation of $f_X$, explaining why it is called the density function. Indeed, if for example $f_X$ is continuous, we can write
$$\P(X \in (a,a+\eps)) = \int_a^{a+\eps} f_X(x)dx = \eps f_X(a) +o(\eps),$$
and thus one can think of $\eps f_X(a)$ as of the probability in being in the interval $(a,a+\eps)$. In particular, notice that $\eps^{-1}\P(X \in (a,a+\eps)) \to f_X(a)$ as $\eps \to 0$. This is of course related to the Fundamental theorem of calculus, which in the case of continuous $f_X$ tells us that $F_X'(x) = f_X(x)$. 


%One should make further remarks:
%\begin{itemize}
%	\item As you will see in the starred section of exercises, not every continuous random variable has a density.
%	\item In fact, by introducing the notion of Lebesgue integral, one could generalize the notion of density for a larger class of $f$. However, this is really not important here - the continuous random variables we will have piecewise continuous densities.
%	\item To show that random variables are equal, i.e. have the same law, it always suffices to show that their cumulative distribution functions are equal. However, notice that if two random variables have densities, it also suffices to show that their densities agree, as it then follows that their c.d.f.-s also agree.
%\end{itemize}
Let us now look at some examples. From the exercise above we see that to describe a continuous random variable with density it suffices to give the density function: an integrable non-negative function with total integral 1. \\


\noindent \textbf{Uniform random variable on $[a,b]$}\\
A random variable $U$ with density $f_U(x) = \frac{1}{b-a}1_{[a,b]}$ is called a uniform random variable on the interval $[a,b]$ and is denoted sometimes $U = U_{[a,b]}$. We have already met the uniform random variable on $[0,1]$ - as expected its law $\P_U$ is equal to the uniform / Lebesgue measure on $[0,1]$, considered as a probability measure on $\R$. It's c.d.f is given by $F_U(x) = 1_{0 \leq x}\min\{x,1\}$. You can also think of it as the limit of discrete uniform random variables taking values in $\{i/n: i = 1 \dots n\}$ - we saw one way of making it precise on Exercise sheet 7. \\

\noindent \textbf{Exponential random variable}\\
Let $\lambda > 0$. The random variable $X$ with density $f_X(x) = \lambda e^{-\lambda x}1_{x \geq 0}$ is called the exponential random variable of parameter $\lambda$, and its law is denoted sometimes $Exp(\lambda)$. (We will check on the exercise sheet that the total mass is 1). In this case you can think of the exponential random variable as a continuous friend of the geometric random variable, as it also satisfies the memoryless property:

\begin{exo}[Exponential r.v. is the only memoryless random variable]
	We say that a continuous a random variable $X$ satisfying $\P(X > 0) = 1$ is memoryless if for every $x, y  > 0$ we have that $\P(X > x+y| X > y) = \P(X>x)$. Prove that the exponential random variable is memoryless. Moreover, prove that every continuous memoryless random variable has the law of the exponential random variable. 
\end{exo}

As geometric random variables, exponential random variables too are related to waiting times, just the underlying process is no longer in discrete time (like a sequence of tosses) but continuous time (like waiting for the next call from a friend). We will be able to make some more precise statements later in the course.\\

%\noindent \textbf{Gamma random variable}\\
%Let $\lambda > 0$ and $t > 0$. Denote by $\Gamma(t) = \int_0^\infty x^{t-1} e^{-x}dx$ the Euler gamma function. 
%The random variable $X$ with density $$f_U(x) = \frac{1}{\Gamma(t)} \lambda^t x^{t-1}e^{-\lambda x}1_{x \geq 0}$$ is called a Gamma random variable of parameters $\lambda$ and $t$. Again it needs to be checked that the total mass really is 1.

%Notice that if we take $t = 1$, we have the exponential variable of parameter $\lambda$. Moreover, if we add up independent exponential random variables, we again obtain a Gamma random variable. This will be on the example sheet.

%Maybe the most frequent Gamma random variable is the case $\lambda = 1/2$ and $t = d/2$, when one talks of a  chi-square distribution of $d$ parameters. This distribution will be important in statistics, and this is the main reason for introducing it here...though we will see more of it coming up very soon!\\

\noindent \textbf{Gaussian random variable}\\

Maybe the most important example of a random variable is that of a normal or Gaussian random variable. Given two parameters $\mu \in \R$ and $\sigma \in \R$, we say that $N$ has the law of a normal random variable of mean $\mu$ and variance $\sigma^2$, denoted $N \sim \Nn(\mu, \sigma^2)$ if its density is given by
$$f_N(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp(-\frac{(x-\mu)^2}{2\sigma^2}).$$
We call the law $\Nn(0,1)$ the standard normal random variable, or the standard Gaussian. Normal laws come up everywhere because of the so called Central limit theorem. A weak version of it could be vaguely stated as follows: 
\begin{itemize}
	\item Let $X_1, X_2, \dots$ be a sequence of i.i.d. random variables such that $X_i$ has the same law as $-X_i$ and moreover, each $X_i$ is bounded in the sense that there is some $C > 0$ with $\P(X_i < C) = 1$. Let $S_n = \sum_{i = 1}^n X_i$. Then in the limit $n \to \infty$ we have that $\frac{S_n}{\sqrt{n}}$ becomes a normal random variable: for every interval $(a,b)$, we have that $\P(\frac{S_n}{\sqrt{n}} \in (a,b)) \to \P(N \in (a,b)$, where $N$ is a Gaussian random variable.
\end{itemize}
For example in physics experiments often we rarely expect to get the 'exact' value, but rather it comes with an error. This error is assumed to be a sum of many independent smaller errors, and thus, unless there is some bias that has not been accounted for, the observed values will have a normal distribution around the actual value. 

We will prove a version of this theorem towards the end of the course, after having developed more tools to work with random variables. There is a first version of this in the starred section of the exercises.

It is common to mention here that although the normal random variable is the most used one, its cumulative distribution function - that has earned its own notation $\Phi_{\mu, \sigma^2}$ - given as always by
$$\Phi_{\mu, \sigma^2}(x) = \P(N \leq t) = \frac{1}{\sqrt{2\pi \sigma^2}}\int_{-\infty}^t \exp(-\frac{(x-\mu)^2}{2\sigma^2})dx$$
does not admit a more explicit formula. So in the old days one had to really check a long table with values to give a numerical answer for, say, $\P(N > 12)$ or $\P(|N| < 200)$. I suspect there might be more modern ways now...

One of the other important aspects of Gaussians are their intimate relation to linear algebra: Gaussian random variables and random vectors behave extremely well under linear transformations, making them already for this reasons central to many probabilistic models.

Here is a simple lemma in this spirit giving also a meaning to $\mu$ and $\sigma^2$ as a shift and scaling:

\begin{lemma}
Let $X_{\mu, \sigma^2}$ be a Gaussian random variable. Further Let $X$ be a standard Gaussian. Then $\sigma X + \mu$ has the same law as $X_{\mu, \sigma^2}$.
\end{lemma}

\begin{proof}
This is a direct computation. Let us denote by $F_{\mu, \sigma^2}$ the c.d.f. of $X_{\mu, \sigma^2}$ and by $F_{X}$ the c.d.f. of $X$. Pick $\sigma > 0$ and let us calculate the c.d.f. of $\sigma X + \mu$:
$$F_{\sigma X + \mu}(t) = \P(\sigma X + \mu \leq t) = \P(X \leq (t-\mu)/\sigma) = F_{X} = \int_{-\infty}^{(t-\mu)/\sigma}\frac{1}{2\pi}\exp(-x^2/2)dx.$$
We now make a change of variable $y = \sigma x + \mu$ to see that the right hand side equals
$$\int_{-\infty}^t \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-(y-\mu)^2/2\sigma^2) = F_{\mu, \sigma^2}(t).$$
\end{proof}


\subsection{Random vectors}

We already saw in the notes and on the example sheet that often several random variables come up in the same probabilistic situation and are naturally defined on the same probability space. So far we were looking mainly at their individual laws, or the situation when they were independent. But this is not always the case. When one starts being interested in the joint behaviour of several random variables, it is sometimes useful to think in terms of random vectors:

\begin{defn}[Random vectors and marginal laws]
Consider a probability space $(\Omega, \F, \P)$. We say that $(X_1, X_2, \dots, X_n)$ is a random vector if and only if each of $X_1, X_2, \dots, X_n$ is a random variable. The law $\P_{X_i}$ of each r.v. $X_i$ is called its marginal law.
\end{defn}

Marginal laws are just the individual laws of random variables $X_i$ that appear as components of a random vector and that we have been discussing so far. We know how to describe those. Yet they don't encode the relation between the random variables. 

For example consider on the one hand $(X_1, X_2)$, where both $X_1$ and $X_2$ encode independent fair coin tosses. On the other hand, consider $(X_1, \widetilde X_2)$, where $X_1$ is a fair coin toss, but $\widetilde X_2$ is heads when $X_1$ is tails and $\widetilde X_2$ is tails if $X_1$ is heads. Then the marginal laws of the vector $(X_1, X_2)$ and $(X_1, \widetilde X_2)$ are the same (why?), yet they clearly describe very different situations! 

So how can we mathematically encode this relation between the random variables? In fact, to look at joint laws, it is more natural to look at $(X_1, \dots, X_n)$ not as just a vector of $\R$-valued random variables, but rather as a $\R^n$-valued random variable:

\begin{lemma}[Joint law of random vectors]\label{rvmes}
Let $\overline X = (X_1, \dots, X_n)$ be a random vector defined on $(\Omega, \F, \P)$. Then $(X_1, \dots, X_n)$ as a vector is a $(\R^n, \F_B)$-valued random variable i.e. the map $\omega \to (X_1(\omega), \dots, X_n(\omega))$ is measurable from $(\Omega, \F)$ to $(\R^, \F_B)$. In particular a random vector induces a probability measure $\P_{\overline X}$ on $(\R_n, \F_B)$ called the joint law of the vector $\overline X$. 

In the other direction, any $(\R^n, \F_E)$-valued random variable gives rise to a random vector according to the definition above.
\end{lemma}

We will not prove this lemma, but just remark that the underlying question here is measurability: does measurability of each component as a function $(\Omega, \F) \to (\R, \F_E)$ guarantee the measurability of the function $(\Omega, \F) \to (\R^n, \F_E)$ and vice-versa. This should remind you of your topology course and vector-valued continuous functions \footnote{Indeed, the statement of interest here is the following. If $(\Omega, \F)$ and $((\Omega_i,\F_i))_{1 \leq i \leq n}$ are measurable spaces, then the map $f: (\Omega, \F) \to (\Pi_{1 \leq i \leq n}\Omega_i, \F_\Pi)$ is measurable if and only if for every $i = 1 \dots n$ the map $f_i = p_i \circ f$ mapping $(\Omega, F) \to (\Omega_i, \F_i)$ is measurable. Compare this to the following statement from topology: if $f_i: (X, \tau_X) \to (Y_i, \tau_{Y_i})$ are continuous, then so is $f: (X, \tau_X) \to (Y_1 \times \dots \times Y_n, \tau_\Pi)$ given by $f = (f_1, \dots, f_n)$.}.

%\begin{lemma}
%Let $(\Omega, \F)$ and $((\Omega_i,\F_i))_{1 \leq i \leq n}$ be measurable spaces. Then the map $f: (\Omega, \F) \to (\Pi_{1 \leq i \leq n}\Omega_i, \F_\Pi)$ is measurable if and only if for every $i = 1 \dots n$ the map $f_i = p_i \circ f$ mapping $(\Omega, F) \to (\Omega_i, \F_i)$ is measurable (here $p_i$ is the projection map to the $i$-th coordinate).
%\end{lemma}

%\begin{lemma}
%Let $\Phi: (\R^n, \tau_E) \to (\R^m, \tau_E)$ be any continuous function and $\overline X$ a random vector in $\R^n$ defined on some probability space $(\Omega, \F, \P)$. Then $\Phi(\overline X)$ is a random vector in $\R^m$, defined on the same probability space.
%\end{lemma}

%\begin{proof}
%This is on the exercise sheet.
%	We saw that in fact $\overline X$ is a measurable function from $(\Omega, \F)$ to $(\R^n, \F_E)$. But now $\Phi$ is continuous from $(\R^n, \tau_E)$ to $(\R^m, \tau_E)$ and in particular measurable as a map from $(\R^n, \F_E)$ to $(\R^m, \F_{\R^m})$. 
	
%	Thus, as a concatenation of measurable maps is measurable (check!), we conclude that $\Phi(\overline X)$ is a $(\R^m, \F_{\R^m})$-valued random variable defined on $(\Omega, \F, \P)$, and hence by Lemma \ref{rvmes} a random vector.
%\end{proof}

This set-up allows us to quickly prove the following basic result:

\begin{lemma}
Let $\overline X$ be a random vector in $\R^n$ and $\overline a$ any fixed vector in $\R^n$. Then $\sum_{i=1}^n a_i X_i$ is a random variable. Also $\Pi_{i = 1}^n X_i$ is a random variable.
\end{lemma}

On the exercise sheet you will prove by hand that the sum of two random variables $X_1$ and $X_2$ is a random variable - and you will see, it requires patience!

\begin{proof}
By above $\overline X$ is a measurable function from $(\Omega, \F)$ to $(\R^n, \F_B)$. But now $\Phi: \R^n \to \R$ given by $\Phi(\overline x) = \sum_{i = 1}^n a_i x_i$ is continuous from $(\R^n, \tau_B)$ to $(\R, \tau_B)$ and in particular it is measurable.

But it is a direct check a concatenation $f_2 \circ f_1$ of measurable maps $f_1: (\Omega, \F) \to (\Omega_1, \F_1)$, $f_2: (\Omega_1, \F_1) \to (\Omega_2, \F_2)$ is $(\Omega, \F) \to (\Omega_2, \F_2)$-measurable. Thus $\sum_{i=1}^n a_i X_i = \Phi(\overline X)$ is measurable from $(\Omega, \F)$ to $(\R,\tau_E)$ and hence a random variable.
\end{proof}


\subsubsection{Joint cumulative distribution function}
Similarly to the case of a single random variable, random vectors can be characterised by a certain family of functions. 

\begin{defn}[Joint cumulative distribution function]
Any function $F: \R^n \to [0,1]$ is called a joint cumulative distribution function (c.d.f.), if it satisfies the following conditions:
	\begin{enumerate}
		\item $F$ is non-decreasing in each coordinate.
		\item $F(x_1, \dots,x_n) \to 1$ when all of $x_i \to \infty$.
		\item $F(x_1, \dots, x_n) \to 0$, when at least one of $x_i \to -\infty$.
		\item $F$ is right-continuous, meaning that for any sequence $(x_1^m, \dots, x_n^m)_{m \geq 1}$ such that for all $m\geq 1$ we have that $x_i^m \geq x_i$, it holds that $F(x_1^m, \dots, x_n^m) \to F(x_1, \dots, x_n)$.
	\end{enumerate}
\end{defn}

Notice that for $n = 1$ we are back to the case of individual c.d.f. Moreover, if we send any $n-1$ coordinates to infinity, then we also obtain the c.d.f. of the remaining coordinate:
$$F_{X_i}(x_i) = F(\infty, \dots, \infty, x_i, \infty, \dots, \infty).$$

As mentioned, each random vector uniquely identifies a joint c.d.f. and vice-versa. One part of the proposition is again easy:

\begin{prop}[Joint c.d.f.s of random vectors]
Let $\overline X :=(X_1, \dots, X_n)$ be a random vector defined on some probability space $(\Omega, \F, \P)$. Then 
$$F_{\overline X}(x_1, \dots, x_n) := \P_{\overline X}(X_1 \leq x_1, \dots, X_n \leq x_n)$$
gives rise to a joint cumulative distribution function. 
\end{prop}

\begin{proof}
    This is left as an exercise.
\end{proof}

However, the existence and uniqueness part given the joint c.d.f. is technical and thus admitted.

\begin{thm}[Existence and uniqueness of random vectors via joint c.d.f. (admitted)]
Any joint c.d.f. gives rise to a unique joint law of a random vector.
\end{thm}


Again, random vectors give us mainly a clearer way of looking at things. We can for example now rephrase independence:

\begin{lemma}[Independence using joint c.d.f.]\label{lem:indcdf}
Consider a random vector $\overline X = (X_1, \dots, X_n)$ defined on some probability space. Then $X_1, \dots, X_n$ are mutually independent if and only if
	$F_{\overline X}(x_1, \dots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2) \cdots F_{X_n}(x_n)$ for all $\ol x = (x_1, \dots, x_n) \in \R^n$. 
\end{lemma}

Many relevant examples come actually from joint laws, where each marginal law is different. However, the case of Gaussian vectors is well-spread in machine learning / statistics and elsewhere. To state this, we first define the notion of density for random vectors.

\begin{defn}[Random vectors with density]
Let $\overline X = (X_1, \dots, X_n)$ be a random vector and let $f_{\overline X}$ be a non-negative integrable function \footnote{Again, you can assume we are using the Riemann integral. In fact one could give a more natural definition via Lebesgue integral, but this one works fine too.} from $\R^n \to [0,\infty)$ with total integral equal to $1$. Then we say that $f_{\overline X}$ is the joint density of $\overline X$ if and only for any box $(a_1, b_1] \times \dots (a_n,b_n]$ 
\begin{equation}\label{eq:densboxes}
\P_{\bar X}(X_1 \in (a_1, b_1], \dots,X_n \in (a_n, b_n] ) = \int_{(a_1, b_1] \times \dots \times (-a_n,b_n]}f_{\overline X}(\bar x )d\bar x.
\end{equation}
\end{defn}


Similarly to the 1d case, we also have the interpretation of this density as representing the probability of being in an infinitesimal neighbourhood around a point $\overline t = (t_1, \dots, t_n)$. Indeed, if $f_{\overline X}$ is continuous, then you can check that we have
\begin{equation}\label{rvectinf}
\P_{\overline X}((X_1, \dots, X_n) \in (t_1,\dots,t_n) + [-\eps/2,\eps/2]^n) = f_{\overline X}(t_1, \dots, t_n)\eps^{n} + o(\eps^n).
\end{equation}
Further, we can let $a_i \to -\infty$, for every $(t_1, \dots, t_n) \in \R^n$ set 
$$F_{\bar X}(t_1, \dots, t_n) := \int_{(-\infty,t_1] \times \dots \times (-\infty,t_n]} f_{\overline X}(\bar x )d\bar x$$
and verify that this indeed gives rise to a c.d.f. Hence as joint c.d.f. characterise the joint law of random variables, can define laws of random vectors via their density function.

We can now state the key example: \\


\noindent \textbf{Gaussian random vector.} The Gaussian (or also normal) random vector is denoted by $\Nn(\overline \mu,C)$, where $\overline \mu$ is a vector in $\R^n$ and $C$ positive definite symmetric $n \times n$ matrix. We will call $\overline \mu$ the mean of the Gaussian vector, and the matrix $C$ the covariance matrix -- we will get to the reasons for this vocabulary in a few lectures time. The density of the Gaussian random vector is given by:
$$f_{\overline X}(x_1, \dots,x_n) = \frac{1}{(2\pi)^{n/2}\sqrt{\det(C)}} \exp(-\frac{1}{2}(\overline x - \overline \mu)^TC^{-1}(\overline x - \overline \mu)).$$
When $\overline \mu = 0$ and $C$ is the $n \times n$ identity matrix $I_n$, we call the law $\Nn(0,I_n)$ the standard Gaussian in $\R^n$. In fact all Gaussian vectors in $\R^n$ are given by just linear transformations of the standard Gaussian - this is what also makes the Gaussians ubiquitous, they behave very well under linear transformations.\\

\section{Mathematical expectation}

We will continue working with random variables and start looking at several different characteristics or properties of their law, based on the concept of mathematical expectation. In many senses mathematical expectation of a probability distribution is the number that one should give if asked for one single number to describe the distribution.

Mathematical expectation, or just 'expectation', or 'expected value', or 'mean' is a fancy name for taking the average in context of probability measures. 
Its introduction in the early times of probability was roughly motivated by a very simple question:
\begin{itemize}
	\item Suppose you are offered the following deal - a dice is thrown and you get as many francs as many dots come up on the top of the dice; but you have to pay $n$ francs independently of the result in return. How many francs should you agree to pay? 
\end{itemize}
Whereas what is really the 'right' answer still depends on some further conditions and assumptions. However, the following vaguely stated mathematical result gives some insight into the problem (and was used in these old times of gambling!): 
\begin{itemize}
	\item Let $X_1, X_2, \dots$ be independent random dice throws. Let $S_n = \sum_{i = 1}^n X_i$. Then in the limit $n \to \infty$ we have that $\frac{S_n}{n}$ converges to $\frac{1+2+3+4+5+6}{6}=3.5$.
\end{itemize}
This result is a specific case of the so called law of large numbers, and it tells you that the average gain from one dice throw is $3.5$. So would this mean that you should offer anything below $3.5$ francs?
While pondering on this worldly problem, let us dig into the mathematical theory.

\subsection{Expected value of a discrete random variable}

We start with the discrete case to lay clear foundations. The general case can be seen as an extension of this: 

\begin{defn}[Expected value of a discrete random variable]\label{def:expd}
Let $X$ be a discrete random variable defined on some probability space $(\Omega, \F, \P)$ and with support $S$. We say that $X$ admits an expected value or that $X$ is integrable if $\sum_{x \in S}|x| \P(X = x) < \infty$. 

For an integrable random variable $X$, the expected value of $X$, denoted $\E(X)$ is defined as
$$\E(X) = \sum_{x \in S} x \P(X = x).$$
\end{defn}


\begin{rem}
Observe the following
\begin{itemize}
	\item The condition for integrability is there of absolute summability - otherwise the order in the sum would matter, and there would be no unique answer to the expectation. We have that $X$ is integrable if $|X|$ is.
	\item The expectation only depends on the law $\P_X$ of the random variable and not the probability space on the background.
	\item Discrete random variables with finite support are always integrable.
\end{itemize}
\end{rem}

Before proving some properties that make the expected value extremely useful, let us look at some examples:\\

\noindent \textbf{Deterministic random variable}\\
If a random variable $X$ takes some value $x \in \R$ with probability 1, then its expectation is also clearly equal to $x$\\

\noindent \textbf{Bernoulli random variable}\\
Let $E$ be an event on a probability space, and consider the random variable $1_E$. As its support is finite, it is integrable. From the definition of expectation, we directly have that $\E(1_E) = \P(E)$. Thus in particular if $X$ is a $Ber(p)$ random variable, then its expectation is just $\E(X) = p$.\\

\noindent \textbf{Uniform random variable}\\
Consider the uniform random variable $U_n$ on $\{1, 2, \dots, n\}$. Again as it takes only finitely many values, it is integrable. Its expected value is 
$$\E(U_n) = \frac{1}{n}\sum_{i=1}^n i = \frac{n+1}{2}.$$

\noindent \textbf{Poisson random variable}\\
Consider the Poisson random variable $P$ of parameter $\lambda > 0$. The support of a Poisson random variable is not finite and thus one needs to verify that it is integrable. But in fact, the same computation also gives the expectation:
$$\E(P) = \sum_{n \geq 0} n \P(P = n) = \sum_{n \geq 1} n \frac{e^{-\lambda}\lambda^n}{n!} = \lambda e^{-\lambda}\sum_{m \geq 0}\frac{\lambda^m}{m!} = \lambda.$$
Hence, even if a random variable can take arbitrary large values, its expectation can be finite. This is, however, not always the case. For example
\begin{itemize} 
\item Consider a random variable $X$ such that it takes value $2^n$ with probability $2^{-n}$. Then clearly $\E(X) = \infty$ and $X$ is not integrable.
\end{itemize}
If a random variable is non-negative, then its expected value doesn't exist only if it is too large, i.e. is infinite. Sometimes one still defines expected value for any positive random variable, just saying that $\E(X) = \infty$, in case it is infinite. 

You will see more examples on the exercise sheet:

\begin{exo}[Expectations of discrete random variables]
	Prove that the expected value of a Binomial random variable $Bin(n,p)$ is equal to $np$. Prove also that the expected value of a geometric random variable of parameter $p$ is equal to $1/p$.
\end{exo}

As mentioned, the expected value is in some sense the best single number to describe a probability  distribution. There are several reasons to say that and first is the following: it minimizes the expected error we make in estimating the value of $X$ just using one deterministic number, when we measure the error in terms of average square differences.

\begin{lemma}
Let $X$ be an integrable discrete random variable with support $S$. Suppose that also $X^2$ is integrable. Then $c = \E(X)$ minimizes the expression $g(c) := \sum_{x \in S}(x-c)^2 \P(X = x)$.

Moreover, show from the definition that the value of $g(\E(X))$ can be written as $\E((X - \E(X)^2)$. This is called the variance of $X$.
\end{lemma}

\begin{proof}
This is on the example sheet.
%As $(x-c)^2 \leq x^2 + c^2$ and $X^2$ is integrable, the sum $g(c) := \sum_{x \in S}(x-c)^2 \P(X = x)$ is nicely summable. 

%To find it's minimum w.r.t. $c$ we differentiate $g(c)$ w.r.t. $c$. As also the sum of the term by term derivatives $\sum_{x \in S} -2(x-c)\P(x = x)$, we know that it equals $g'(c)$. But in can be equally written as $2c - 2\E(X)$. Thus we see that $g'(c)$ at $c = \E(X)$. Further, taking the second derivative we see that this is non-negative and thus $g(c)$ is indeed minimal at $\E(X)$.
\end{proof}

%\begin{rem}
%Notice that the value of $g(\E(X))$ can be written as $\E((X - \E(X)^2)$. This is in fact called the variance of $X$. And we will come back to it in a bit.
%\end{rem}

Another good reason for liking expectation is the fact that it is a linear operator on random variables. Together with this, let us also verify some other simple properties.

\begin{prop}\label{prop:drv}
Let $X, Y$ be two integrable discrete random variables defined on the same probability space. Then the expected value satisfies the following properties:
\begin{itemize}
	\item It is linear: we have that $\E(\lambda X) = \lambda \E(X)$ for all $\lambda \in \R$. Further, $X+Y$ is integrable and $\E(X + Y) = \E(X) + \E(Y)$.
	\item If $X \geq 0$ i.e. $\P(X \geq 0) = 1$ , then $\E(X) \geq 0$,
	\item If $X \geq Y$ i.e. $\P(X \geq Y) = 1$ , then $\E(X) \geq \E(Y)$. Deduce that if $\P(c \leq X \leq C) = 1$, then $c \leq \E(X) \leq C$.
	\item We have that $\E(|X|) \geq |\E(X)|$.
\end{itemize}
\end{prop}

\begin{proof}
The fact that $\E(\lambda X) = \lambda \E(X)$ follows directly from the definition.
Let us next prove that $X + Y$ is integrable and $\E (X+Y) = \E X + \E Y$. Denote by $S_X, S_Y$ the supports of $X$ and $Y$ respectively. Denote by $S_{X+Y}$ the support of $X+Y$. Notice that 
$$\P(X+Y = s) = \sum_{x \in S_X}\sum_{y \in S_Y}\P(X = x, Y = y)1_{x+y = s}$$
Thus we can write
$$\sum_{s \in S_{X+Y}}|s|\P(X+Y = s) = \sum_{s \in S_{X+Y}}\sum_{x \in S_X}\sum_{y \in S_Y}|x+y|\P(X = x, Y = y)1_{x+y = s}.$$
By triangle inequality we can bound $|x+y| \leq |x| + |y|$ and thus obtain
\begin{equation}\label{eq:what}
\sum_{s \in S_{X+Y}}|s|\P(X+Y = s) \leq \sum_{s \in S_{X+Y}}\sum_{x \in S_X}\sum_{y \in S_Y}(|x| + |y|)\P(X = x, Y = y)1_{x+y = s}.
\end{equation}
Now, observe that for fixed $x$ and $y$ either $\P(X = x, Y = y) = 0$ or $x+y \in S_{X+Y}$ and we have that
$$\P(X = x, Y = y) = \P(X = x, Y = y)\sum_{s \in S_{X+Y}}1_{x+y = s}.$$
Moreover, for fixed $x$ by the law of total probability we have that
$$\sum_{y \in S_Y}\P(X = x, Y = y) = \P(X = x).$$
Thus as everything in Equation \eqref{eq:what} is positive, we can now switch the order of summation, and to recognize the RHS as a sum of 
$$\sum_{x \in S_X}\sum_{y \in S_Y}\sum_{s \in S_{X+Y}}|x|\P(X = x, Y = y)1_{x+y = s} = \sum_{x \in S_X} |x|\P(X = x)$$
and
$$\sum_{y \in S_Y}\sum_{x \in S_X}\sum_{s \in S_{X+Y}}|y|\P(X = x, Y = y)1_{x+y = s} = \sum_{y \in S_Y} |y|\P(Y = y).$$
Hence we bound 
$$\sum_{s \in S_{X+Y}}|s|\P(X+Y = s) \leq \sum_{x \in S_{X}}|x|\P(X = x) + \sum_{y \in S_{Y}}|y|\P(Y = y)$$
and deduce integrability. Thereafter, the same way of separating sums also gives that $\E(X+Y) = \E(X) + \E(Y)$. 

The rest of the exercise is on the example sheet.
%For the second bullet point, we notice that if $X \geq 0$ with full probability, then for every $s \in S_X$, we have that $s \geq 0$. Thus it follows from definition of expectation that $\E(X) \geq 0$. 

%For the third bullet point, notice that by the condition $X - Y \geq 0$. Thus $X-Y \geq 0$ with full probability, and hence by the second bullet point $\E(X-Y) \geq 0$. The first bullet point then gives that $\E(X) \geq \E(Y)$. Plugging in $Y = c$ in this inequality, and noticing that $\E c = c$, gives $\E(X) \geq c$. The other inequality follows similarly.

%Finally, for the fourth bullet point notice that $-\E(X) = \E(-X)$ by the first point. Hence it suffices to show that $\E(X) \leq \E|X|$. But this just follows from the definition, as $\P(X = x)$ is always positive for $x \in S_X$ and hence
$$\E(X) = \sum_{x \in S_X}x\P(X = x) \leq \sum_{x \in S_X}|x|\P(X = x) = \E(|X|),$$
%where in the last equality we use that $\P(|X| = |x|) = \P(X = x) + \P(X = - x)$ and the fact that $|x| \in |S_X|$ if and only if either $x \in S_X$ or $-x \in S_X$.
\end{proof}

A very similar proof gives that if $X, Y$ are independent and integrable discrete random variables, then $XY$ is integrable and $\E(XY) = \E(X)\E(Y)$.

\begin{exo}\label{exo:indepE}
Let $X, Y$ be independent and integrable discrete random variables. Then $XY$ is integrable and $\E(XY) = \E(X)\E(Y)$.
\end{exo}

This allows us to come to the other fundamental property of the expectation - the empirical average converges to the mathematical expectation, allowing us to justify why we would should maybe be happy to pay any less than 3.5 francs to repeatedly be able to play the dice came from above...

\begin{thm}[A version of law of large numbers]\label{thm:wllnd}
Let $X_1, X_2, \dots$ be i.i.d. integrable discrete random variables such that $X_1^2$ is also integrable. Then for every $\eps > 0$
$$\P(|\frac{1}{n}\sum_{i=1}^n X_i - \E(X_1)| > \eps) \to 0$$
as $n \to \infty$
\end{thm}

Roughly, this law of large numbers says that if you repeat the same random experiment independently $n$ times to obtain i.i.d random variables $X_1, X_2, \dots, X_n$ then as $n \to \infty$ the average of $X_i$ converges to the expectation of $X_1$. This is quite remarkable that the distribution of the variables does not play any larger role in this limit - only the integrability and the expectation matter. Both of these theorems are related to so called ergodic theorems, which roughly link the temporal (here $n$) and spatial (here $\E$) averages.

We need one final ingredient before proving this:

\begin{prop}[Markov]
Let $X$ be a non-negative integrable discrete random variable. Then 
$\P(X \geq t) \leq t^{-1}\E(X)$.
\end{prop}

\begin{rem}
This and the independence claim of course hold also for the general random variables, we just need to first define their expectation!
\end{rem}

\begin{proof}[Proof of Theorem]
By assumption there is some $C$ such that $\E X_1^2 < C$. Let $S_n = n^{-1}\sum_{i=1}^n X_i$.

Our aim is to use the Markov's inequality. However, as absolute value is hard to work with we will instead use it for the square, which amends itself to linearity of exepctation and the property of independence from above:
$$\P(|S_n - \E(X_1)| > \eps) = \P((S_n - \E(X_1))^2 > \eps^2) \leq \E((S_n - \E(X_1))^2)/\eps^2.$$
So let us calculate $\E((S_n-\E X_1)^2)$. First by writing out $S_n$, opening the brackets inside expectation and then using linearity of expectation we have
$$\E(|S_n-\E X_1|^2) = \sum_{i,j \leq n} n^{-2}\E\left[(X_i-\E X_1)(X_j-\E X_1)\right].$$
We have that $\E X_j = \E X_1$. Thus we see that by linearity
$$\E\left[(X_i-\E X_1)(X_j-\E X_1)\right]  = \E(X_i X_j) + (\E(X_1))^2 - 2(\E(X_1))^2 =\E(X_i X_j) -(\E (X_1))^2 .$$
But for $i \neq j$, by independence also $\E(X_i X_j) = \E(X_i)\E(X_j) = (\E(X_1))^2$, giving us $$\E\left[(X_i-\E X_1)(X_j-\E X_1)\right] = 0$$ for $i \neq j$.
Hence 
$$\E(|S_n-\E X_1|^2) = n^{-2}\sum_{i = 1}^n\left( \E(X_i^2)-(\E(X_1))^2\right) = n^{-2} n^{-1}C \to 0$$
as $n \to \infty$. Hence we see that
$$\P(|S_n-\E X_1| > \eps) \leq \eps^{-2}n^{-1}C \to 0$$
and the theorem follows.


\end{proof}

We are still to prove the claim in Exercise \ref{exo:indepE} and the Markov's inequality. The first one will be on the next example sheet, Markov's inequality comes now:

\begin{proof}[Proof of Markov's inequality:]
Let $X$ be a non-negative discrete integrable random variable.
Then $Y_t = X 1_{X \geq t}$ is also a non-negative discrete integrable random variable as $Y_t \leq X$. But now observe that $Y_t \geq t 1_{X \geq t}$ and thus 
$$\E(X) \geq \E(Y_t) \geq \E(t 1_{X \geq t}).$$
But $\E(t 1_{X \geq t}) = t \P(X \geq t)$ by linearity and the fact that $1_E$ is Bernoulli random variable
We obtain $\E(X) \geq t \P(X \geq t)$ as desired.
\end{proof}

Hopefully you got convinced that the notion of mathematical expectation is pretty useful. We will now see how to generalize it to arbitrary, not necessarily discrete random variables.

\subsection{Expected value of an arbitrary random variable}

The idea for defining the expectation of a general random variable $X$ is to approximate it by discrete random variables.
More precisely, given $X$, we define the discretizations of $X$ as:
$$X_n(\omega) = 2^{-n}\lfloor 2^n X(\omega)\rfloor = \sum_{k \in \Z}k2^{-n}1_{X(\omega) \in [k2^{-n}, (k+1)2^{-n})}.$$
Notice that $X_n$ is indeed a discrete random variable - it is a non-decreasing function of $X$, so it is a random variable, and it takes only countably many values, thus it is discrete. The following exercise says that these discretizations really approximate the initial random variable very well. 

\begin{exo}[Discretizations are nice]\label{exo:disc}
Let $X$ be a random variable defined on $(\Omega, \F, \P)$. and $(X_n)_{n \geq 1}$ be the discretizations $X_n = 2^{-n}\lfloor 2^n X\rfloor = \sum_{k \in \Z}k2^{-n}1_{X \in [k2^{-n}, (k+1)2^{-n})}.$ 

Prove that for every $\omega \in \Omega$, we have that $X_n(\omega) \leq X(\omega) \leq X_n(\omega) + 2^{-n}$ and thus the sequence $(X_n(\omega))_{n \geq 1}$ converges to $X(\omega)$.  
\end{exo}

We can now use the definition of the expectation $\E(X)$ for discrete random variables $X$ to define expected value of an arbitrary random variable:

\begin{prop}[Expected value of a random variable]\label{prop:expg}
Let $X$ be a random variable defined on some probability space. If $\E(|X_m|) < \infty$ for some $m$, then $\E(|X_n|) < \infty$ for all $n$ and we call $X$ integrable. 
The expected value of $X$ is then defined as
$$\E(X) = \lim_{n \to \infty} \E(X_n).$$
\end{prop}

\begin{rem}
Observe again that the expectation only depends on the law of $X$ and not on the underlying probability space: this is clear in the case of discrete random variables, but now notice that if $X$ and $Y$ have the same law, then so do the discretizations $X_n$ and $Y_n$. 
\end{rem}

%\begin{rem}[Integrability for non-negative random variables]
%Notice that $X$ is integrable if and only if $|X|$ is integrable. And a priori before calculating the expectation of $X$ we have to make sure it is integrable.

%Now, when $X \geq 0$ almost surely (i.e. $\P(X \geq 0) = 1$), there are exactly two options: either $X$ is integrable and $\E X < \infty$, or it is not integrable. In the latter case, like for infinite sums of positive numbers, we could still define $\E X := \infty$, i.e. allow the expectation to take infinite value too. We will see that this convention becomes hand.
%It is important to verify that a random variable is integrable before calculating the expectation. We will see below that for example bounded random variables are automatically integrable.
%\end{rem}

\begin{rem}
A peek into future: if you consider $(\Omega, \F, \P) = ([0,1], \F_{L}, \P_U)$ where $\F_L$ is the Lebesgue $\sigma-$algebra and $\P_U$ the Lebesgue measure (we also called it uniform measure). Then for any integrable random variable $X$, which is just a measurable function from $([0,1], \F_L)$ to $([0,1], \F_E)$, $\E X$ is its Lebesgue integral. You will see a more general construction in your Analysis IV course using a larger family of approximations.
\end{rem}

The idea for proving this proposition is just to show that the sequence $\E(X_n)$ is Cauchy.
\begin{proof}
Notice that from the Exercise \ref{exo:disc} above we see that $X_1 - 1 \leq X_n \leq X_1 + 1$ and hence $|X_n| \leq |X_1|+ 1$. Thus from Proposition \ref{prop:drv} it follows that $\E(|X_n|) < \infty$ if and only if $\E(|X_1|) < \infty$ giving the first claim. 

We now claim that $\E(X_n)$ is a Cauchy sequence. So consider $m \geq n$. Then from Proposition \ref{prop:drv} it follows that
$$|\E(X_n) - \E(X_m)| = |\E(X_n - X_m)| \leq \E(|X_n - X_m|).$$
But we can bound $|X_n - X_m| \leq 2^{-n}$ using Exercise \ref{exo:disc}. Hence $|\E(X_n) - \E(X_m)| \leq \E(2^{-n}) = 2^{-n}.$
It follows that the sequence $(\E(X_n))_{n \geq 1}$ is Cauchy and thus converges to a unique limit as $n \to \infty$.
\end{proof}

An easy but important sanity check is that this definition indeed agrees with the previous definition for discrete random variables, i.e. that the Definition \ref{def:expd} of $\E(X)$ and the definition of $\E(X)$ by Proposition \ref{prop:expg} agree for any discrete random variable $X$. This is on the example sheet.

%\begin{rem}[Jargon - 'almost surely']
%We have tried to avoid too much probabilistic jargon so far, but it is now high time to introduce at least one expression:	One says that an event $E$ on a probability space $(\Omega, \F, \P)$ happens almost surely, if $\P(E) = 1$.

%For example, if for some $c \in \R$ we have that $\P(X = c) = 1$, we would say that $X$ is almost surely a constant. Or if $\P(X = Y) = 1$ for some random variables $X, Y$ on the same probability space, we would say $X = Y$ almost surely, or if $\P(X > 0) = 1$, we would say that $X$ is positive almost surely. 
%\end{rem}

Further, one can also check that all the properties that hold for the expectation of the discrete random variable, also hold for the expectation in general:

\begin{prop}\label{prop:grv}
	Let $X, Y$ be two integrable random variables defined on the same probability space. Then the expected value satisfies the following properties:
\begin{itemize}
	\item It is linear: we have that $\E(\lambda X) = \lambda \E(X)$ for all $\lambda \in \R$. Further, $X+Y$ is integrable and $\E(X + Y) = \E(X) + \E(Y)$.
	\item If $X \geq 0$ i.e. $\P(X \geq 0) = 1$ , then $\E(X) \geq 0$,
	\item If $X \geq Y$ i.e. $\P(X \geq Y) = 1$ , then $\E(X) \geq \E(Y)$. Deduce that if $\P(c \leq X \leq C) = 1$, then $c \leq \E(X) \leq C$.
	\item We have that $\E(|X|) \geq |\E(X)|$.
\end{itemize}
Further also the Markov inequality holds.
\end{prop}

\begin{proof}
All these points follow from Proposition \ref{prop:drv} via discretizations and Exercise \ref{exo:disc}. This is a somewhat tedious verification that I leave for you. 

For example, as for all $n$ , we have that $X_n + 2^{-n} \geq X$, then $X \geq 0$ means that $X_n \geq -2^{-n}$. It follows from Proposition \ref{prop:grv} that $\E(X_n) \geq -2^{-n}$, implying that for every $\eps > 0$, for all $n$ large enough $\E(X_n) \geq -\eps$ and hence $\E(X) \geq 0$.

Markov's inequality can be proved either by discretization or in fact by exactly the same proof we gave above.
\end{proof}


Let us now see that in the case of random variables with density, we can use Riemann integration and the density to calculate expectation.

\begin{prop}[Expected value for r.v. with density]\label{prop:meancts}
Let $X$ be a random variable with density $f_X$. Then $X$ is integrable iff $\int_\R |x|f_X(x)dx < \infty$ and we have
$$\E(X) = \int_\R x f_X(x)dx.$$
\end{prop}

\begin{proof}
Consider the discretizations $X_n = 2^{-n}\lfloor 2^nX \rfloor$. Notice that
$$\P(X_n \in [k2^{-n}, (k+1)2^{-n})) = \int_{k2^{-n}}^{(k+1)2^{-n}}f_X(x)dx$$
and hence
$$\E(|X_1|) = \sum_{k \geq 0} k2^{-1} \int_{k2^{-1}}^{(k+1)2^{-1}}f_X(x)dx + \sum_{k \geq 1}   k2^{-1} \int_{-k2^{-1}}^{(-k+1)2^{-1}}f_X(x)dx.$$
Now, if $|x| \in [k2^{-1}, (k+1)2^{-1})$ then $k2^{-1} \leq |x| \leq k2^{-1}+2^{-1}$. 
Using the fact that $\int_\R f_X(x)dx = 1$ and that $f_X$ is non-negative, we conclude that 
$$-1+\int_\R |x|f_X(x)dx  \leq \E(|X_1|) \leq 1+\int_\R |x|f_X(x)dx.$$
Thus $X$ is integrable iff $\int_\R |x|f_X(x)dx < \infty$.


Next, as 
$$\E(X_n) = \sum_{k \in \Z} k2^{-n} \int_{k2^{-n}}^{(k+1)2^{-n}}f_X(x)dx,$$
we see similarly to above that also
$$\E(X_n) \leq \int_\R x f_X(x)dx \leq \E(X_n) + 2^{-n}.$$
But $\E(X_n) \to \E(X)$ as $n \to \infty$, and hence the proposition now follows by taking $n \to \infty$.
\end{proof}

Let us calculate densities for some known random variables:\\

\noindent \textbf{Uniform random variable on $[a,b]$}\\
Consider a uniform random variable $U$ on $[a,b]$. Recall its density is given by $f_U(x) = (b-a)^{-1}1_{x \in [a,b]}$. First notice that $U$ is bounded and hence integrable. Thus we calculate:
$$\E(U) = (b-a)^{-1}\int_\R x 1_{x \in [a,b]} dx = (b-a)^{-1}\int_a^b x dx = \frac{b^2-a^2}{2(b-a)} = \frac{a+b}{2}.$$

\noindent \textbf{Gaussian random variable}\\
Consider a standard normal random variable $N \sim \Nn(0,1)$. We first note that 
$$\frac{1}{\sqrt{2\pi}}\int_\R |x| \exp(-\frac{x^2}{2})dx =\frac{2}{\sqrt{2\pi}}\int_0^\infty x \exp(-\frac{x^2}{2})dx = \frac{2}{\sqrt{2\pi}} < \infty.$$
Thus $N$ is integrable. We further notice that
$$\E(N) = \frac{1}{\sqrt{2\pi}}\int_\R x \exp(-\frac{x^2}{2})dx = \E(-N),$$
as the density of $-N$ is the same as that of $N$. Hence Proposition \ref{prop:grv} implies that $\E(N) = 0$. 

Now, consider a general Gaussian random variable $N_{\mu, \sigma^2} \sim \Nn(\mu, \sigma^2)$. Recall that we can write $N_{\mu, \sigma^2} \sim \sigma N + \mu$ and hence $N_{\mu, \sigma^2}$ is integrable. Further, we can use Proposition \ref{prop:grv} one more time to deduce that $\E N_{\mu, \sigma^2} = \sigma \E(N) + \mu = \mu$. This is the reason why $\mu$ is called the mean of the Gaussian random variable.

Again, further examples are on the exercise sheet.

%\begin{exo}[Expectations of continuous random variables]
%	Prove that the Gamma random variable $Gamma(\lambda, t)$ is integrable. What is its expectation? Deduce the expectation for the exponential random variable. Is the standard Cauchy random variable integrable? [As proved in the last example sheet, the density of the standard Cauchy random variable is $f_X(x) = \frac{1}{\pi(1+x^2)}$].
%\end{exo}

\subsection{Expected value of a function of a random variable}

It comes out that the expected value, even if just a number, is very useful tool to describe a random variable. Often we might not be interested in the expectation of some given random variables, but of certain functions of these random variables. For example, we have already seen that given a r.v. $X$ we might be interested in $\E\left((X-\E X)^2\right)$, or given $X, Y$, we might be interested in $\E XY$. In fact, as we will see, if we know $\E g(X)$ for sufficiently many functions $g$, then this determines the random variable itself!

To start, let us look at the following proposition that generalizes the exercise showing that for discrete random variables $\E\left((X-s)^2\right) = \sum_{x \in S_X} (x - s)^2\P(X = x)$, i.e. that gives us a nice way to calculate expectations of functions of a r.v.:

\begin{prop}\label{calcexp}
Let $\ol X = (X_1, \dots, X_n)$ be a random vector defined on $(\Omega, \F, \P)$ and $\phi$ a measurable function from $(\R^n, \F_E)$ to $(\R, \F_E)$, so that $\phi(\ol X)$ is a random variable.
\begin{itemize}
	\item If all $X_1, \dots, X_n$ are discrete and $\phi(\ol X)$ is integrable, then
	$$\E(\phi(\ol X)) = \sum_{\ol x \in S_{\ol X}}\phi(\ol x) \P(\ol X = \ol x),$$
	where $S_{\ol X} \subseteq \R^n$ is the support of the random vector $\ol X$, i.e. the set of $\overline s = (s_1, \dots,  s_n) \in \R^n$ such that $\P(\ol X= \ol s) > 0$ for all $\ol x \in S_{\ol X}$ and $\P(\ol X \in S_{\ol X}) = 1$.
	\item If $\ol X$ is a random vector with density, $\phi(X)$ an integrable random variable and $\phi$ sufficiently nice - meaning that $\phi^{-1}([a,b))$ is Riemann measurable for any interval $[a,b)$ - then
	$$\E(\phi(\ol X)) = \int_{\R^n}\phi(\ol x)f_{\ol X}(\ol x) d\bar x.$$	
\end{itemize}
\end{prop}

The condition 'sufficiently nice' is of course not quite natural. This is yet again due to the fact that Riemann integration and measurability in the sense of Borel (or Lebesgue) do not play together in full harmony. After Analysis IV next semester, you should be able to revisit many of these results and restate them in more natural ways, if interested of course. Still, notice that the condition holds for many natural functions like $x^n$ or $\exp(x)$.

\begin{proof}
Let us start from the discrete case, which works exactly like Exercise 3 on Sheet 10 after checking that if $\phi$ is measurable then $\phi(X)$ is still a discrete random variable. Let us still spell it out in the notes.

So let $S_\phi$ denote the support of $\phi(\ol X)$. By definition, $\phi(\ol X)$ is integrable iff 
$$\sum_{s \in S_\phi} |s| \P(\phi(\ol X) = s) < \infty$$ and then
$$\E(\phi(\ol X)) = \sum_{s \in S_\phi} s \P(\phi(\ol X) = s).$$
By the law of total probability we can write 
$\P(\phi(\ol X) = s) = \sum_{\overline x \in S_{\ol X}} \mathbb{P}\left(\overline{X}=\overline{x}\right)\cdot1_{\phi(\ol x) = s}$ and thus the the whole expression can be written as 
$$\sum_{s \in S_\phi} x \sum_{\overline x \in S_{\ol X}} 1_{\phi(\ol x) = s} \P(\ol X = \ol x) = \sum_{\overline x \in S_{\ol X}}\P(\ol X = \ol x) \sum_{s \in S_\phi}s 1_{\phi(\ol x) = s},$$
where we can change the order of summation as the series is absolutely summable. To conclude, notice that for any fixed $\ol x \in \R^n$, we have that $\sum_{s \in S_\phi}s 1_{\phi(\ol x) = s} = \phi(\ol x)$.

The case of the random variables with density is admitted i.e. non-examinable, but I still give the proof for those interested.


To prove the case for random variables with density, we use discretizations - we set $\phi_n(\ol x) = 2^{-n}\lfloor \phi(\ol x) 2^n \rfloor$. Then - given integrability - we have that
$$\E(\phi_n(\ol X)) = \sum_{k \in \Z}k2^{-n}\P(\phi_n(\ol X) = k2^{-n}).$$
Now, given that $\phi^{-1}([a,b))$ are Riemann-measurable, we can write
$$k2^{-n}\P(\phi_n(\ol X) = k2^{-n}) = \int_{\R^n} 1_{\ol x \in \phi^{-1}([k2^{-n},(k+1)2^{-n}))}k2^{-n}f_{\ol X}(\ol x) d\bar x.$$
Again by absolute summability \footnote{More precisely, we are using there that if either $\sum_{n \geq 1}\int_\R |f_n(x)|dx < \infty$ or $\int_\R\sum_{n \geq 1} |f_n(x)|dx < \infty$, then $\int_\R\sum_{n \geq 1} f_n(x)dx = \sum_{n \geq 1}\int_\R f_n(x)dx$. You have met the analogous result for swapping two sums $\sum_{k \geq 1} \sum_{n \geq 1}$, and the proof is basically the same.} we can switch the order of sum and integration to get
$$\E(\phi_n(\ol X)) = \int_{\R^n} f_{\ol X}(\ol x) \sum_{k \in \Z} 1_{\ol x \in \phi^{-1}([k2^{-n},(k+1)2^{-n}))}k2^{-n} d\bar x.$$
As above, for any fixed $\ol x$, we have that $1_{\ol x \in \phi^{-1}([k2^{-n},(k+1)2^{-n}))}$ is equal to $1$ for only one value of $k$ and thus from the definition of $\phi_n$, we obtain $$\sum_{k \in \Z} 1_{\ol x \in \phi^{-1}([k2^{-n},(k+1)2^{-n}))}k2^{-n} = \phi_n(\ol x).$$ Hence 
$$\E(\phi_n(\ol X)) = \int_{\R^n}\phi_n(\ol x) f_{\ol X}(\ol x)  d\bar x.$$
We can now conclude similarly to Proposition \ref{prop:meancts}.
\end{proof}


Looking at expectations of functions of a random variable turns out to be a powerful thing:

\begin{prop}\label{prop:charg}
Let $X, Y$ be two random variables. Then $X$ and $Y$ are equal in law if and only if for all bounded continuous functions $g: \R \to \R$ we have that $\E g(X) = \E g(Y)$.
\end{prop}

\begin{proof}
If $X$ and $Y$ have the same law, then also do $g(X)$ and $g(Y)$ for any continuous and bounded $g$. Hence, as bounded functions are integrable and the expectation only depends on the law of the r.v., we indeed have that $\E g(X) = \E g(Y)$.

In the other our aim is to show that $\forall t \in \R$, $F_X(t) = F_Y(t)$. To do this recall that $F_{X}(t) = \P(X \leq t) = \E(1_{x \leq t})$, so our aim will be to consider continuous approximations $g_{t,n}$ of the indicator function $1_{x \leq t}$, defined as follows. Fix some $t \in \R$ and set $g_{t,n}(x) = 1$ if $x \leq t$, we set $g_{t,n}(x) = 0$ if $x \geq t+2^{-n}$ and we set $g_{t,n}(x) = 1-2^n(x-t)$ inside the interval $(t,t+2^{-n})$. 

Then, one the one hand
$$F_{X}(t) = \P(X \leq t) = \E(1_{x \leq t}) \leq \E(g_{t,n}(X))$$
and on the other hand
$$\E(g_{t,n}(X)) \leq \E(1_{x \leq t+2^{-n}}) = \P(X \leq t+2^{-n}) = F_{X}(t+2^{-n}).$$
Thus by right-continuity of $F_X(t)$ we see that $\E(g_{t,n}(X))$ converges to $F_X(t)$ as $n \to \infty$.
But similarly also $\E(g_{t,n}(Y))$ converges to $F_Y(t)$ as $n \to \infty$. As by assumption  $\E(g_{t,n}(X)) =  \E(g_{t,n}(Y))$, we can conclude the proposition.
\end{proof}

We already saw that if $X, Y$ are independent, then their product factorises. But in fact there is a sort of converse too - $X, Y$ are independent if the expectation factorizes for all continuous functions!

\begin{prop}\label{prop:indexpg}
Let $X, Y$ be two random variables. Then 
\begin{itemize}
\item If for all $g: \R \to \R, h: \R \to \R$ continuous and bounded we have that 
	\begin{equation}\label{eq:indp}
	\E\left(g(X)h(Y)\right) = \E g(X) \E h(Y),
	\end{equation} then $X$ and $Y$ are independent.
	\item On the other hand, if $X$ and $Y$ are independent, then for all measurable functions $g,h: \R \to \R$ such that $g(X)$ and $h(Y)$ are integrable the Equation \eqref{eq:indp} holds.
\end{itemize}
\end{prop}

\begin{proof}
The first part follows similarly to the last proposition:

From Lemma \ref{lem:indcdf} we know that to prove $X, Y$ are independent, it suffices to prove that for all $s, t \in \R$
we have that $F_{(X,Y)}(s,t) = F_X(s)F_X(t)$. Further, recall that $F_{(X,Y)}(s,t) = \E1_{X \leq s, Y \leq t} = \E1_{X \leq s}1_{Y \leq t}.$
We follow the strategy of Proposition \ref{prop:charg}. Indeed, consider the same continuous functions $g_{t,n}(x)$ satisfying $1_{x \leq t} \leq g_{t,n}(x) \leq 1_{x \leq t+2^{-n}}.$

Using the expression of $F_{(X,Y)}$ above, definition of $g_{t,n}$ and properties of expectation be can bound
$$F_{(X,Y)}(s,t) \leq \E g_{s,n}(X)g_{t,n}(Y) \leq F_{(X,Y)}(s+2^{-n}, t+2^{-n}).$$
By assumption $$\E g_{s,n}(X) g_{t,n}(Y) = \E g_{s,n}(X) \E g_{t,n}(Y)$$.
Now by right-continuity of $F_{(X,Y)}$, we know that $F_{(X,Y)}(s+2^{-n}, t+2^{-n})$ converges to $F_{(X,Y)}(s, t)$ and hence also $\E g_{s,n}(X) g_{t,n}(Y)$ does. Further we have seen that $\E g_{s,n}(X)$ converges to $F_X(s)$ and similarly $\E g_{t,n}(Y)$ converges to $F_X(t)$. Thus we conclude that $F_{(X,Y)}(s,t) = F_X(s)F_X(t)$ as desired.\\

For the other direction, we first observe the following (this will be on the exercise sheet):
\begin{exo}
    Prove that if $X, Y$ are independent random variables, then so are $g(X), h(Y)$.
\end{exo}

Given this, the second point follows when we show that for any integrable independent random variables $X, Y$ we have that $\E(XY) = \E(X) \E(Y).$ 
The discrete case was on the exercise sheet 11. 
%We first deal with the case of discrete random variables, and then pass to the limit using approximations. We will discuss this next time.\\

%\noindent \textbf{The discrete case} \\
%Denote the supports by $S_X, S_Y$ and write
%\[\E(X) \E(Y) = \left(\sum_{x \in S_X}x \P(X = x)\right) \left(\sum_{y \in S_Y}y \P(Y = y)\right) = \sum_{x \in S_X}\sum_{y \in S_Y} xy \P(X = x)\P(Y = y).\]
%Now, for any random variables $X, Y$ and every fixed $x \in S_X, y \in S_Y$ we have the identity 
%$$\P(X = x, Y = y) = \P(X = x, Y = y)\sum_{s \in S_{XY}} 1_{xy = s}.$$
%Further, by independence of $X, Y$ we have $\P(X = x, Y = y) = \P(X = x)\P(Y = y).$ Thus we can write
%$$\sum_{x \in S_X}\sum_{y \in S_Y} xy \P(X = x, Y = y) = \sum_{x \in S_X}\sum_{y \in S_Y}xy \P(X = x, Y = y)\sum_{s \in S_{XY}} 1_{xy = s} .$$
%By integrability of $X, Y$, this triple-series is absolutely summable, and thus we can change the order of sums and observe $xy1_{xy = s} = s1_{xy = s}$ to get
%$$\sum_{s \in S_{XY}} \sum_{x \in S_X}\sum_{y \in S_Y} s1_{xy = s} \P(X = x, Y = y).$$
%Finally, we observe that $$\sum_{x \in S_X}\sum_{y \in S_Y} 1_{xy = s} \P(X = x, Y = y) = \P(XY = s)$$
%which implies the claim for discrete r.v. Observe that %this very same change of summation also gives the %integrability of $XY$. \\

%\noindent \textbf{The general case} \\
The general case proceeds again via approximation and is left as an exercise. 

%If $X, Y$ integrable, then $X_n, Y_n$ are integrable and by the previous part then also $X_nY_n$ is integrable. 

%Observe further that for $x, y \geq 0$, one has 
%$$x_n y_n - 2^{-n} \leq (xy)_n \leq xy \leq (x_n+2^{-n})(y_n+2^{-n}).$$
%Similarly, for $x \geq 0, y < 0$
%$$(x_n+2^{-n}) y_n - 2^{-n} \leq (xy)_n \leq xy \leq (x_n)(y_n+2^{-n}).$$
%Treating also the two other cases, one notices that in general we have
%$$x_n y_n -2^{-n}(|y_n| +|x_n| +1) \leq (xy)_n \leq x_n y_n + 2^{-n}(|x_n| + |y_n|+1)$$
%and in particular also
%$$|(xy)_n| \leq |x_n y_n| + 2^{-n}(|x_n| + |y_n|+1).$$
%These inequalities now hold almost surely when we replace $x,y$ by $X, Y$. Hence, as $|X_n|, |Y_n|$ and $|X_n Y_n|$ are all integrable, we conclude that $XY$ is integrable and we can take expectations.

%Thus by integrability, the inequalities above and basic properties of expectation
%$$\E X_n Y_n - 2^{-n}(\E|X_n| + \E|Y_n| + 1) \leq  \E(XY)_n \leq \E X_n Y_n + 2^{-n} (\E|X_n| + \E|Y_n|+1).$$
%Now $\E|X_n| \leq \E |X| + 1 < \infty$ and similarly $\E|Y_n| \leq \E |Y| + 1 < \infty$, Thus as $n \to \infty$ $$2^{-n}(\E|X_n| + \E|Y_n|+1) \to 0.$$
%Further, by the discrete case we have $\E X_n Y_n  = \E X_n \E Y_n$. As by definition of expectation $\E X_n \to \E X$, $\E Y_n %\to \E Y$ and $\E(XY)_n \to \E XY$ , we conclude the general case.
\end{proof}

%\subsubsection{[$\star$ non-examinable] Expectation = Lebesgue integral}

\subsection{Variance and covariance}

Next to the mean value or expectation, a key parameter or characteristic of a random variable is its variance (and its standard deviation, which is just the square-root of the variance). 

This measures the deviation from the mean, and in fact we already saw it when characterising the expectation as a minimzer of deviation:
%Before defining that, let us make a comment on non-negative random variables:

%\begin{rem}[Postive random variable and integrability]\label{rem:posrv}
%A priori we have defined $\E X$ only if we knew that $X$ was integrable. However, when we talk of positive random variables, it is reasonable to write down $\E X$ even before knowing that $X$ is integrable. Indeed, if a random variable is a.s. positive, then there are exactly two options - either $\E X$ is finite and well-defined, or it is not finite and we can set $\E X :=+\infty$. We will use this convention once in a while and give a useful condition condition on the exercise sheet.
%\end{rem}


\begin{defn}[Variance of a random variable]
Let $X$ be an integrable random variable. Then if $\E(|X|^2) < \infty$, we say that $X$ has a finite second moment and define its variance $$\Var (X) := \E((X-\E X)^2) \geq 0.$$ Standard deviation is defined as $\sigma(X) := \sqrt{\Var X}$. 
\end{defn}

Notice that indeed $(X-\E X)^2$ is integrable when $|X|^2$ is, as we can write $(X-\E X)^2 \leq 2|X|^2 + 2(\E X)^2$. A useful tool for calculating variance is to notice that by opening the square
$$\Var(X) = \E\left((X -\E X)^2\right) = \E(X^2) - 2\E(X \E X) + (\E X)^2 = \E(X^2) - (\E X)^2.$$
So let us calculate some variances using this:

\begin{itemize}
	\item The variance of a Bernoulli random variable $X \sim Ber(p)$ is $\E(X^2) - (\E X)^2 = p-p^2 = p(1-p)$. Why is this reasonable?
	\item Similarly, using the same formula we can calculate the variance of an exponential random variable $X \sim Exp(\lambda)$. Indeed, as $x^2$ satisfies the conditions of Proposition \ref{calcexp}, we can write 
	$$\E X^2 = \lambda \int_0^\infty x^2 \exp(-\lambda x)dx.$$
	We now calculate by doing twice integration by parts
	$$\lambda \int_0^\infty x^2 \exp(-\lambda x)dx = 2\int_0^\infty x \exp(-\lambda x) dx = 2\lambda^{-1}\E X = 2\lambda^{-2}.$$
	Hence $\Var (X) = \lambda^{-2}$.
\end{itemize}

Variance tells us how much the random variable fluctuates or deviates around its mean, as is illustrated for example by the following lemma, whose proof was on the example sheet.

\begin{lemma}[Chebyshev's inequality]
Let $X$ be an integrable random variable with finite variance. Then $\P(|X -\E X| > t) \leq \frac{\Var(X)}{t^2}.$
\end{lemma}
%
%\begin{proof}
%	This follows directly from the Markov's inequality $\P(Y > t) \leq \frac{\E Y}{t}$ that we proved for non-negative integrable random variables $Y$ on the previous exercise sheet. Indeed, we just apply Markov's inequality to $Y = (X -\E X)^2$ to get that
%	\[\P(|X - \E X| > t) = \P((X - \E X)^2 > t^2) \leq \frac{\Var(X)}{t^2}.\]
%\end{proof}

%In fact, variance also gives us a new view on expectation itself as the minimizer of certain error: if $X$ is an integrable random variable of finite variance, then the real number $a$ that minimizes the so called mean squared error: $\E(X -a)^2$ is given by $a = \E X$! Again, you will find this on the example sheet. 


%\subsubsection{An interlude on some slang - 'almost surely'}

%We have tried to avoid too much probabilistic jargon so far, but it is now high time to introduce at least one expression:
%\begin{defn}[Almost surely]
%	One says that an event $E$ on a probability space $(\Omega, \F, \P)$ happens almost surely, if $\P(E) = 1$.
%\end{defn}

%For example, if for some $c \in \R$ we have that $\P(X = c) = 1$, we would say that $X$ is almost surely a constant. Or if $\P(X = Y) = 1$ for some random variables $X, Y$ on the same probability space, we would say $X = Y$ almost surely, or if $\P(X > 0) = 1$, we would say that $X$ is positive almost surely. 


\subsubsection{Covariance and correlation}
As discussed, one is often is interested how two random variables are related to each other. We already saw the notion of independence - random variables are independent if they don't influence each other at all. In the other extreme there is the case where they are equal, i.e. $\P(X = Y) = 1$ in which case we say $X = Y$ almost surely. Both of those are very strong notions. The precise relation of two random variables is encoded in their joint law, but that can be quite complicated.

Here we introduce a simpler and weaker measure of how two random variables are related, and a way to in some sense measure the level of dependence.

\begin{defn}[Covariance and correlation]
	Suppose that $X, Y$ are two integrable random variables of finite variance defined on the same probability space. The covariance of $X$ and $Y$, denoted $Cov(X,Y)$ is then defined as 
	$$\Cov(X,Y)= \Cov(Y,X) = \E\left ((X-\E X)(Y- \E Y)\right) = \E(XY) -\E X \E Y.$$
	If neither of $X, Y$ is almost surely a constant, then the correlation $\rho(X,Y)$ is defined as
	$$\rho(X, Y) = \frac{\Cov(X,Y)}{\sqrt{\Var (X)\Var(Y)}}.$$
\end{defn}

A first question might be why is even covariance well-defined? I.e. why is $\E(XY)$ finite when $X, Y$ have finite variance?
This follows from the Cauchy-Schwarz inequality, which I believe you have already seen in some form. You will find an non-eximinable proof at the end of the section.

\begin{thm}[Cauchy-Schwarz inequality]
Let $X, Y$ be two random variables on $(\Omega, \F, \P)$ such that $X^2, Y^2$ are integrable. Then $|XY|$ is also integrable, and moreover
$$\E(|XY|) \leq \sqrt{\E(X^2)\E(Y^2)}.$$ Moreover, the equality holds if and only if $|X| = \lambda |Y|$ almost surely for some $\lambda > 0$.
\end{thm}

Notice that in particular it also follows that $$\E(XY) \leq |\E(XY)| \leq \E|XY| \leq  \sqrt{\E(X^2)\E(Y^2)}.$$ The relevant cases of equality can be also worked out.

Using this inequality, we see that not only are covariance and correlation well defined, but also we can see that having full correlation means that the random variables are almost surely equal.

\begin{lemma}[Covariance and dependence]
Let $X, Y$ be two random variables of finite positive variance defined on the same probability space.
\begin{itemize}
	\item Then the correlation $\rho(X,Y) \in [-1,1]$. Further, it is equal to $1$ if and only if there exist some $\lambda > 0, c \in \R$ such that $X = \lambda Y + c$ almost surely; it is equal to $-1$ if and only if there exist some $\lambda > 0, c \in \R$ such that $X = -\lambda Y + c$ almost surely;
	\item  Further, if $X, Y$ are independent, integrable with finite variance, then their covariance is zero. 
\end{itemize}
\end{lemma}

\begin{proof}
The first part follows from the Cauchy-Schwarz inequality. 

For the second part we calculate:
$$\Cov(X,Y) = \E((X-\E X)(Y - \E Y)) = \E (XY) - \E(X) \E(Y).$$
But by independence of $X, Y$ we know that $\E(XY) = \E X\E Y$ and we conclude.
\end{proof}

Given a random vector, it is often useful to define the covariance between each pair of components.

\begin{defn}[Covariance matrix]
Let $\ol X = (X_1, \dots, X_n)$ be a random vector such that all components have finite variance. Then the covariance matrix $\Sigma_{i,j}$ is defined as
$$\Sigma_{i,j} = \Cov(X_i, X_j).$$
\end{defn}

In fact, we have already met a covariance matrix! indeed, for a Gaussian random vector $\Nn(\ol \mu, C)$, the matrix positive-definite symmetric matrix $C$ is the covariance matrix and $\ol \mu = (\E X_1, \dots, \E X_n)$:

\begin{exo}[Independence and Gaussians] 
Prove that for a Gaussian random vector $\bar X \sim \Nn(\ol \mu, C)$, the matrix $C$ is the covariance matrix and $\ol \mu = (\E X_1, \dots, \E X_n)$. Show that in the case of a Gaussian random  vector, if $\Cov(X_i, X_j) = 0$, then $X_i$ and $X_j$ are independent.
\end{exo}

Observe that this in particular means that a Gaussian vector is determined only by its mean and covariance, which is very nice indeed!


\subsection{Moments of a random variable}

We have seen that $\E(X)$ and $\E((X-\E X)^2)$ contain valuable information about a random variable $X$. Moreover, we saw that if we look at $\E g(X)$ for all bounded continuous $g$, then this determines the law of $X$ completely. But this is already quite a lot of information!
An intermediate task would be to ask $\E X^n$ for all $n \geq 1$. Does knowing this determine the random variable?

\begin{defn}[Moments of a r.v.]
Let $X$ be a random variable and $n \in \N$. If $\E |X|^n < \infty$, we say that $X$ admits a $n$-th moment. We call $\E X^n$ the $n$-th moment of $X$. 
\end{defn}

To understand the relation between different moments, let's recall the Jensen's inequality. A function $\phi: \R \to \R$ is called convex if for all $x,y$ and all $\lambda \in [0,1]$ we have that 
$$\phi(\lambda x + (1-\lambda) y) \leq \lambda \phi(x) + (1-\lambda) \phi(y).$$
We call $\lambda x +(1-\lambda) y$ a convex combination of $x, y$. Using this vocabulary, Jensen's inequality can be reworded as saying that the image under $\phi$ of a convex combination of two points is always smaller than the convex combination of the images of the two points under $\phi$. (What does it mean geometrically?)

Jensen's inequality in the probabilistic set-up is stated as follows: 

\begin{thm}[Jensen's inequality]
Let $X$ be an integrable random variable and $\phi$ a convex function such that $\phi(X)$ is also integrable \footnote{Recall that a convex function is continuous and thus if $X$ is a random variable, then so is $\phi(X)$}. Then
$$\phi(\E X) \leq \E \phi(X).$$
\end{thm}

Notice the similarity with the defining property of convexity: $\E X$ can be thought of as a convex combination of the possible values of $X$. Thus, for example, if $X$ takes only two values $x, y$ with probabilities $\lambda$ and $1-\lambda$ then Jensen's inequality is just a reformulation of the defining property of convexity.

I hope you have seen and will see many different proofs of this nice inequality. Still, there is one in the appendix on this section for completeness. 

As a corollary we have the following simple lemma, saying that the existence of higher moments implies the existence of lower moments too:

\begin{lemma}
Let $X$ be a random variable defined on some probability space $(\Omega, \F, \P$ that admits a $n$-th moment.
Then it also admits a $m$-th moment for all $m \leq n$ and moreover $\E |X|^n \geq \left(\E( |X|^m)\right)^{n/m}$.
\end{lemma}

\begin{proof}
	Let $m \leq n$. Let us first notice that if $|X|^n$ is integrable, then also is $|X|^m$ with $m \leq n$. Indeed, we can bound 
	$$|X(\omega)|^m \leq \max(|X(\omega)|^n, 1) \leq |X(\omega)|^n +1$$
	and thus integrability of $|X|^m$ follows from that of $|X|^n$.
	
	Now, for $n \geq m$, consider $\phi(x) = |x|^{n/m}$. This is a convex function. Hence, as both $|X|^m$ and $|X|^n = \phi(|X|^m)$ are integrable, we can apply Jensen's inequality to $\phi$ and $|X|^m$ and obtain
	$$\E|X|^n =\E(\phi(|X|^m))  \geq \phi(\E |X|^m) = \left(\E( |X|^m)\right)^{n/m},$$
    concluding the proof.
\end{proof}


In particular, we conclude that if the second moment of $X$ exists, then both $X$ is integrable and of finite variance. Many random variables you will see in statistics or numerics will have finite variance, so it's useful to have a good condition for that. You will see on the example sheet that the converse is not true, there will be examples of integrable random variables with infinite variance and so on.

%The converse however is not true due to the following explicit counterexamples :

%\begin{exo}[Moments]
%For $k > 1$, let $c_k = \sum_{k \geq 1}n^{-k}$. Define the discrete random variable $X_k$ with support $\Z$ by setting $\P(X_k = n) = n^{-k} c_k^{-1}$ for $n \geq 1$.

%Show that for $k \leq 2$, $X_k$ is not integrable. Further, show that for $k > 2$, $X_k^m$ is integrable if and only if $m < k-1$. Calculate $\P(X_k \geq n)$ by hand and compare the result with the bound obtained from Markov's inequality using finiteness of $m$-th moments for $m < k-1$. 
%\end{exo}

The existence of moments has a direct influence on how the tails of the random variable behave. Indeed, by Markov's inequality if $\E |X|^n < \infty$, we know that $$\P(X > t) \leq \P(|X|^n > t^n) \leq \frac{\E |X|^n}{t^n},$$
i.e. the tail behaves like $O(t^{-n})$. In case of finite variance we only knew that the tail behaves like $O(t^{-2})$ for example. Or in simple words - having higher moments that very big values are taking with smaller probability.

Let us now come to the interesting question - do the moments uniquely determine the distribution? This is true in quite large generality, but not always. We will here prove a partial result:

\begin{prop}\label{prop:momentsagree}
Let $X, Y$ be two almost surely bounded random variables, i.e. r.v. such that almost surely $X \in [-A,A]$ and $Y \in [-A, A]$ for some $A > 0$. Suppose further that $\E X^n = \E Y^n$ for every $n \in \N$. Then $X$ and $Y$ have the same law.
\end{prop}

Before embarking on the proof, observe that trivially for bounded random variables all moments do exist - namely, if $X$ is bounded then every $|X|^n$ is bounded too. The proof we give relies on the following beautiful result, saying that one can approximate each continuous function on a finite interval arbitrary well using polynomials:

\begin{thm}[Stone-Weierstrass]\label{thm:sws}
Let $f$ be a continuous function on some interval $I = [-A, A]$. Then $f$ can be uniformly approximated by polynomials: i.e. there is a sequence of polynomials $(P_n)_{n \geq 1}$ such that $(P_n)_{n \geq 1}$ converges to $f$ in $(C(I, \R), d_\infty)$, where as usual $d_\infty(f,g) = \sup_{x \in I} |f(x) - g(x)|$.
\end{thm}

Most likely, you will see the proof of this theorem in several courses from several points of view. There is a short probabilistic, but non-examinable proof at the end of the subsection. Let us here see how it implies the proposition.

\begin{proof}[Proof of Proposition \ref{prop:momentsagree}]
The proposition follows rather easily from Stone-Weierstrass theorem. Indeed, by the assumption and by linearity of expectation, we see that $\E P(X) = \E P(Y)$ for each polynomial $P$. 

Our aim is to use Proposition \ref{prop:charg}, i.e. to prove that $\E \widehat g(X) = \E \widehat g(Y)$ for all continuous bounded $\widehat g$. Notice that any such $\widehat g$ gives rise to a continuous function $g: [-A,A] \to \R$, by restriction. Moreover as $X, Y \in [-A,A]$ almost surely, we see that $\E \widehat g(X) = \E g(X)$ and hence it suffices to argue that $\E g(X) = \E g(Y)$ for continuous functions on $[-A,A]$.

Given such a function $g$, by the Stone-Weierstrass theorem for every $\eps > 0$, there is some polynomial $P_\eps$ such that $d_\infty(g, P_\eps) < \eps$. As $\E P_\eps(X) = \E P_\eps(Y)$, we can write
$$|\E g(X) - \E g(Y)| = |\E g(X) - \E P_\eps (X) + \E P_\eps(Y) - \E g(y)|,$$
and bound this from above using by triangle inequality by
$$|\E \left(g(X)-P_\eps(X) \right)| +|\E \left(g(Y)-P_\eps(Y) \right)|.$$
Further, $|\E \left(g(X)-P_\eps(X) \right)| \leq \E|g(X)-P_\eps(X)|$. But now as $X \in [-A,A]$ almost surely, and $|g(x) - P_\eps(x)| < \eps$ for $x \in [-A, A]$, we see that $|g(X) - P_\eps(X)| < \eps$ almost surely, and hence by Proposition \ref{prop:grv} we deduce that $\E|g(X)-P_\eps(X)| < \eps$.

Hence we conclude that $|\E g(X) - \E g(Y)|  \leq 2\eps$ and as $\eps > 0$ was arbitrary we conclude that $\E g(X) = \E g(Y)$. As $g$ was arbitrary, the proposition now follows from Proposition \ref{prop:charg}.
\end{proof}

For variables that do not have finite support, this characterisation can fail for several reasons. 
First, of course all moments might not exist and then only the few existing moments might not characterize the distribution. %For example, if you define discrete random variables $X_1$ and $X_2$ with supports $\Z \setminus \{0\}$ and $2\Z \setminus \{0\}$ respectively by setting $\P(X_1 = k) = ck^{-3}$ and $\P(X_2 = 2k) = ck^{-3}$ with $c = \frac{1}{2\sum_{k \geq 1}k^{-3}}$, then $X_1, X_2$ are integrable with zero mean by symmetry. However neither admits a second moment (see Exercise sheet) and they are also not equal in law as their supports are different. 
Second, even if all moments exist, they might grow too quickly to characterize the distribution:

\begin{exo}[Moment problem]
Let $X$ be a standard normal random variable. Prove that $W = \exp(X)$ admits all moments and calculate these moments. 
Let $a > 0$, and consider a discrete random variable $Y_a$ with support $$S_a = \{a e^m: m \in \Z\}$$ and defined by $$\P(Y_a = ae^m) = \frac{1}{Z}a^{-m}e^{-m^2/2}$$ with $Z = \sum_{m \in \Z} a^{-m}e^{-m^2/2}$ (why is it finite?). Show that $Y_a$ admits all moments and that moreover for every $n \in \N$, $\E W^n = \E exp(Xn) = \E Y_a^n$.
\end{exo}

\subsubsection{Moment generating function}

We considered moments of random variables and saw that they might give a useful countable collection of numbers that fully characterizes the underlying random variable. But what if instead of moments we look at some other family of functions $g(X)$ and their expectations? It comes out that a very useful family is directly related to moments: we consider $\E e^{t X}$ for all $t \in \R$ such that $e^{t X}$ is integrable.

%\begin{thm}[Moment generating function]
%Suppose $X$ is a random variable such that $\exp(tX)$ is integrable for some interval $I = (-c,c)$ around $0$. We say that $X$ admits a moment-generating function (MGF) in a neighbourhood around $0$. Denoting $M_X(t) = \E \exp(t X)$ for $t \in I$, we have that
%\begin{itemize}
%	\item $M_X(t)$ is a smooth function on $I$ with $M_X(0) = 1$ and with derivative $M^{(n)}_X(0) = \E X^n$. 
%	\item In particular, if $X,Y$ are almost surely bounded, then if $M_X(t) = M_Y(t)$ for all $t$ in some open interval around $0$, implies that $X$ and $Y$ agree in law.
%\end{itemize}
%\end{thm}

%\begin{proof}
%	Only the first part requires a proof, as the second part then follows from Proposition \ref{prop:momentsagree}.
%	We can also directly see that $M_X(0) = 1$.
	
%	As $|X|^n \leq c_1\left(\exp(tX) + \exp(-tX)\right)$, we deduce that $X$ admits all moments. Similarly $\exp(|tX|) \leq \exp(tX) + \exp(-tX)$ and thus we deduce that $\exp(|tX|)$ is also integrable for all $t \in (-c,c)$. Further, notice that for every $\eps > 0, n \in \N$, we also have that $x^n\exp(tx) \leq C_{n,\eps} \leq \exp(|t+\eps||x|)$, and hence also $X^n\exp(tX)$ is integrable for every $t \in (-c,c)$ and every $n \in \N$. 
	
%	Now, we can write for $t_0, t \in (-c,c)$ the Taylor expansion of $\exp(tx)$ around $t_0$:
%	$$\exp(tx) = \exp(t_0x)\left(1 + (t-t_0)x + \frac{(t-t_0)^2}{2}x^2\exp(t_{\theta,x} x)\right),$$
%	with $t_{\theta,x}$ in the interval between $t_0$ and $t$ and continuous in $x$.
			%\sum_{k = 1}^n \frac{t^k}{k!}x^k + \frac{t^{n+1}}{(n+1)!}x^{n+1}\exp(t_\theta x),$$
%	Hence $t_{\theta,X}$ is a random variable and we can write
%	$$\exp(tX) = \exp(t_0X)\left(1 + (t-t_0)X + \frac{(t-t_0)^2}{2}X^2\exp(t_{\theta,X} X)\right),$$
%	But now all the terms are integrable, and hence
%	$$M_X(t) - M_X(t_0) = (t-t_0)\E \left(\exp(t_0 X) X\right) + \frac{(t-t_0)^2}{2}\E\left( X^2 \exp((t_0+t_{\theta,X}) X)\right).$$	
%	Thus as  $X^2 \exp((t_0+t_{\theta,X}) X) \leq X^2\exp(|t_0+t_{\theta,X}| |X|)$ almost surely and the latter is integrable, we have
%	\[|\frac{1}{t-t_0}\left[M_X(t) - M_X(t_0)\right] - \E(\exp(t_0 X) X)|= O(|t-t_0|).\]
%	Taking the limit $t \to t_0$, we see that $M_X^{(1)}(t_0)$ exists and equals $\E(\exp(t_0 X) X)$. In particular setting $t_0 = 0$, we see that $M_X^{(1)}(0) = \E X$. 
	
%	The case of higher derivatives follows completely analogously. 	
%\end{proof}

\begin{defn}[Moment generating function]
If $X$ is a random variable such that $\exp(tX)$ is integrable for some interval $I = (-c,c)$ around $0$. We say that $X$ admits a moment-generating function (MGF) in a neighbourhood around $0$ and denote $M_X(t) = \E \exp(t X)$ for $t \in I$.
\end{defn}

The name comes from the fact that when $M_X(t)$ exists in a small interval, we can write
$$M_X(t) = \E(\exp(tX)) = \E(\sum_{n \geq 1} \frac{t^nX^n}{n!}).$$
Checking that you can exchange the summation and the expectation (On the Exercise sheet), one obtains
$$M_X(t) = \sum_{n \geq 1} \frac{t^n}{n!} \E X^n.$$
In particular, from here it is not hard to deduce that if we look at $M_X(t)$ as a function of $t$, then in fact moments $\frac{d^n}{dt^n} M_X(t)$ evaluated at $t = 0$ just gives the $n$-th moment. We will skip this calculation that is not examinable.

It comes out that MGF-s also characterize the distribution. We state this result and you are free to use it, though the proof is out of the scope of this course:

\begin{thm}[MGF determines the distribution (admitted)]
Let $X, Y$ be random variables such that $M_X(t)$ and $M_Y(t)$ exist in some open interval around $0$, and moreover $M_X(t) = M_Y(t)$ in this interval. Then $X$ and $Y$ have the same law.
\end{thm}

In fact moment generating functions and this concrete theorem for MGFs also nicely generalize to random vectors:

\begin{thm}[MGF for random vectors (admitted)]\label{thm:mgfvrv}
Let $\ol X$ be a random vector taking values in $\R^n$ such that $\E e^{\langle \ol t, \ol x \rangle} < \infty$ for $\ol t$ in some open neighbourhood of $0$.\footnote{Here $\langle \cdot, \cdot \rangle$ denotes the inner product in $\R^n$} We then call $M_{\ol X}(\ol t) = \E e^{\langle \ol t, \ol x \rangle}$ the moment generating function of $\ol X$. Again, if MGFs of two random vectors $\ol X$ and $\ol Y$ are equal in some neighbourhood around $0$, then $\ol X$ and $\ol Y$ have the same law.
\end{thm}

These two results are extremely useful. First, as an application MGF-s can be used to determine independence:

\begin{lemma}[Independence and MGF]
Let $X, Y$ be random variables such that there exists an open interval $I\subset\R$ containing zero such that $M_X(t)$ and $M_Y(t)$ exist for all $t\in I$. Then $X, Y$ are independent iff for each $t,s\in I$, $M_X(t)M_Y(s) = M_{(X,Y)}((t,s))$.
\end{lemma}

I didn't have time to do this proof in the course, so it is admitted. But I will still give the proof here.
\begin{proof}
Firstly, if $X, Y$ are independent then the condition follows directly from Proposition \ref{prop:indexpg}. Indeed, for each $t, s \in I$ we can take $g(x) = \exp(tx)$ and $h(y) = \exp(sy)$. Then $M_X(t) = \E g(X)$ and $M_Y(s) = \E h(Y)$ and by assumption both are integrable. Hence that proposition implies that $M_X(t)M_Y(s) = \E \exp(tX + sY) = M_{(X,Y)}(t,s)$.

The other direction is a direct application of Theorem \ref{thm:mgfvrv}: indeed, let $(X, Y)$ be a pair of random variables such that for each $t,s\in I$, $M_X(t)M_Y(s) = M_{(X,Y)}((t,s))$. Further, let $(\tilde X, \tilde Y)$ be a pair of independent random variables such that $\tilde X$ has the law of $X$ and $\tilde Y$ has the law of $Y$. In particular then $M_{X}(t) = M_{\tilde X}(t)$ and $M_{Y}(s) = M_{\tilde Y}(s)$for all $t,s \in I$. 

Now, by the first part $M_{\tilde X}(t)M_{\tilde X}(s) = M_{(\tilde X, \tilde Y)}((t,s))$. We conclude that $M_{(X, Y)}((t,s)) = M_{(\tilde X, \tilde Y)}((t,s))$ and deduce from Theorem \ref{thm:mgfvrv} that $(X, Y)$ and $(\tilde X, \tilde Y)$ have the same joint law. In particular $X$ and $Y$ are independent.
\end{proof}

Second, it really makes some things much easier, in particular calculations with Gaussians:

\begin{exo}
Prove $\ol X$ is a Gaussian vector with mean $\ol \mu$ and covariance $C$ if and only if $M_{\ol X}(\ol t) = \exp(\langle \ol t, \ol \mu \rangle + \frac{1}{2}\langle \ol t, C \ol t\rangle).$ Deduce that
\begin{itemize}
	\item If $X$ is a standard Gaussian on $\R^n$, then so is $OX$ for every orthogonal $n \times n$ matrix.
	\item The Gaussian vector with mean $\ol \mu$ and covariance $C$ on $\R^n$ can be written as $A\ol Y + \ol \mu$, where $\ol Y$ is the standard Gaussian on $\R^n$ and $C = \sqrt{A A^T}$ (You may assume such a matrix $A$ exists, but you have seen it in linear algebra!)
\end{itemize}
\end{exo}

Thus having an MGF can really simplify and reduce calculations. The drawback of moment generating functions is that they do not always exist. 

\begin{exo}
	Consider the log-normal random variable, i.e. $Z = \exp(X)$ where $X$ is a standard Gaussian. Prove that there is no open interval around $0$ such that $M_t(Z)$ exists in this interval.
\end{exo}

This can be mended by considering what is called the characteristic function:

\begin{defn}[Characteristic function] 
Let $X$ be a random variable. Then 
$$c_X(t) = \E e^{it X} = \E \cos(tX) + i \E \sin(tX)$$ 
is called the characteristic function of $X$.
\end{defn}

The nice thing is that the characteristic function exists for all $t \in \R$ as both $\cos(t X)$ and $\sin(t X)$ are trivially bounded. Moreover, it uniquely characterizes the law of the random variable and in case of random variables with density, it corresponds to the Fourier transform of the density. But this and much more will already topic of a future course...

%In the very first part - in determining the MGF of Gaussian vectors - a calculation is necessary. However, it is basically the same calculation as showing that every Gaussian can be represented as an affine map of the standard Gaussian (Exercise 2.5, point 1), so instead of rewriting the calculation, we will use this point.
%We will here prove the first part assuming Exercise 2.5, point 1, which is on sheet 7. A direct calculation making the proof stand-alone would work equally well -- some calculation has to be done somewhere, either directly here or when showing that affine transformations of Gaussians are Gaussian using density.

%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\begin{proof}
Notice that as soon was we show that a $\Nn(\ol \mu, C)$ has the MGF $\exp(\langle \ol t, \ol \mu \rangle + \frac{1}{2}\langle \ol t, C \ol t\rangle)$, Theorem \ref{thm:mgfvrv} implies that any random vector with this MGF has to be the Gaussian vector with mean $\ol \mu$ and covariance $C$.

So let us argue that a Gaussian vector does have this MGF. First, notice that $\ol X - \ol \mu$ is a Gaussian vector of mean zero and the claim is equivalent to showing that $M_{\ol X - \ol \mu} = \exp(\frac{1}{2}\langle \ol t, C \ol t\rangle).$
Thus it suffices to consider a centred Gaussian vector $\ol Y \sim \Nn(0, C)$. We calculate:
\[M_{\ol Y}(\ol t) = \int_{\R^n} \exp(\frac{1}{2}\langle \ol x, \ol t\rangle)\frac{1}{(2\pi)^{n/2}\sqrt{\det C}}\exp(-\frac{1}{2}\langle \ol x, C^{-1}\ol x\rangle)d\ol x.\]
But now we can write 
\[\langle \ol x, \ol t \rangle -\frac{1}{2}\langle \ol x, C^{-1}\ol x\rangle = -\frac{1}{2}\langle \ol x - C\ol t, C^{-1}(\ol x - C \ol t) \rangle + \frac{1}{2}\langle \ol t, C \ol t \rangle.\]
Noticing that 
\[\frac{1}{(2\pi)^{n/2}\sqrt{\det C}}\exp(-\frac{1}{2}\langle \ol x - C\ol t, C^{-1}(\ol x - C \ol t) \rangle)\]
	is the density of a Gaussian vector with cov. matrix $C$ and mean $C \ol t$, we deduce that 
\[M_{\ol Y}(\ol t) = \exp(\frac{1}{2}\langle \ol t, C \ol t \rangle)\int_{\R^n} \frac{1}{(2\pi)^{n/2}\sqrt{\det C}}\exp(-\frac{1}{2}\langle \ol x - C\ol t, C^{-1}(\ol x - C \ol t) \rangle) d\ol x =  \exp(\frac{1}{2}\langle \ol t, C \ol t \rangle)\]
	giving the MGF for the Gaussian vector.
	
The other claims follow from the most general claim: consider $A$ a surjective map from $\R^n \to \R^m$ and let $\ol s \in \R^m$. Then
\[M_{A \ol X}(\ol s) = \E \exp(\langle A\ol X, \ol s \rangle) = \E \exp(\langle \ol X, A^T \ol s \rangle) = M_{\ol X}(A^T \ol s).\]
But now by the first part
\[M_{\ol X}(A^T \ol s) = \E \exp(\langle A^T \ol s, \ol \mu\rangle+ \frac{1}{2}\langle A^T \ol s, CA^T\ol s\rangle) = \exp(\langle \ol s, A\ol \mu \rangle + \frac{1}{2}\langle \ol s, ACA^T\ol s \rangle).\]
$ACA^T$ is symmetric as $C$ is. Moreover, as $C$ can be written as $BB^T$ we have $$\langle ACA^T \ol x, \ol x \rangle = \langle B^TA^T \ol x, BA \ol x \rangle \geq 0.$$ As moreover $ACA^T$ is of full-rank, it is also positive definite and the claim follows by recognizing the MGF of a Gaussian vector
of mean $A \ol \mu$ and covariance matrix $ACA^T$.
\end{proof}
\begin{rem}
	
If instead of computing the MGF for the Gaussian vector, one assumes Exercise \ref{exo:gaussvect} point (1), one can alternatively deduce the MGF as follows. 

Let $\ol Z$ be the standard Gaussian vector in $\R^n$. By Exercise \ref{exo:gaussvect} point 1, we know that there is some pos. definite matrix $A$ such that $AA^T = C$ and $\ol X$ has the same law as $A\ol Z + \ol \mu$.
\[M_{\ol X}(\ol t) = \E\exp(\ip{\ol t, \ol X}) = \E\exp(\ip{\ol t, \ol X})  = \E\exp(\ip{\ol t, \ol \mu} + \ip{\ol t, A\ol Z}) = \exp(\ip{\ol t, \ol \mu})\E\exp(\ip{A^T\ol t, \ol Z}).\]
As the coordinates of the standard Gaussian vector are independent, we can use Proposition \ref{prop:indexpg} to write
\[\E \exp(\ip{s, \ol Z}) = \E \exp(\sum_{i = 1}^n s_i Z_i) = \Pi_{i = 1}^n\E \exp( s_i Z_i).\]
So it remains to calculate the MGF for the standard Gaussian. This again follows from complete-the-square trick:
\[\E \exp(s_i Z_i) = \frac{1}{\sqrt{2\pi}} \int_\R \exp(s_i x)\exp(\frac{-x^2}{2})dx = \frac{1}{\sqrt{2\pi}}\int_\R \exp(\frac{s_i^2}{2})\exp(\frac{-(x-s_i)^2}{2})dx = \exp(\frac{s_i^2}{2}).\]
Hence, $\E \exp(\ip{s, \ol Z}) = \exp(\frac{\| s\|^2}{2})$ and thus we find the MGF of the Gaussian vector $\ol X$:
\[M_{\ol X}(\ol t) = \E\exp(\ip{\ol t, \ol \mu} +\ip{A^T\ol t, \ol Z})  = \exp(\ip{\ol t, \ol \mu}+\frac{\| A^T \ol t\|^2}{2}) = \exp(\ip{\ol t, \ol \mu}+\frac{\ip{\ol t, C \ol t}}{2}).\]
\end{rem}
\end{comment}
% We will not really get into this, but still mention for further reference the following central theorem of probability:

%\begin{thm}[MGF determines the distribution (admitted)]
%For any random variable $X$, the characteristic function $c_X(t) = \E e^{it X}$ exists for all $t \in \R$. Moreover,the characteristic functions of two random variables $X, Y$ are equal if and only if $X$ and $Y$ have the same law. 

%Moreover, a similar statement holds for random vectors: for $\ol X$ taking valuse in $\R^n$ and any $\ol t \in \R^n$, we define $c_{\ol X}(\ol t) = \E e^{i\langle \ol t, \ol x \rangle}$, where $\langle \cdot, \cdot \rangle$ denotes the inner product in $\R^n$. Again $c_{\ol X}(\ol t)$ exists for all $\ol t \in \R^n$ and the characteristic functions of two random vectors $\ol X$ and $\ol Y$ are equal if and only if $\ol X$ and $\ol Y$ have the same law.
%\end{thm}



\subsection{$\star$ Proofs of some auxiliary results (non-examinable) $\star$}


[$\star$ non-examinable section begins $\star$]

In this non-examinable section we present proofs of some auxiliary results. I do recommend the probabilistic proof of the Stone-Weierstrass theorem, it is a gem!

First let us prove the Cauchy-Schwarz inequality:

\begin{proof}[Proof of Cauchy-Schwarz inequality]
Define $\widehat Y, \widehat X$ as $\widehat Y = \frac{Y}{\sqrt{\E (Y^2)}}$ and  $\widehat X = \frac{X}{\sqrt{\E (X^2)}}$. This is possible as $X^2, Y^2$ are integrable.
Notice that by definition then $\E (\widehat Y^2) = \E (\widehat X^2) = 1$. Moreover, the Cauchy-Schwarz inequality is then equivalent to
\begin{equation}\label{eq:cshat}
\E(|\widehat X \widehat Y|) \leq 1.
\end{equation}
But now for every $\omega \in \Omega$, we have that $|\widehat X(\omega) \widehat Y(\omega)| \leq \frac{1}{2}(\widehat X^2(\omega) + \widehat Y^2(\omega))$.
Thus we see that $|XY|$ is integrable and by properties of expectation 
$$\E(|\widehat X\widehat Y|) \leq \frac{1}{2}\E(\widehat X^2 + \widehat Y^2) = 1,$$
and the inequality \ref{eq:cshat} follows.

The equality holds if and only if $|\widehat X \widehat Y| =\frac{1}{2}(\widehat X^2 + \widehat Y^2)$ almost surely, which in turn holds if and only if $|\widehat X| = |\widehat Y|$ almost surely. As $\widehat Y, \widehat X$ are normalized versions of $X, Y$, this is turn holds if $|X| = \lambda |Y|$ almost surely for some $\lambda > 0$.
\end{proof}

Next, it is time to prove Jensen's inequality. We will do it using the following chracterization of convex functions:

\begin{itemize}
	\item $\phi: \R \to \R$ is convex if and only if for every $x \in \R$, there is some $c = c(x) \in \R$ so that for every $y \in \R$, we have that $\phi(x+y) \geq \phi(x) + c_x y$.
\end{itemize}

\begin{proof}[Proof of Jensen's inequality]
Consider  $x = \E X$ and $y = X - \E X$. Then injecting this in the formulation of convexity just above, we obtain
$$\phi(X) \geq \phi(\E X) + c (X - \E X)$$
almost surely. Taking now expectation, and using the fact that $\E (X - \E X)) = 0$, we deduce
$$\E \phi(X) \geq \phi(\E X)$$
as claimed.
\end{proof}

And finally the cute probabilistic proof of the Stone-Weierstrass theorem:

\begin{proof}[Proof of Theorem \ref{thm:sws}]
	By translation and scaling, it is simple to see that it suffices to prove the theorem for the case $I = [0,1]$ and $f$ continuous on $[0,1]$.
	Now for every $x \in [0,1], n \in \N$ let $X_{n,x}$ be a Binomial random variable of parameters $(n,x)$
	We define $P_n(x) = \E f(X_{n,x}/n).$
	By Proposition \ref{calcexp} we then have
	$$P_n(x) = \sum_{k = 0}^n f(k/n) {n \choose k}x^k(1-x)^{n-k},$$
	and hence $P_n(x)$ is a polynomial of order $n$ in $x$.
	
	We claim that $P_n(x)$ converges to $f$ uniformly.
	First, notice that as $f$ is continuous on $[0,1]$ it is bounded by some $M$, and uniformly continuous - i.e. for every $\eps > 0$, there is some $\delta_\eps > 0$ so that if $|x-y| < \delta_\eps$, then $|f(x) - f(y)| < \eps.$
	
	Now, write
	$$|P_n(x) - f(x)| = |\E(f(X_{n,x}/n) - \E f(x)| \leq \E|f(X_{n,x}/n) - f(x)|.$$
	The crux is something we have already seen: in fact $X_{n,x}$ is very close to its expectation $xn$ for $n$ large.
	Indeed, we by Chebyshev's inequality and the fact that $\Var(X_{n,x}) = nx(1-x)$
	$$\P(|X_{n,x}/n - x| > t/n) = \P(|X_{n,x} - nx| > t) \leq \frac{\Var X_{n,x}}{t^2} = \frac{nx(1-x)}{t^2}.$$
	In particular, if we choose $t = n^{2/3}$, then  $\P(|X_{n,x}/n - x| > n^{-1/3}) < n^{-1/3}$.
	
	To use this fact we write:
	$$\E|f(X_{n,x}/n) - f(x)| = \E\left(|f(X_{n,x}/n) - f(x)|1_{|X_{n,x}/n - x|>n^{-1/3}}\right) + \E\left(|f(X_{n,x}/n) - f(x)|1_{|X_{n,x}/n - x|<n^{-1/3}}\right).$$
	Then as $|f(x)| < M$ for $x \in [-A, A]$, we can bound the first term by $$M\E 1_{|X_{n,x}/n - x|>n^{-1/3}} = M \P(|X_{n,x}/n - x| > n^{-1/3}) < Mn^{-1/3}.$$
	Fix some $\eps > 0$ and choose $n$ large enough so that $n^{-1/3} < \delta_\eps$. We can bound the second term by
	$$\E \eps 1_{|X_{n,x}/n - x|<n^{-1/3}} \leq \eps.$$
	Hence if we also require that  $n^{-1/3} < \eps$, we obtain altogether
	$$\E|f(X_{n,x}/n) - f(x)| < Mn^{-1/3} + \eps \leq (M+1)\eps.$$
	As this is uniform in $x$ and holds for arbitrary $\eps$, the theorem follows.
\end{proof}

[$\star$ non-examinable section ends $\star$]


\section{Limit theorems}

In this section, we will look at infinite sequences of events and infinite sequences of random variables. Some questions we will be interested in:
\begin{itemize}
	\item When can we be sure that at least one of the events $A_1, A_2, \dots$ happens? For example, under what conditions can you guarantee that you will eventually win with a lottery or get a 6 in the exam? Or suppose, you start a random walk in Manhatten - at every corner you choose uniformly one of four directions. Will you ever get back to your hotel?
	\item Under what criteria do only finitely many of the events $A_1, A_2, \dots$ of a sequence happen? This could for example be used to model whether an infectious disease will only have a limited spread
	\item When can we say something about the limit of the sequence of random variables $X_1, X_2, \dots $? In what senses can we talk about convergence? We have already seen some vague statements in the lines that $Bin(n,\lambda/n)$ converge to Poisson or that the empirical average of i.i.d. random variables converges to its expectation. What are the right mathematical notions and statements?
\end{itemize}
%Such questions come up naturally, and sometimes are become tractable and even easy. Indeed, looking at limiting situations makes things sometimes clearer. For example, somtimes to gain understanding of complex random systems, e.g. like complex networks, it is useful to see what happens if we let the size of the network go to infinite. Can we talk of some infinite network?

%%%% A COMMMENT ON SPACES WITH INFINITELY MANY EVENTS RANDOM VARIABLES!


%This is not dissimilar to using notions like continuity, differential equations etc - they all just make mathematical models much much simpler.


\subsection{Infinite collections of events and random variables}

Before stating a few interesting limit theorems, let us start by formalizing some of the limiting notions in the context of events. Fix a probability space $(\Omega, \F, \P)$ and a sequence of events $E_1, E_2, \dots$ that could for example be repetitions of the same random situation, like repetitive coin tosses. \footnote{As discussed, it is not trivial to construct a probability space on which we would have an infinite sequence of independent coin tosses, but here we take this for granted.} 

Recall that if we say $E_i$ is an event we mean that $E_i \subseteq \Omega$ and $E_i \in \F$. Each $\omega$ gives a random state of the universe, and $\omega \in E_i$ if the event $E_i$ happens for this particular state. 

Now, we say that
\begin{itemize}
	\item First, we could ask whether at least one event of the sequence $(E_n)_{n \geq 1}$ happens. By definition, $\{\omega \in \Omega: \omega \in E_i \text{ for some }i\} = \bigcup_{n \geq 1}E_n$. Sometimes one says that '$E_i$ happens eventually'. An example would be the following example from an earlier example sheet: when we toss independent coins, we eventually obtain heads with full probability (this also follows from the lemma just below). Notice that there is some sequence of tosses that gives no heads - the sequence $TTTTT\dots$, however as it has $0$ probability, it does not matter.
	\item Second, we might ask whether infinitely many events $E_i$ happen. Let us first formalise it: one can check that $$\{\omega \in \Omega: \omega \in E_i \text{ for infinitely many }i\} = \bigcap_{m \geq 1}\bigcup_{n \geq m} E_n.$$ The event described this way is also sometimes denoted by $\limsup_{n \geq 1} E_n$. In the case of coin tossing, each $E_i$ could mean that the $i$-th toss comes up heads, and we have seen that in the case of independent coins, indeed $E_i$ would happen infinitely often with full probability. 
	\item Finally, we might ask whether all but finitely many $E_i$ happen. One can again see (on the exercise sheet), that $$\{\omega \in \Omega: \omega \in E_i \text{ for all but finitely many }i\} = \bigcup_{m \geq 1}\bigcap_{n \geq m} E_n.$$ This event is also denoted by $\liminf_{n \geq 1} E_n$. An example situation would be as follows: you start with 10 CHF, and as long as you have some money left, you bet with the European central bank (that can always print more money when needed!) on whether independent coin tosses are head or tails. The winner gets 1 CHF, and the loser loses 1 CHF. It's a mathematical fact that after almost surely, after finitely many bets you are left with $0$ CHF. So if we denote by $E_i$ the event after $i$ bets you are bankrupt, this event fails only finitely many times.
\end{itemize}

Here are some useful criteria to study such events. First, a very naive criterion:

\begin{lemma}
Let $E_1, E_2, \dots$ be independent events of probability $p_i$. Then $\P(\bigcup_{i \geq 1} E_i) = 1$ if and only if $\Pi_{i=1}^n (1-p_i) \to 0$ as $n \to \infty$.
\end{lemma}

\begin{proof}
	This is on the exercise sheet.
\end{proof}

For example, if each event happens with the same probability $p$, then $\Pi_{i = 1}^n p_i = p^n$, which clearly goes to zero. So even if you toss a coin that comes up heads with probability $0.00001$, you will eventually see heads.

A verey useful criteria for verifying that some even cannot happen but finitely many times is given by the first Borel-Cantelli lemma:

\begin{lemma}[Borel-Cantelli I]
Let $E_1, E_2, \dots$ be any sequence of events on a common probability space $(\Omega, \F, \P)$. If $\sum_{n \geq 1}\P(E_n) < \infty$, then almost surely only finitely many events $E_i$ happen, i.e. $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 0$.
\end{lemma}

Notice that we are not assuming anything on the dependence or independence of the events $E_i$! Also, this lemma does not say that there is some fixed number $1000$ of events that happen. Indeed, exactly how many events can happen and exactly which events happen depends on $\omega \in \Omega$. 

For example, consider a sequence of unfair coins with probability of heads for the $n$-th coin given by $n^{-2}$. If $E_n$ denotes the event of obtaining heads on the $n$-th toss, then $\sum_{n \geq 1} \P(E_n) < \infty$. Thus, by the lemma, we see that almost surely one obtains only finitely many heads in an infinite sequence of coin tosses. However, notice that whether you obtain $10$ or even $100$ heads depends on the exact sequence of tosses, i.e. on the 'randomness' encoded by the state $\omega \in \Omega$. 

\begin{proof}
Fix some $\eps > 0$. As $\sum_{n \geq 1}\P(E_n) < \infty$, we can find some $n_0 \in \N$ such that $\sum_{n \geq n_0} \P(E_n) < \eps$.
But now as $\P(A \cap B) \leq \P(B)$, 
$$\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) \leq \P(\bigcup_{n \geq n_0} E_n) \leq \sum_{n \geq n_0} \P(E_n) < \eps,$$
where in the last inequality we use the union bound. As $\eps$ was arbitrary, the claim follows.
\end{proof}

The short proof might make you suspicious if it is of any use, but we will see shortly how it is for example to obtain convergence of random variables.

This is partly complemented by the second Borel-Cantelli lemma, which gives a condition for infinitely many events to happen. Notice that here we again ask for independent events.

\begin{lemma}[Borel-Cantelli II]
Let $E_1, E_2, \dots$ be a sequence of independent events on a common probability space $(\Omega, \F, \P)$. Suppose that $\sum_{n \geq 1}\P(E_n) = \infty$. Then almost surely infinitely many events $E_i$ happen, i.e. $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 1$.
\end{lemma}

\begin{proof}
On the exercise sheet
\begin{comment}
We have that 
$$\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 1 - \P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c)$$
and hence it suffices to show that $\P(\bigcup_{m \geq 1}\bigcap_{n \geq m}E_n^c) = 0$.
By the union bound
$$\P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c) \leq \sum_{m \geq 1} \P(\bigcap_{n \geq m} E_n^c).$$
Further, as $E_i$ are independent, so are $E_i^c$, and hence
$$\P(\bigcap_{n \geq m} E_n^c) = \Pi_{n \geq m} \P(E_n^c) = \Pi_{n \geq m}(1-\P(E_n)).$$
Now using the inequality $1-x \leq e^{-x}$ for $x \in [0,1]$, we can bound the RHS further by $\exp(-\sum_{n\geq m} \P(E_n))$. 
But the sum in the exponential equals $\infty$ by the assumption. Thus $\P(\bigcap_{n \geq m} E_n^c) = 0$, hence $\P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c)  = 0$ and we conclude.
\end{comment}
\end{proof}


\subsection{Convergence of random variables}
When we switch from events to sequences of random variables $X_1, X_2, \dots$, the first question is again - which questions can we even ask? 

For example some questions that we might be interested in are:
\begin{itemize}
    \item Is some value $\geq k$ attained by the sequence of random variables?
    \item Are all but finitely many of $X_i$ positive?
    \item Is the sequence of random variables bounded in absolute value?
    \item Does it converge?
\end{itemize}

For the first one measurability is clear, as we can write it as the union $\bigcup_{i \geq 1} \{X_i \geq k\}$, similarly for the second one. For the third one, already some thought might be required: the event that the sequence of random variables is bounded in absolute value by $M \in \N$ is given by $E_M := \bigcap_{i \geq 1} \{|X_i| \leq M\}$. But we want to allow different bounds for different sequences. So we have to take also a union over $M$ to get $\bigcup_{M \in \N} E_M$, which again shows that the question makes fully sense. The fourth one we state as a lemma, but it is easy to check:

\begin{lemma}
Let $X, X_1, X_2, \dots$ be random variables on a common probability space.
Show that the sets $E := \{\omega: (X_i(\omega))_{i \geq 1} \text{ converges}\}$ and $E_\infty :=  \{\omega: (X_i(\omega))_{i \geq 1} \text{ converges to }X(\omega)\}$ are events, i.e. are measurable.
\end{lemma}

\begin{proof}
Is left to you to do.
\end{proof}

\subsubsection{Almost sure convergence and the Law of large numbers}

The exercise above introduces also what is maybe the most natural notion of convergence for random variables on the same probability space. For this notion, the setting is as follows: we have some random variables $X_1, X_2, \dots$ defined on the same probability space $(\Omega, \F, \P)$ and we just ask about the event $\{\omega \in \Omega: X_n(\omega) \text{ converges}\}$ as above. For example, again with coin tossing you might toss coin a hundred times and take the average, and then a thousand times and take the average. Do these averages converge? 

\begin{defn}[Almost sure convergence]
Let $X, X_1, X_2, \dots$ be random variables defined on a common probability space $(\Omega, \F, \P)$. If we have that $\P(\{\omega \in \Omega: (X_n(\omega)_{n \geq 1} \to X(\omega)\}) = 1$, then we say that the sequence $(X_n)_{n \geq 1}$ converges almost surely to $X$.
\end{defn}

We saw that these events are indeed measurable. An useful and nice criteria for almost sure convergence comes from the Borel-Cantelli I:

\begin{lemma}\label{lem:convcr}
Let $X, X_1, X_2, \dots$ be random variables defined on a common probability space $(\Omega, \F, \P)$. 
If we can find a decreasing sequence $(a_n)_{n \geq 1}$ of non-negative numbers converging to zero such that for the events $E_n := \{\omega: |X_n(\omega) - X(\omega)| > a_n\}$ we have $\sum_{n \geq 1} \P(E_n) < \infty$, then $X_i$ converges almost surely to $X$.
\end{lemma}

\begin{proof}
By Borel-Cantelli I, only finitely many of $E_n$ happen almost surely, i.e. $\P(\cap_{m \geq 1}\cup_{k \geq m} E_k) = 0$. But now observe that
$$\{\omega: X_n(\omega) \text{ does not converge to }X(\omega)\} \subseteq \cap_{m \geq 1}\cup_{k \geq m} E_k.$$
Indeed, for every $\omega$ such that $X_n(\omega)$ does not converge to $X(\omega)$, there is some $\eps > 0$ and some subsequence $(n_l)_{l \geq 1}$ with $|X_{n_l}(\omega) - X(\omega)| > \eps$. In particular if we let $k$ be such with $a_k < \eps$, we have that $\omega \in E_{n_l}$ for all $n_l > k$ and thus we conclude. 
\end{proof}

One of the most important examples of almost sure convergence is the law of large numbers, of which we already saw one version in Theorem \ref{thm:wllnd}. 

\subsubsection{Law of large numbers}

Let us first restate a more general version of the Weak law of large numbers, i.e. Theorem \ref{thm:wllnd}.

\begin{thm}[Weak law of large numbers (WLLN)]
Let $X_1, X_2, \dots $ be i.i.d. integrable random variables defined on $(\Omega, \F, \P)$. Then as $n \to \infty$, we have that
$$\P(|\frac{\sum_{i = 1}^n X_i}{n} - \E X_1| > \eps) \to 0,$$
i.e. the sequence $S_n = \frac{\sum_{i = 1}^n X_i}{n}$ converges in probability to $\E X_1$.
\end{thm}

As mentioned below, its proof in the case of variables with finite variance is exactly as in the proof of Theorem \ref{thm:wllnd}. It can be strenghtened to give the strong law of large numbers.

\begin{thm}[Strong law of large numbers (SLLN)]
Let $X_1, X_2, \dots $ be i.i.d. integrable random variables defined on $(\Omega, \F, \P)$. Then we have that

$$\P(\frac{\sum_{i = 1}^n X_i}{n} \text{ converges to } \E X_1) = 1,$$
i.e. the sequence $S_n = \frac{\sum_{i = 1}^n X_i}{n}$ converges almost surely to $\E X_1$.
\end{thm}

Roughly, both theorems say that if you repeat the same random experiment independently $n$ times to obtain i.i.d random variables $X_1, X_2, \dots, X_n$ then as $n \to \infty$ the average of $X_i$ converges to the expectation of $X_1$. This is quite remarkable that the distribution of the variables does not play any larger role in this limit - only the integrability and the expectation matter. Both of these theorems are related to so called ergodic theorems, which roughly link the temporal (here $n$) and spatial (here $\E$) averages. But what is the difference of these two theorems? 

The weak law says that if you do independent experiments $X_1,X_2, \dots$ and look at the average outcome of the first $n$ of them with $n$ large, then the random variable you obtain is very close to the constant $\E X_1$. Indeed, for evert $\eps > 0$, if you do sufficiently many experiments then the probability that this random average differs from $\E X_1$ by more than $\eps$ is less than, say, $0.00001$. WLLN does not however say how the consecutive averages behave for a fixed sequence of outcomes.

The strong law on the other hand says exactly that almost surely for any sequence of outcomes, if you look at the average of the first $n$ outcomes and then increase $n$, these averages converge to $\E X_1$. SLLN doesn't look only at snaphots for fixed $n$, but describes for every sequence the evolution of averages.

In both cases, both the integrability and independence are important. You will think about the role of integrability on the example sheet; for necessity of some independence you can consider the case $X_1 = X_2 \dots$. Then the average of $X_1, \dots, X_n$ is just equal to $X_1$ and has no reason to converge to a constant. In general, LLN also holds under some weak dependence, but this is out of scope here.

For the sake of completeness, we rewrite the proof of the special case of WLLN again too, although we stress it is the same proof as that of Theorem \ref{thm:wllnd}.

\begin{proof}[Proof of WLLN for i.i.d. random variables with bounded variance]
	
%We prove it in two steps:\\

%\noindent \textbf(The case of bounded random variables:}

Suppose that $\E X_1^2 < C$. In this case $\E(|S_n-\E X_1|^2)$ is well defined and we can write
$$\E(|S_n-\E X_1|^2) = \sum_{i,j \leq n} n^{-2}\E\left[(X_i-\E X_1)(X_j-\E X_1).\right]$$
But $X_1, X_2, \dots$ are mutually independent and $\E X_j = \E X_1$. Thus we see that if $i \neq j$, then $E\left[(X_i-\E X_1)(X_j-\E X_1).\right] = 0$.
Hence 
$$\E(|S_n-\E X_1|^2) = n^{-2}\sum_{i = 1}^n \Var(X_i) = n^{-1}C \to 0$$
as $n \to \infty$. By Chebyschev inequality we have that
$$\P(|S_n-\E X_1| > \eps) \leq \eps^{-1}n^{-1}C \to 0$$
and and WLLN for random variables with bounded variance follows.

\end{proof}

Notice that we didn't really use independence here - just the fact that $Cov(X_i, X_j) = 0$ for all $i,j$! Moreover, we also didn't use that the variables were i.i.d., we just used that for all $i \geq 1$, we have that $\E X_i^2 < C$ - i.e. the variances are uniformly bounded.

We prove SLLN under even stronger hypothesis. The proof starts very similarly, but then we apply the corollary of Borel-Cantelli lemma from above.

\begin{proof}[Proof of SLLN for i.i.d. random variables with $\E X_i^4 < C$]
	
	%We prove it in two steps:\\
	
	%\noindent \textbf(The case of bounded random variables:}
	
	Suppose that for some $C > 0$, we have $\E X_i^4 < C$. By increasing the value of $C$ (but not the number of notations!) we can assume that for this $C$ also $\E (X_i - \E X_i)^4 < C$ for some $C > 0$ (why?). In this case $\E(|S_n-\E X_1|^4)$ is well defined and we can write $$\E(|S_n-\E X_1|^4) = \sum_{i,j,k,l \leq n} n^{-4}\E\left[(X_i-\E X_1)(X_j-\E X_1)(X_k-\E X_1)(X_l-\E X_1)\right].$$
	Notice that if one index appears only once (e.g. we have $i = 1$, $j = k = l = 2$), then as in the proof of WLLN $$\E\left[(X_i-\E X_1)(X_j-\E X_1)(X_k-\E X_1)(X_l-\E X_1).\right] = 0$$ because of independence and the fact that $\E X_1 = \E X_i$. 
	Hence 
	$$\E(|S_n-\E X_1|^4) = n^{-4}\sum_{i,j \leq n} \E \left[(X_i - \E X_1)^2(X_j - \E X_1)^2\right].$$
	By Cauchy-Schwarz, 
	$$\E \left[(X_i - \E X_1)^2(X_j - \E X_1)^2\right] \leq \E \left[(X_i - \E X_1)^4\right] \leq C.$$
	Thus 
	$$\E(|S_n-\E X_1|^4) \leq C n^{-2}.$$
	We now apply Lemma \ref{lem:convcr} with sequence $a_n = n^{-1/8}$. Indeed, by Markov's inequality
	$$\P(E_n) = (|S_n-\E X_1| > n^{-1/8}) = \P(|S_n-\E X_1|^4 > n^{-1/2}) \leq \frac{\E|S_n-\E X_1|^4}{n^{-1/2}} \leq C n^{-3/2}$$
    and thus $\sum_{n \geq 1}\P(E_n) < \infty$. Hence this Lemma applies the almost sure convergence of $S_n$ to $\E X_1$. 
\end{proof}

\begin{rem}
Again, notice that in this proof we don't use the fact that $X_i$ are identically distributed, we only use that $\E X_i^4 < C$.	You should ask yourself: why did we need in this proof the $4$-th moment, and in WLLN only the $2$-nd moment?
\end{rem}

These two theorems are the basis for the so called frequentist approach to probability. Indeed, we have the following immediate corollary (recall how annoying it was to prove it on the first example sheet!)

\begin{cor}
Let $E_1, E_2, \dots $ be independent events with $\P(E_i) = p$. Then $\frac{\#\{(E_i)_{i \leq n}\text{ that occur}\}}{n}$ converges almost surely to $p$.
\end{cor}

\begin{proof}
This follows directly from SLLN by noticing that $1_{E_1}, 1_{E_2}, \dots $ are i.i.d integrable random variables of expectation $p$.
\end{proof}

So for example, if you have a coin with unknown probability $p$ of obtaining heads. Then to determine $p$, you start tossing the coin, and look at the average number of heads you get in $n$ trials, and then SLLN says that with probability one these averages converge to $p$! It's an interesting question to see 'how fast' it converges to $p$, i.e. how precisely you might know $p$ after, say, 25 or 100 throws...Although answering this question will be outside of the scope of this course, it is in certain settings related to the Central limit theorem, that describes the fluctuations of the average around its mean and is described in the next section.


%\begin{rem}[$\star$ non-examinable $\star$]
%In the spirit of the first half of the course, you might further ask - given the joint laws of any $(X_{i_1}, \dots, X_{i_n})$ for any finite subset $\{i_1, \dots, i_n\}$ of $\N$, can we even define a common probability space $(\Omega, \F, \P)$ such that $X_1, X_2, \dots$ are random variables defined on this space and satisfy the given joint laws? We have argued that this is possible in case $X_1, X_2, \dots$ are mutually independent by the construction of a product measure. This can be generalized to hold for more general sequences, as long as certain consistency conditions hold for the finite-dimensional joint laws. The relevant theorem is called Kolmogorov Extension Theorem. However, we will restrict ourselves to sequences of independent random variables, and thus will not go any deeper into this.
%\end{rem}


\subsubsection{Convergence in law}
 
The other important notion of convergence is that of 'convergence in law'. This is a convergence statement about just the laws of random variables and thus applies also to sequences of random variables defined on different probability spaces. Geometrically you think of it as the convergence of either cumulative distribution functions or maybe even more graphically of histograms. 

For example, you could think of the following situation - your aim of life is to learn to toss a perfect random coin. In the beginning, you don't throw strong enough and there is a bias for the coin to do only one revolution and come on top with the side that was downwards. So you model your throw with $Ber(p)$ random variable with $p \neq 1/2$. As you practice more and more, you get better and finally your coin tosses are really nearly perfect $Ber(1/2)$ random variables. At different stages of your development your toss outcomes have different distributions, that you can model on different probability spaces. Over time these probability distributions start looking more and more like $Ber(1/2)$ in sense that their probability laws converge.

In fact, we have already seen this notion when we talked about the convergence of certain Binomial random variables to Poisson random variables, or discrete uniform random variables to discrete continuous random variables.

\begin{defn}[Convergence in law]
We say that a sequence of random variables $X_1, X_2, \dots$ converges in law (also: converges in distribution) to a random variable $X$ if $F_{X_n}(t) \to F_X(t)$ for every $t$ that is a continuity point of $F_X$, i.e. that is such that $\P(X = t) = 0$. 
\end{defn}

Notice that we don't ask $X_1, X_2, \dots$ to be defined on the same probability space! This is not necessary, as we are in any case only looking at their laws $\P_{X_i}$, that are uniquely characterized by $F_{X_i}$. 

It might be strange that we don't ask for convergence at all points $t\in \R$. The reason is the following: consider deterministic random variables $X_n$ taking value $1/n$. Then we would intuitively want to say that $X_n$ converge to the deterministic random variable $X$ that takes value $0$ almost surely. However, notice that $F_{X_i}(0) = 0$ for all $n \in \N$, but $F_X(0) = 1$. Thus if we asked for convergence for all $t$, the random variables $X_n$ would not converge to $0$...however, with the definition given above, they nicely do! Notice that if the limiting random variable is continuous, we really do ask the pointwise convergence of c.d.f. at all points.

To better understand the notion of convergence in law, it might be useful to think of an equivalent criteria. In fact there are many equivalent criteria!

\begin{prop}
Let $X_1, X_2, \dots$ be a sequence of random variables. They converge to a random variable $X$ in law if and only if for every $a < b$ with $\P(X = a) = \P(X = b) = 0$ we have that $\P(X_n \in (a,b)) \to \P(X \in (a,b))$
\end{prop}

\begin{proof}
This is not hard, but is admitted this year.
%If $(X_n)_{n \geq 1}$ converge in law to $X$ then by definition $F_{X_n}(t) \to F_X(t)$ for any continuity point $t$ of $F_X(t)$.
%In particular, if $\P(X  = a) = \P(X = b) = 0$, then the points $a, b$ are such continuity points. We can write 
%$$\P(X \in (a,b)) = F_X(b) - F_X(a) = \lim_{n \to \infty} (F_{X_n}(b) - F_{X_n}(a)).$$ 
%But now $\P(X_n \in (a,b)) = (F_{X_n}(b^-) - F_{X_n}(a))$. It suffices to now see that $\lim_{n \to \infty} F_{X_n}(b^-) = \lim_{n \to \infty} F_{X_n}(b)$. But this follows from the fact that $b$ is a continuity point as for every $\eps > 0$ we have that $$F_{X_n}(b-\eps) \leq F_{X_n}(b^-) \leq F_{X_n}(b)$$
%and if $b-\eps$ is also a continuity point, we deduce 
%$$F_X(b-\eps) \leq \liminf_{n \to \infty} F_{X_n}(b^-) \leq  \limsup_{n \to \infty} F_{X_n}(b^-)  \leq F_X(b),$$
%which letting $\eps \to 0$ gives the desired equality.

%In the other direction, we want to prove that for each $t$ with $\P(X = b) = 0$, we have that $\P(X_n < b) \to \P(X < b)$.
%Now, we know that for any $a < b$ with $\P(a = 0)$, we have $\P(X_n \in (a, b)) \to \P(X \in (a, b))$. As there are only countably many $a$ with $\P(X = a) > 0$, we can choose $a \to -\infty$ and conclude that $\P(X_n < b) \geq \P(X_n \in (a,b)) \to_{n \to \infty} \P(X \in (a,b))$. As $\P(X \in (a,b)) \to \P(X < b)$ as $a \to -\infty$, we deduce that $\liminf_{n \to \infty} \P(X_n < b) \geq \P(X < b)$. Similarly one can see that $\liminf_{n \to \infty} \P(X_n > b) \geq \P(X > b)$. 
%But now
%$$1 \geq \liminf_{n \to \infty}(\P(X_n < b)+\P(X_n > b)) \geq \liminf_{n \to \infty}\P(X < b) + \liminf_{n \to \infty}\P(X > b) \geq \P(X < b) + \P(X > b).$$
%As $\P(X < b) + \P(X > b) = 1$, we see that in fact the inequalities have to be equalities and thus we conclude.
\end{proof}

\begin{rem}
In fact the same proof gives a seemingly weaker but actually equivalent condition: we ask that for all $a < b$, it holds that $\liminf_{n \geq 1} \P(X_n \in (a,b) \geq \P(X \in (a,b)$. I leave it to you to check.
\end{rem}

%few criteria in special cases:

%\begin{prop}[A criteria for convergence in law for discrete r.v.]
%Let $X, X_1, X_2, \dots$ be discrete random variables Then if $\P(X_i = s) \to \P(X = s)$ for all $s \in S_X$, we have that $X_i$ converges in law to $X$. 
%\end{prop}

%\begin{proof}

%	As the support of any discrete random variable is countable, we can enumerate the support as $S_X = \{s_1, s_2, \dots \}$. As by definition $\P(X = s_i) > 0$ and $\sum_{i \geq 1} \P(X = s_i) = 1$, for every $\eps > 0$ we can find $k_0$ such that $\sum_{i \geq k_0+1} \P(X = s_i) < \eps.$
%	Denote $S_0 = \{s_1, \dots, s_{k_0}\}$. 
	
%	Now, for every $\eps > 0$ we can find $n_0 \in \N$, such that for all $n \geq n_0$ and for all $i \in \{1, \dots, k_0\}$ it holds that $|\P(X_n = s_i)-\P(X = s_i)| < \eps/k_0$.
%	We claim that for all such $n \geq n_0$ and all $x \in \R$, it then holds that $|F_X(x) - F_{X_n}(x)| < 4\eps$.
%	Indeed, we can write
%	$$F_X(x) = \sum_{i = 1}^{k_0} \P(X = s_i)1_{s_i \leq x} + \sum_{i \geq k_0+1} \P(X = s_i)1_{s_i \leq x}$$
%	and $$F_{X_n}(x) = \sum_{i=1}^{k_0} \P(X_n = s_i)1_{s_i \leq x} + \sum_{s \in S_{X_n}\setminus S_0}\P(X_n = s)1_{s \leq x}.$$
%	By the choice of $n \geq n_0$
%	$$|\sum_{i = 1}^{k_0} \P(X = s_i)1_{s_i \leq x} - \sum_{i=1}^{k_0} \P(X_n = s_i)1_{s_i \leq x}| < \eps.$$
%	Further, by the choice of $k_0$ 
%	$$\sum_{i \geq k_0+1} \P(X = s_i)1_{s_i \leq x} < \eps.$$
%	Finally, as $$\sum_{i=1}^{k_0} \P(X_n = s_i) > \sum_{i = 1}^{k_0} \P(X = s_i) - \eps > \sum_{i \geq 1}\P(X = s_i) - 2\eps = 1 - 2\eps,$$
%	we have $$\sum_{s \in S_{X_i}\setminus S_0}\P(X_n = s)1_{s \leq x} < 2\eps.$$
%	This implies that for $n$ large enough, for all $x \in \R$,
%	$$|F_X(x) - F_{X_n}(x)| < 4\eps$$
%	and as $\eps$ was arbitrary, we see that $F_{X_n}(x) \to F_X(x)$ for every $x \in \R$ and thus indeed $X_n$ converge to $X$ in law.
%\end{proof}

%From the example above, we see that the opposite is not necessarily true. However, the proposition covers the useful direction. For example, we can now deduce that:

%A similar proof gives a useful criteria in the case where the limiting random variable is discrete:

%\begin{exo}[A criteria for convergence in law for discrete r.v.]
%Let $X, X_1, X_2, \dots$ be discrete random variables Then if $\P(X_i = s) \to \P(X = s)$ for all $s \in S_X$, we have that $X_i$ converges in law to $X$. [Hint: start from the case when $X$ has finite support. Then observe that for any discrete random variable $X$ and any $\eps > 0$, you can find a subset $S^\eps$ of the support such that $\P(X \notin S^\eps) < \eps$.]
%\end{exo}

%\subsection{An explicit construction of the uniform measure}

%\begin{prop}[A construction of the uniform measure]
%Let $X_n$ be the uniform random variable with support $\{\frac{1}{n}, \frac{2}{n}, \dots, 1\}$. Then $X_n \to U$ in law, where $U$ is the uniform random variable on $[0,1]$. 
%\end{prop}

%In particular, this gives an explicit construction of the uniform measure on $([0,1], \F_E, \P)$ - it is the limit of the laws $\P_{X_n}$. 

%\begin{proof}
%We have that $F_U(x) = x1_{x \in [0,1]}$. On the other hand $F_{X_n}(x)$ is equal to $k/n$ whenever $x \in [k/n, (k+1)/n)$. Thus for every $x \in \R$, we have that $|F_U(x) - F_{X_n}(x)| < 1/n$ giving the claim.  
%\end{proof}

%\begin{rem}[$\star$ Non-examinable $\star$]
%So why all the fuss about the construction of the Lebesgue measure in the beginning of the course? First of all, notice that we are building on top of the results form before: we have not proved a statement of the form 'if $F_{X_n}$ converge to some $F$, then there is a probability measure with this c.d.f.' So it's not really a stand-alone proof. Still, it could be made into a stand-alone proof by proving such a result from a more functional-analysis perspective. You will probably meet something in these lines in a measure theory / functional analysis course.%, if you take such a course - basically, one can look at the set of all probability measures on $([0,1],\F_E)$ and endow it with a topology, called the weak convergence of probability measures, such that convergence in law is equivalent to convergence in this topology. This would tell you straight away that if you have probability measures that converge in law, then the limit would also be a probability measure, giving a way to construct the uniform measure on $(\Omega, \F_E)$ hands-on, without first constructing the Lebesgue measure \footnote{It does, however, not directly construct the measure on $\F_{Leb}$ that you will see in Analysis IV.}.
%\end{rem}
The most important example of convergence in law is the Central limit theorem.

\subsection{Central limit theorem}

The final result of the course is the Central Limit Theorem (CLT). 

\begin{thm}[Central Limit Theorem]
Let $X_1, X_2, \dots $ be i.i.d. random variables of finite variance $\sigma^2$ defined on the same probability space. Then $n^{-1/2}\sum_{i =1}^n (X_i-\E X_i)$ converges in law to $N(0, \sigma^2)$.
\end{thm}

This is a remarkable result, saying that if we add up independent random variables of finite variance we always end up with the same distribution - the Gaussian distribution! This is the reason why at least heuristically measurement errors in physics look like Gaussians - they are sums of small independent contributions, or why Gaussians come up when looking at distributions of say heights in a population. This phenomenon that individual properties of the random variables $X_i$ only influence the limiting law by a few parameters - the expectation, variance - is sometimes called universality. 

In the CLT both the assumption of finite variance and independence are crucial: you will see an example about moment conditions on the exercise sheet. To see that without independence CLT could fail consider for example the case of $X_1 = X_2 = \dots$. Then $n^{-1/2}\sum_{i =1}^n X_i = n^{1/2}X_1$ which certainly does not converge and has no reason to be a Gaussian. Whereas the condition of independence can be relaxed somewhat, there has to be a fair amount independence to guarantee that the effect of each $X_i$ on the sum is negligible!


We can now for example deduce very easily the following non-trivial result:

\begin{cor}
Let $X_n$ be a $Bin(n, p)$ random variable. Then $\frac{X_n-np}{\sqrt{n}}$ converges in law to a Gaussian of variance $\sigma^2 = p(1-p)$. 
\end{cor}

\begin{proof}
We can write $X_n - np = \sum_{i = 1}^n (Y_i - \E Y_i)$, where $Y_i$ are i.i.d. $Ber(p)$ random variables. Then by the CLT, we have that $\frac{X_n - np}{\sqrt{n}} = \frac{\sum_{i = 1}^n (Y_i - \E Y_i)}{\sqrt{n}}$ converges to a Gaussian of variance $\Var(Y_i) = p(1-p)$.

\end{proof}

Further if we consider $\pm 1$ valued random variables, then $\sum X_i $ is exactly equal to the number of ones obtained minus the number of minus ones obtained. The law of large numbers says that this number will be roughly $n(p-1/2)$, where $p$ is the probability of obtaining $1$; the CLT describes the fluctuations.

There are many interesting aspects in the statement:
\begin{itemize}
\item The scaling factor $1/\sqrt{n}$. This can be explained by a variance calculation. We have $$\Var(c_n\sum_{i=1}^n(X_i - \mu)) = c_n^2n\Var(X_1),$$
which forces $c_n = 1/\sqrt{n}$ if we hope to have something of $O(1)$
\item Why Gaussian? To see this we observe that if $X_1, X_2, \dots $ are independent centred Gaussians of variance $\sigma^2$, then so is $n^{-1/2}\sum_{i =1}^n X_i$! In fact Gaussians are the only random variables satisfying this property!
\end{itemize}

The proof of the CLT is not examinable this year, but in fact departs from this same idea.

We will again prove CLT under further hypothesis, in particular we assume $\E |X_i|^3 < \infty$. There are many different proofs of this theorem, all explaining different facets of the theorem. The one we follow is based on the following idea: for Gaussians the result holds by above. Now, given general variables $Y_i$, we will just try to swap them one by one for Gaussian random variables of the same mean and variance. We always make an error, but if we can control the cumulative error, then we are done. This is exactly what we will do in the non-examinable proof.

\subsubsection{Proof of CLT (non-examinable)}
The key step described above is encapsulated in the following proposition - we gave a more general statement that implies it in the class.

\begin{prop}[Lindeberg Exchange Principle]\label{prop:lindeb}
Let $X_1, X_2, \dots $ be i.i.d. zero mean unit variance random variables and with $\E |X_i|^3 < \infty$. Let further $Y$ be a standard Gaussian. Define $S_n := n^{-1/2}\sum_{i = 1}^n X_i$. Then for every $f: \R \to \R$ smooth with uniformly bounded derivatives up to third order, we have that 
$|\E f(S_n) - \E f(Y)| \to 0$ as $n \to \infty$.
\end{prop}

%\begin{rem}
%Here and elsewhere I will use that big-$O$ notation: for some limit, say $n \to \infty$ and some quantities $(a_n)_{n \geq 1}$ we say that $a_n = O(b_n)$ if $|a_n/b_n|$ stays bounded as $n \to \infty$. So in the previous statement it means that there is some $C > 0$ such that $|\E f(S_n) - \E f(Y)| \leq C \sup_{x \in \R} |f'''(x)|n^{-1/4}$. 
%\end{rem}

Before proving the proposition, let us see how to deduce CLT from this proposition. The idea is as follows: we saw already that knowing $\E g(X)$ for all continuous bounded $g$ determines the distribution of $X$. In fact, this would be also true if we only assumed it to hold for smooth $g$! Moreover, convergence in law can be also deduced from knowing the convergence of $\E g(X_n) \to \E g(X)$ for all $g$ that are smooth and bounded, and have further conditions on derivatives. The idea is similar to Proposition \ref{prop:charg} - we approximate indicator functions $1_{X < x}$ via smooth functions and thus obtain the convergence the c.d.f at all continuity points.

\begin{lemma}\label{lemma:cnvlaw}
Suppose that $X, X_1, X_2, \dots$ are random variables. If for all smooth bounded $g$ with uniformly bounded derivatives up to $3$rd order we have $\E g(X_n) \to \E g(X)$ as $n \to \infty$, then $X_n$ converge in law to $X$.
\end{lemma}

\begin{proof}
This runs by approximating $F_X(t), F_{X_n}(t)$ by $\E g_t(X), \E g_{t,n}(X)$ for well-chosen $g_t$ and $g_{t,n}$, quite similarly to what we've seen.
\end{proof}

\begin{proof}[Proof of CLT:]
Given random variables $X_i$ of variance $\sigma^2$, we have that $\widehat X_i := \frac{X_i - \E X_i}{\sigma}$ are zero mean and unit variance. Thus we can apply Proposition \ref{prop:lindeb} and Lemma \ref{lemma:cnvlaw} to deduce that $n^{-1/2}\sum_{i =1}^n \widehat X_i$ converges to a standard Gaussian. But now multiplying everything by $\sigma$ gives the CLT. 
\end{proof}

It remains to prove the proposition. 
\begin{proof}[Proof of Lindeberg Exchange Principle:]
	Let $Y$ and $Y_1, Y_2 \dots$ be i.i.d. standard Gaussians. 
	For $k \geq 1$, write
	$$S_{n,k} := \frac{\sum_{i=1}^{k-1} X_i + \sum_{i=k}^{n} Y_i}{n^{1/2}}.$$
	Notice that $S_{n,n+1} = S_n$ and $S_{n,1} = n^{-1/2}\sum_{i = 1}^n Y_i \sim N(0,1)$. Thus we can write 
	\begin{equation}\label{eq:ts}
	f(S_n) - f(Y) = \sum_{k=1}^{n} f(S_{n,k+1}) - f(S_{n,k}).
	\end{equation}
	Our aim will be to control each individual summand.
	To do this write further
	$$S^0_{n,k} := \frac{\sum_{i=1}^{k-1} X_i + \sum_{i=k+1}^{n} Y_i}{n^{1/2}},$$
	where we have omitted the $k$-th term altogether. 
	
	By third-order Taylor's approximation we can write a.s.
	$$f(S_{n,k+1}) = f(S_{n,k}^0) + \frac{X_k}{n^{1/2}}f'(S_{n,k}^0) + \frac{X_k^2}{2n}f''(S_{n,k}^0)+ \frac{X_k^3}{6n^{3/2}}f'''(x_1),$$
	with $x_1$ between $S_{n,k+1}$ and $S_{n,k}^0$ and similarly
	$$f(S_{n,k}) = f(S_{n,k}^0) + \frac{Y_k}{n^{1/2}}f'(S_{n,k}^0) + \frac{Y_k^2}{2n}f''(S_{n,k}^0)+ \frac{X_k^3}{6n^{3/2}}f'''(x_2).$$
	Taking expectations, as $X_k$ is independent of $S_{n,k}^0$, we see that 
	$$\E f(S_{n,k+1}) = \E f(S_{n,k}^0) + \E \frac{X_k}{n^{1/2}} \E (S_{n,k}^0) + \E \frac{X_k^2}{2n} \E f''(S_{n,k}^0) + \E \left(\frac{X_k^3}{6n^{3/2}} f'''(x_1)\right).$$
	Using further that $X_k$ has mean zero, unit variance and $\E |X_k|^3 < \infty$, we obtain that
	$$\E f(S_{n,k+1}) = \E f(S_{n,k}^0) + \frac{1}{2n} \E f''(S_{n,k}^0)+ E_r,$$
	with $|E_r| \leq \E \left(\frac{|X_k|^3}{6n^{3/2}}|f'''(x_1)|\right) = O(n^{-3/2})$ as by assumptions on $f$, we have that $|f'''(x)| < C$ and $\E |X_k|^3 < \infty$.
	Similarly, as also $Y_k$ is independent of $S_{n,k}^0$, we obtain that 
	$$\E f(S_{n,k}) =  \E f(S_{n,k}^0) + \frac{1}{2n} \E f''(S_{n,k}^0)+ \widehat E_r,$$
	with $|\widehat E_r| = O(n^{-3/2})$.
  Thus	$|\E f(S_{n,k+1}) - \E f(S_{n,k})| = O(n^{-3/2})$. By the triangle inequality we obtain 
  $$|\E \left(f(S_n) - f(Y)\right)| \leq \sum_{k=1}^n |\E f(S_{n,k+1}) - \E f(S_{n,k})| = O(n^{-1/2})$$ and the proposition follows.
\end{proof}

I wish there was more...but that's all!
\end{document}
%But think for example of the following. Assume that we have $X_1, X_2, \dots$ be a sequence of random variables on the same probability space, each with law $Geo(1/2)$ but such that we know nothing about the dependence structure. What can we say about the maximum of $n$ first random variables? 

Using Borel-Cantelli I, we can easily get some nice information: 

\begin{exo} Assume that we have $X_1, X_2, \dots$ be a sequence of random variables on the same probability space, each with law $Geo(1/2)$. Let $E_n = \{\max_{i =1}^n X_i > \sqrt{n}\}$. Show that almost surely only finitely many of $E_1, E_2, \dots $ happen, i.e. $\P(\bigcap_{n \geq 1}\bigcup_{i \geq n} E_i) = 0$. Deduce that there exists some random variable $C: \Omega \to \R$ that takes a.s. non-negative values and such that $\P(\max_{i = 1}^n X_i(\omega) < C(\omega)\sqrt{n}) = 1$.
\end{exo}

This is partly complemented by the second Borel-Cantelli lemma, which gives a condition for infinitely many events to happen. Notice that here we again ask for independent events.

\begin{lemma}[Borel-Cantelli II]
Let $E_1, E_2, \dots$ be a sequence of independent events on a common probability space $(\Omega, \F, \P)$. Suppose that $\sum_{n \geq 1}\P(E_n) = \infty$. Then almost surely infinitely many events $E_i$ happen, i.e. $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 1$.
\end{lemma}

\begin{proof}
We have that 
$$\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 1 - \P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c)$$
and hence it suffices to show that $\P(\bigcup_{m \geq 1}\bigcap_{n \geq m}E_n^c) = 0$.
By the union bound
$$\P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c) \leq \sum_{m \geq 1} \P(\bigcap_{n \geq m} E_n^c).$$
Further, as $E_i$ are independent, so are $E_i^c$, and hence
$$\P(\bigcap_{n \geq m} E_n^c) = \Pi_{n \geq m} \P(E_n^c) = \Pi_{n \geq m}(1-\P(E_n)).$$
Now using the inequality $1-x \leq e^{-x}$ for $x \in [0,1]$, we can bound the RHS further by $\exp(-\sum_{n\geq m} \P(E_n))$. 
But the sum in the exponential equals $\infty$ by the assumption. Thus $\P(\bigcap_{n \geq m} E_n^c) = 0$, hence $\P(\bigcup_{m \geq 1}\bigcap_{n \geq m} E_n^c)  = 0$ and we conclude.

\end{proof}

As already exemplified by the proof, the criteria of independence is indeed necessary:

\begin{exo}
Find events $E_1, E_2, \dots$ on the same probability space such that $\sum_{n \geq 1}\P(E_n) = \infty$, but $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) = 0$. Also, find events $E_1, E_2, \dots$ such that $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n)$ happens with probability $p \in (0,1)$.
\end{exo} 

These lemmas look very innocent, but actually have nice applications (we will see some later). First, a simple corollary says that independent events either happen infinitely often with probability $1$ or $0$ - this is quite remarkable, as a priori one might think that it could happen with any probability, like in the exercise above. So we see how the 'simple-looking' assumption of independence can really sway things:

\begin{cor}
Let $E_1, E_2, \dots$ be mutually independent events on a common probability space. Then $\P(\bigcap_{m \geq 1}\bigcup_{n \geq m} E_n) \in \{0,1\}$, i.e. $E_i$ happens infinitely often either with probability $0$ or $1$.
\end{cor}
\begin{proof}
This follows directly from the Borel-Cantelli lemmas, as either $\sum_{n \geq 1}\P(E_n) < \infty$ or $\sum_{n \geq 1}\P(E_n) = \infty$.
\end{proof}

In fact, this is a special case of a more general Kolmogorov 0-1 law, that we only meet in the non-examinable section this year. \\

Things are similar, but a bit more exciting when we switch from events to sequences of random variables $X_1, X_2, \dots$. Again, firstly the question is what we can even ask about an infinite sequence of random variables - not all functionals might be measurable!

For example some questions that we might be interested in are:
\begin{itemize}
    \item Is same value $k$ attained by the sequence of random variables?
    \item Are all but finitely many of $X_i$ positive?
    \item Is the sequence of random variables bounded in absolute value?
    \item Does it converge?
\end{itemize}

For the first one measurability is clear, as we can write it as the union $\bigcup_{n \geq 1} \{X_i = k\}$, similarly for the second one. For the third one, already some thought might be required: the event that the sequence of random variables is bounded in absolute value by $M \in \N$ is given by $E_M := \bigcap_{n \geq 1} \{|X_i| \leq M\}$. But we want to allow different bounds for different sequences. So we have to take also a union over $M$ to get $\bigcup_{M \in \N} E_M$, which again shows that the question makes fully sense. 

%As before, sequences of random variables can be seen equences of random variables $X_1, X_2, \dots$. It is more general than the sequence of events, as we could always replace events $E_i$ with their indicator functions $1_{E_i}$ and have a sequence of random variables with exactly the same information. It is also richer, as we soon see.

%\begin{exo}
%Let $E_1, E_2, \dots$ be a sequence of events on $(\Omega, \F, \P)$. Express the events $E_i$ happens eventually, $E_i$ happens infinitely often and $E_i$ for all but finitely many $i$ in terms of random variables $1_{E_i}$. Restate the conditions and conclusions of Borel-Cantelli lemmas using indicator functions and expectations. 
%\end{exo}

%Recall that given some collection of events $G \subseteq \mathcal F$ on $(\Omega, \F, \P)$, we could consider the smallest $\sigma$-algebra $\sigma(G)$ containing these events. Also, given a random variable, i.e. a measurable function $X: (\Omega, \F) \to (\R, \F_E)$ we could talk of the $\sigma$-algebra generated by $X$. This $\sigma$-algebra was denoted by $\sigma(X)$, is given by $\{X^{-1}(F): F \in \F_E\}$ and generated, for example, by all open intervals. Intuitively $\sigma(X) \subseteq \F$ singles out the events of $\F$ that contain information about $X$. Similarly, if we have a sequence of random variables $X_1, X_2, \dots$ one can talk about say $\sigma(X_n, X_{n+1}, \dots)$. One can define this as the smallest $\sigma$-algebra making all $X_i$ with $i \geq n$ measurable, i.e. containing all events in $\F$ that have information about $X_n, X_{n+1}, \dots$; it is explicitly given by, for example $\sigma(\bigcup_{i \geq n}\sigma(X_n))$. Finally, recall that the intersection of any $\sigma$-algebras is again a $\sigma$-algebra.


%\subsubsection{Kolmogorov zero-one law}
%We are now ready to state Kolmogorov's zero-one law. Let me first state, and then we decipher it.

%\begin{prop}[Kolmogorov's zero-one law]
%Let $X_1, X_2, \dots$ be independent random variables. Consider the tail $\sigma$-algebra $\F_\infty$ given by $\bigcap_{n \geq 1}\sigma(X_n, X_{n+1}, \dots)$. Then for any event $E \in \F_\infty$ we have that $\P(E) \in \{0,1\}$.
%\end{prop}

%This proposition says that in case of independent random variables, any event whose occurrence does not depend on any first $n$ random variables, has to be deterministic! 

%\begin{proof}
%We will prove that any event $E \in \F_\infty$ is independent of itself. This suffices, as recall that such independence gives $\P(E)^2 = \P(E)$, which implies $\P(E) \in \{0,1\}$. 


%So consider $E \in \F_\infty = \bigcap_{n \geq 1}\sigma(X_n, X_{n+1}, \dots)$. Then for every $n \in \N$, we have that $E \in \sigma(X_n, X_{n+1}, \dots)$. But the variables $X_1, X_2, \dots$ are mutually independent, and thus in particular this means that
%any event $E \in \sigma(X_n, X_{n+1}, \dots)$ is independent of $(X_1, \dots, X_{n-1})$ in the sense that $1_E$ is independent of $(X_1, \dots, X_{n-1})$ (why?). As this holds for every $n$, we see that in fact $E$ is independent of $X_1, X_2, \dots$ and hence of any event in $\sigma(X_1, X_2, \dots)$. But $E$ itself belongs to $\sigma(X_1, X_2, \dots)$! 

%Thus we deduce that $E$ is independent of itself and conclude.
%\end{proof}

%Having seen this relatively light proof, you might wonder whether there are any interesting tail events at all. In fact, there are plenty!
%Consider the sequence of random variables $X_1, X_2, \dots$ defined on $(\Omega, \F, \P)$. Then one can define $S_n = \sum_{i \geq 1}^n X_i$. As before, we think of $S_n$ as of a (possibly dependent) random walk that at step $i$ moves by $X_i$ units. It is natural to ask about large time behaviour of this walk and a natural first question would be just the convergence - does $S_n$ converge absolutely, and with which probability? 

%In asking such a question one should first be very careful - can one even ask this question? In other words, is the set $E_c = \{\omega: S_n(\omega) \text{ converges absbolutely}\}$ measurable, i.e. in $\F$? To check this, one goes back to definitions. First, a sum $\sum_{i \geq 1} x_i$ converges absolutely if and only if for each $\eps > 0$ there is some $n_\eps$ such that $\sum_{i \geq n_\eps}|x_i| < \eps$, which is equivalent to knowing that for all $m \geq n_0$, we have that $\sum_{i = n_\eps}^m |x_i| < \eps$. Now, we have getting closer to seeing measurability: first, for $m \geq n$, the event $$F_{n,m,\eps} = \{\sum_{i = n}^m |X_i| < \eps\} \in \F$$ and hence also $$F_{n,\eps}:=\bigcap_{m \geq n} F_{n,m,\eps} = \{\sum_{i \geq n} |X_i| < \eps\} \in \F.$$ But the set $E_c$ describing all $\omega$ such that $\sum_{i \geq 1} X_i(\omega)$ converges absolutely can be formulated as $$\bigcap_{k \geq 1} \bigcup_{n \geq 1} F_{n, k^{-1}} \in \F.$$

%Thus we see that indeed at least we can ask the question.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%5 LET US SHOW THIS PROBABILITY IS ZERO OR ONE?
%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{rem}[$\star$ non-examinable $\star$] Let us look at the special case of $X_1, X_2, \dots$ mutually independent. Notice that in fact, whether a sum converges or not, does not depend on its first $n$ terms for any $n$. In fact, the event $E_c$ that $\sum_{i \geq 1} X_i$ converges is for any $n \in \N$ the same as $\sum_{i \geq n} X_i$ converges absolutely. So we see that for every $n$ in fact $E_c$ depends only on the random variables $X_n, X_{n+1}, \dots$. But this implies that the probability of $E_c$ is either $0$ or $1$! Indeed, the event $E_c$ is determined by $(X_i)_{i \geq 1}$, meaning that $E \in \sigma(X_1, X_2, \dots)$. On the other hand it is also independent of $X_1, X_2, \dots$. But then the event $E$ is independent of itself and hence it's probability is either $0$ or $1$  This is a special case of the Kolmogorov $0-1$ law, and you will find this special case in the example sheet in the non-examinable section.
%\end{rem}

%One can ask several very similar questions:

%%%%%%%%% MAYBE STILL ON THE SHEET
%\begin{exo}
%Let $X_1, X_2, \dots $ be independent $\{1,-1\}$ valued random variables defined on the same probability space $(\Omega, \F, \P)$. Consider the sums $S_n = \sum_{i = 1}^n X_i$.
%Prove that the set $E_r = \{\omega: \text{for some }i\in \Z \text{, }S_n(\omega) = i \text{ for infinitely many }n\}$ is an event on $(\Omega, \F, %\P)$. Show that it's probability is either $0$ or $1$. 
%What if we instead consider $E_0 = \{S_n = 0 \text{ for infinitely many }n\}$?
%\end{exo}

\end{document}

\noindent \textbf{Geometric random variable}\\
A random variable that takes values in the set $\N$, each value $k$ with probability $p (1-p)^{k-1}$ for some $0 < p \leq 1$ is called a geometric random variable of parameter $p$. We denote the law of a geometric random variable by $Geo(p)$. One should again check that this even defines a random variable, by seeing that the probabilities do sum to one.

A geometric random variable describes the following situation: we have independent events $E_1, E_2, \dots $ each of success probability $p$ and we are asking for the smallest index $k$ such that the event $E_k$ happens. For example, $Geo(1/2)$ describes the number of tosses needed to get a first heads. This will be made precise on the exercise sheet.

There is also a nice property that characterizes the geometric r.v.:

\begin{lemma}[Geometric r.v. is the only memoryless random variable]
We say that a random variable $X$ with values in $\N$ is memoryless if for every $k, l \in \N$ we have that $\P_X(X > k+l| X > k) = \P_X(X>l)$. Every geometric random variable is memoryless, and in fact these are the only examples of memoryless random variables on $\N$.
\end{lemma}

\begin{proof}
Let us start by proving that the geometric random variable satisfies the memoryless property. First, notice that if $\P(X = 1) = 1$, then $X$ is a degenerate geometric random variable with $p = 1$. So we can suppose that we work in the case $\P(X > 1) > 0$.

Let us check that a geometric r.v. is memoryless. First, it is easy to check that for a geometric random variable $X$, we have that $\P(X > l) = (1-p)^l$ for some $p \in (0,1]$. As by the definition of conditional probability $$\P(X > k+l|X > k) = \frac{\P(X > k+l)}{\P(X > k)},$$ 
it follows that $\P(X > k+l|X > k) = (1-p)^{k+l-k} = (1-p)^l = \P(X > l)$ as desired.

Now, let us show that each random variable satisfying the memoryless property has the law of a geometric random variable. Again if $\P(1) = 1$, we are done. Otherwise we can write
$$\P(X > 1+ l| X > 1)\P(X > 1) = \P(X > 1+l).$$
As for a memoryless random variable $\P(X > l) = \P(X > 1+ l| X > 1)$, we obtain
$$\P(X > l)\P(X>1) = \P(X > l+1).$$
Thus inductively $\P(X > l) = \P(X > 1)^l$ and hence $X$ is a geometric random variable of parameter $p = 1 - \P(X > 1)$.
\end{proof}~\\

\noindent \textbf{Poisson random variable}\\

Poisson was a French mathematician who has famously said that the life is good for only two things - mathematics and teaching mathematics. His random variables come up quite often. 

The Poisson random variable is a discrete random variable with values in $\{0\} \cup \N$ and taking the value $k$ with probability $$e^{-\lambda}\frac{\lambda^k}{k!}$$
for some $\lambda > 0$. We denote this distribution by $Poi(\lambda)$. Poisson random variables describe occurrences of rare events over some time period, where events happening in any two consecutive time periods are independent. For example, it has been used to model
\begin{itemize}
	\item The number of visitors at a small off-road museum.
	\item More widely, the number of stars in a unit of the space.
	\item Or more darkly, it was used to also model the number of soldiers killed by horse kicks in the Prussian army.
\end{itemize}

One way we see the Poisson r.v. appearing is via a limit of the Binomial distribution if the success probability $p$ scales like $1/n$:

\begin{lemma}[Poisson random variable as the limit of Binomials]
Consider the Binomial distribution $Bin(n,\lambda/n)$. Prove that as $n \to \infty$ it converges to $Poi(\lambda)$ in the sense that for every $k\in \{0\} \cup \N$, we have that $$\P(Bin(n,\lambda/n) = k) \to e^{-\lambda}\frac{\lambda^k}{k!}.$$
\end{lemma}

\begin{proof}
By definition, for any fixed $n \in \N$ and $k\in \{0\} \cup \N$, we have
$$\P(Bin(n,\lambda/n) = k) = {n \choose k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}.$$
Using
$${n \choose k} = \frac{n!}{(n-k)!k!} = \frac{n(n-1)\cdots (n-k+1)}{k!}.$$
we can write
$$\P(Bin(n,\lambda/n) = k) = \frac{\lambda^k}{k!}\left(1-\frac{\lambda}{n}\right)^n \frac{n(n-1)\cdots (n-k+1)}{n^k}\left(1-\frac{\lambda}{n}\right)^{-k}.$$
But now as $n \to \infty$
$$\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda}.$$
Moreover, for any fixed $t > 0$ also $\frac{n-t}{n} \to 1$ as $n \to \infty$ and hence
$$\frac{n(n-1)\cdots (n-k+1)}{n^k} \to 1$$
and
$$\left(1-\frac{\lambda}{n}\right)^{-k} = \left(\frac{n-\lambda}{n}\right)^{-k}\to 1,$$
proving the lemma.\end{proof}

To connect this to the occurrences of rare events described before, one could think as follows. Suppose we try to model the number of arrivals over time window $[0,1]$, say one year in a distant location. We then cut a time-window $[0,1]$ into $n$ equal time-segments of length $1/n$ with $n$ large, say into 365 days, so that we can suppose that at each time-segment, say each day, there is at most one arrival. In this case we can describe the arrival or non-arrival using $Ber(p)$ or $1_E$ for some event $E$. If we further suppose that all days are alike, we can take this parameter $p$ to be the same for all time-segments of the same length, e.g. for all days. Moreover, if we suppose that an arrival in one time-segment does not influence arrivals in other time-intervals, we can assume that all events $E$ corresponding to different time intervals are mutually independent. Hence the total number of arrivals is the number of independent events happening, when the event probability is $p$ - we saw above that this gives a $Bin(n,p)$ random variable. But now, if you check carefully the proof above, you see that if $p$ is not of the form $\lambda/n$ for some $\lambda > 0$, then in fact the number of events will either go to infinity or go to zero - i.e. to have a non-trivial random variable in the limit $n \to \infty$, we are forced to set $p = \lambda/n$.

Poisson random variables also behave very well under taking independent copies. In particular, the related Poisson point processes is a very interesting random process:

\begin{exo}[Poisson random variables]
	Let $X_1 \sim Poi(\lambda_1)$ and $X_2 \sim Poi(\lambda_2)$ be two independent random variables defined on the same probability space. 
	\begin{itemize}
		\item Prove that then $X_1 + X_2$ is also a Poisson random variable with parameter $\lambda_1 + \lambda_2$. 
		\item Let now $Y_1, Y_2, \dots$ be independent $Ber(p)$ random variables defined on the same probability space. Prove that $X := \sum_{i = 1}^{X_1} Y_i$ also has the law of $Poi(p\lambda)$ and $X_1 - X$ has the law of $Poi((1-p)\lambda)$ and is independent of $X$.
	\end{itemize}
	Now, we consider what is called a Poisson point process on $\N$: This is a collection of i.i.d. random variables $(X_i)_{i \in \N}$ where each $X_i  \sim Poi(\lambda)$. For example you can think that some Newtonian apples fall on each integer. What is the law of the total number of apples on a finite set $S \subseteq \N$? Now colour every apple independently red with probability $p$ and green with probability $1-p$ - i.e. every apple is ripe with probability $p$. Prove that restricting to only ripe / green apples also gives a Poisson point process on $\N$ and that moreover these processes are independent.
	
	Finally, let $i_1$ be the first index of $\N$, which contains at least one apples, let $i_2$ be the second index that contains at least one apple etc. What is the distribution of the vector $(i_1, i_2-i_1, i_3-i_2, \dots)$?
\end{exo}
%Finally, taking the limit is a natural thing to do, as cutting it to exactly $n$ pieces is quite arbitrary, and if the assumptions above hold for $n$, they coul also hold for smaller time-segments. 
%But now why did we choose exactly to cut time into $n$ pieces? Maybe it is reasonable to expect that you could cut into arbitrarily small time intervals and the number of arrivals still behaves independently on each interval, and the probability of an arrival scales linearly with time-length. This would correspond to taking the limit $n \to \infty$ in the description, and hence by the previous lemma we see that the Poisson distribution $Poi(\lambda)$ describes the number of events that occurs in the whole time-interval $[0,1]$. 
\end{document}

%% PUT THE CASE FROM UNIFORM MEASURE IN NON-EXAMINABLE!

\begin{eg}
Let us calculate the c.d.f of the so called Bernoulli random variable $X$ that takes value $1$ with probability $p$ and $0$ with probability $1-p$. Notice that all indicator functions of events correspond to such random variables with $\P(E) = p$. 

We have $F_X(x) = (1-p)1_{x \geq 0} +p1_{x \geq 1}$. More generally for a random variable that takes only finite number of values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$, we have
$F_X(x) =\sum_{i = 1 \dots n}p_1 1_{x \geq x_i}$. (Why?)
\end{eg}
Thus we see that $F_X$ encodes the behaviour of $X$ rather naturally. Let us now look at this relation between the cumulative distribution function $F_X$ and the random variable $X$ more closely. By $F(x^-)$ we denote the limit of $F(x_n)$ with $(x_n)_{n \geq 1} \to x$ from below, i.e. by numbers $x_n < x$.

\begin{lemma}[C.d.f vs r.v.]\label{lem:cumvsrv}
	Let $X$ be a random variable on some probability space $(\P, \Omega, \F)$ and $F_X $ its cumulative distribution function. Then for any $x < y \in \R$
	\begin{enumerate}
		\item $\P(X < x) = F(x-)$ 
		\item $\P(X > x) = 1 - F(x)$ 
		\item $\P(X \in (x,y)) = F(y-) - F(x)$.
		\item $\P(X = x) = F(x) - F(x-)$.
	\end{enumerate}
\end{lemma}

\begin{proof}
This is on exercise sheet.
%First from $F(X) = \P(X \leq x)$ it follows directly that 
%$$1-F(X) = 1 - \P(X \leq x) = \P(X > x).$$ 
%Now, write $\{X < x\} = \cup_{n \geq 1}\{X < x - 1/n\}$. Then by Proposition \ref{prop:propmeas}, we conclude that
%$$F(x-) = \lim_{n \geq 1}F(x-1/n) = \lim_{n \geq 1} \P(X \leq x-1/n) = \P(X < x).$$
%To finish the first part, observe that by additivity of $\P$ under disjoint events
%$$\P(X \in (x,y)) + \P(X \leq x) = \P(X < y)$$
%and thus $\P(X \in (x,y)) = F(y-) - F(x).$
%For the second part, notice similarly that by additivity under disjoint events $$\P(X = x) + \P(X < x) = \P(X \geq x),$$
%from which it again follows that $\P(X = x) = F(x) - F(x-)$.
\end{proof}

Thus we see that all jumps of $F_X$ correspond to points where $\P_X(X = x) > 0$. But how many jumps are there?

\begin{lemma}
A cumulative distribution function $F_X$ of a random variable $X$ has at most countably many jumps. 
\end{lemma}

\begin{proof}
Let $S_n$ be the set of jumps that are larger than $1/n$ and $\widehat S_n$ any finite subset of $S_n$. 
Then $\widehat S_n$ is measurable and $1 \geq \P(X \in S_n) \geq |\widehat S_n|n^{-1}$. Thus it follows that $|\widehat S_n| \leq n$.
As this holds for any finite subset of $S_n$, we deduce that $|S_n| \leq n$ and in particular $S_n$ is finite.

Now the set of all jumps can be written as a union $\bigcup_{n \geq 1} S_n$. Hence as each $S_n$ is finite and a countable union of finite sets is countable, we conclude.
\end{proof}

These jumps of a c.d.f. $F_X$ are sometimes called atoms of the law of $X$. More precisely, we call $s \in \R$ an atom for the law of $X$ if and only if $\P(X = s) > 0$.

In the extreme case $F_X$ increases only via jumps, i.e. is piece-wise constant changing value at most countable times.
%%%% BRING OUT OF THE TEXT
Precisely:
\begin{defn}[Piece-wise constant with at most countable jumps]
We say that $f: \R \to [0, \infty)$ is piece-wise constant with countably many jumps iff there is some countable set $S$ and some real numbers $c_s > 0$ for $s \in S$ such that $\sum_{s \in S} c_s < \infty$ and
$$ f(x) = \sum_{s \in S} c_s1_{x \geq s}.$$
\end{defn}
In the other extreme $F_X$ could also be everywhere continuous. This differentiates two categories of random variables:

\begin{defn}[Discrete vs continuous random variables]
A random variable is called discrete if its c.d.f. $F_X$ is piece-wise constant changing value at most countable many times. It is called continuous if its c.d.f. $F_X$ is continuous.
\end{defn}

\end{document}
Let's see that these two extreme correspond to discrete and continuous random variables defined before:

\begin{exo}
Prove that a random variable $X$ is discrete if and only if $F_X$ is piece-wise constant changing value at most countable many times. Moreover, prove that $X$ is a continuous random variable if and only if $F_X$ is continuous.
\end{exo}

%\begin{proof}[Proof of Lemma \ref{lemdisc}]
%First, suppose that $F_X$ is piece-wise constant with countably many jumps. By definition, it means that there are a countable $S$ and $c_s > 0$ for $s \in S$ such that we can write
%$$F_X(x) = \sum_{s \in S} c_s1_{x \geq s}.$$
%Notice that $c_s = F_X(s) - F_X(s-)$ and thus as $F_X(\infty) = 1$ \footnote{Here and elsewhere $F_X(\infty) = \lim_{x \to \infty} F_X(x)$}, we have that $1 = \sum_{s \in S} F(s) -F(s-)$. 
%Thus
%$$\P(X \in S) = \sum_{s \in S} \P(X = s) = \sum_{i \geq 1}F(s) -F(s-) = 1,$$
%and the claim follows.

%On the other hand, suppose that there is some countable $\widetilde S$ such that $\P(X \in \widetilde S) = 1$. Then the set $ S \subseteq \widetilde S$ such that for every $s \in S$ also $\P(X = s) > 0$ is countable as a subset of a countable set. Moreover, it satisfies $\P(X \in S) = 1$ as definition of $S$, we have $\P(X \in \widetilde S \backslash S) = 0$. 

%Define $\widetilde F_X(x) = \sum_{s \in S} \P(X = s) 1_{x \geq s}$. We see that $\widetilde F_X$ is a piecewise constant function changing value only finitely many times. Moreover, also $\P(X \geq x) = \sum_{s \in S} \P(X = s) 1_{x \geq s}$ and thus $F_X(x) = \widetilde F_X(x)$ and the claim follows.%
% We now claim that $\widetilde F_X$ is the c.d.f. of $X$: indeed, we have that $\P(X \geq x) = \sum_{s \in S}

%Then for any $(a,b) \in S^c$ we have that $F(b-) - F(a) = \P(X \in (a,b)) \leq \P(X \in S^c) = 0$ and thus $F$ is constant on any such interval.
%\end{proof}

%\begin{lemma}[Discrete random variables]
%Consider a random variable $X$. Then its cumulative distribution function $F_X$ is piece-wise constant if and only if there is a countable set $S \subseteq \R$ with $\P(X \in S) = 1$.
%\end{lemma}

As the following proposition says, the c.d.f. of any random variable can be written as a convex combination of c.d.f-s of a discrete and continuous random variable. 

\begin{prop}
Any cumulative distribution function $F$ can be written uniquely as convex combination of a continuous c.d.f $F_c$ and a piece-wise constant c.d.f. with countably many jumps $F_j$ i.e. for some $a \in [0,1]$ we have that  $F = a F_{j} + (1-a)F_{c}$.
\end{prop}

In the exercise sheet you will see how to interpret as saying that each random variable can be written as a random sum of a continuous and discrete random variable.

\begin{proof}
	If $F$ is either continuous or piece-wise constant with countably many jumps, the existence of such writing is clear. So suppose that $F$ is neither. Write $S$ for the countable set of jumps of $F$. Define 
	
$$\widehat F_{j}(x) = \sum_{s \in S}1_{x \geq s}(F(s) - F(s-)),$$
which is piece-wise continuous with countably many jumps.

We claim that $\widehat F_{c} := F - \widehat F_{j}$ is continuous. Indeed, by definition both $F$ and $\widehat F_{j}$ both right-continuous, and thus is also their difference. Moreover, both are continuous at any continuity point $x$ of $F$, i.e. when $x \notin S$ as by definition then $F(x) = F(x^-)$ and one can check the same for $F_j$. Finally, when $s \in S$, then again by definition of $\widehat F_{j}$, we have that $$F(s) - F(s-) = 1_{s \geq s}(F(s) - F(s-)) = \widehat F_{j}(s) - \widehat F_{j}(s-)$$
and thus $\widehat F_{c}$ is continuous at such $s$ too.

Now, as $F$ is neither continuous nor piece-wise constant increasing with jumps, we have that $0 < \widehat F_{j}(\infty) < 1$ and $0 < \widehat F_{c}(\infty) < 1$. Hence, we can define 
$$F_{j}(x) := \frac{\widehat F_{j}(x)}{\widehat F_{j}(\infty)}$$
and
$$F_{c}(x) := \frac{\widehat F_{c}(x)}{\widehat F_{c}(\infty)}.$$
By definition both of those are non-decreasing, right-continuous satisfying the correct limits at $\pm \infty$ and hence are c.d.f-s for random variables. As $F_{j}$ increases only via jumps and $F_{c}$ is continuous, we have the desired writing with $a =  \widehat F_{j}(\infty)$ and $1- a = \widehat F_{c}(\infty)$.

Uniqueness is left as an exercise.
%To see the uniqueness of the decomposition, suppose that one can write
%$$F_X = a F_{Y_1} + (1-a)F_{Y_2} = b F_{Z_1} + (1-b)F_{Z_2},$$
%where both $Y_1$ and $Z_1$ are discrete and $Y_2, Z_2$ continuous random variables. Then $a F_{Y_1}  - b F_{Z_1}$ has to be continuous, but also piecewise constant with countably many jumps. As $a F_{Y_1}(-\infty)  - b F_{Z_1}(-\infty) = 0$, the only possibility is that it is constantly zero. As $F_{Y_1}(\infty) = 1 = F_{Z_1}(\infty)$, it follows that $a = b$ and $F_{Y_1} = F_{Z_1}$. Thus also $F_{Y_2} = F_{Z_2}$ and the proposition follows. 

\end{proof}

\subsection{Discrete random variables}

There are several families of laws of discrete random variables that come up again and again. As we will see, sometimes these laws also have very nice mathematical characterizations. 

Recall that to characterise the law of a random variable, we can either give the value of $\P_X(F)$ for a sufficiently large set of $F$ (e.g. all intervals) or give the c.d.f. For a discrete random variable it suffices to just determine the support $S$ and determine $\P_X(X = s)$ for each $s \in S$ (why?). \\



\noindent \textbf{Bernoulli random variable}\\
As mentioned already, a random variable that takes only values $\{0,1\}$, taking value $1$ with probability $p$ is called a Bernoulli random variable of parameter $p$. It is named after the Swiss mathematician Bernoulli, who also thought that all sciences need mathematics, but mathematics doesn't need any. Leaving you to judge, let us see that these examples come up very often.

Namely, on every probability space $(\Omega, \F, \P)$, every indicator function of an event, i.e. $1_E$ gives rise to a Bernoulli random variable and the parameter $p$ is equal to the probability of the event. Indeed for any event $E$ in a probability space $(\Omega, \F, \P)$ the indicator function $1_E: (\Omega, \F) \to (\R, \F)$ is measurable and hence  a random variable. Moreover, it is $\{0,1\}$ valued by definition and $\P(\{1_E = 1\}) = \P(E) = p$. 

Sometimes one talks about Bernoulli random variables more generally whenever there are two different outcomes, e.g. also when the values are $\{-1,1\}$. We then call it the Bernoulli random variable with values $\{-1,1\}$.\\

\noindent \textbf{Uniform random variable}\\
Any random variable that takes values in a finite set $S = \{x_1, \dots, x_n\}$, each with equal probability $1/n$ is called the uniform random variable on $S$. We call the law of this random variable the uniform law. Its c.d.f is given by simply $F_X(x) = n^{-1}\sum_{i = 1}^n 1_{x \geq x_i}$.

Examples are - a fair dice, the outcome of roulette, taking the card from the top of a well-mixed pack of cards etc...Concretely, for a trivial example is that if we model a fair dice on $\Omega = \{1,2,3,4,5,6\}$, $\F = \Po(\Omega)$ and $\P({i}) = 1/6$, then the random variable $X(\omega) := \omega \in \R$ gives rise to a uniform random variable. 

We use this family of random variables every time we have no a priori reason to prefer one outcome over the other. A fancy mathematical way of saying this would be to say that the uniform law is the only probability law on a finite set that is invariant under permutations of this set. We will also see on the example sheet that this is the so called maximum entropy probability distribution with values in a finite set $S$.\\

\noindent \textbf{Binomial random variable}\\
A random variable that takes values in the set $\{0,1,\dots,n\}$, and takes each value $k$ with probability $$p^k(1-p)^{n-k}{n \choose k}$$ is called a binomial random variable of parameters $n \in \N$ and $0 \leq p \leq 1$ (why do the probabilities sum to one?). We denote the law of such a binomial random variable by $Bin(n,p)$. 

Notice that for $n = 1$, we have the Bernoulli random variable. Bernoulli random variable comes up naturally in models of independent coin tosses, random graphs, or models of random walks. The reason why it comes up so often is that it always describes the following situation - we have a sequence of independent indistinguishable events and we count the number of those who occur. Or in other words, the Binomial random variable $Bin(n,p)$ can be seen as a sum of $n$ independent $Ber(p)$ random variables. 

\begin{exo}[Binomial r.v. is the number of occurring events]
Suppose we have $n$ mutually independent events $E_1, \dots, E_k$ of probability $p$ on some probability space $(\Omega, \F, \P)$. Consider the random number of events that occurs: $X = \sum_{i = 1}^n 1_{E_i}$. Prove that $X$ is a random variable and has the law $Bin(n,p)$.
\end{exo}

For a concrete lively example, let's go back to the Erdos-Renyi random graph on $n$ vertices, where each edge is independently included with probability $p$. We can then fix some vertex $v$ and consider the random variable $M_v$ giving the number of vertices adjacent to $v$, i.e. linked to $v$ by an edge. The exercise above shows that this random variable has law $Bin(n-1,p)$.\\

%\begin{proof}
%	Notice that $X \in \{0, \dots, n\}$ and for every $k = 0 \dots n$, $\{X = k\}$ can be written as $$\{X = k\} = \bigcup_{I \subseteq \{1, \dots, n\}, |I| = k}\cap_{i \in I}E_i\cap_{i \notin I}E_i^c.$$ Hence, $X^{-1}((-\infty,x])$ is measurable for any $x$ and $X$ is a random variable.
	
%	But now we can write 
%	$$\P(X =k) = \P(\bigcup_{I \subseteq \{1, \dots, n\}, |I| = k}\cap_{i \in I}E_i\cap_{i \notin I}E_i^c).$$
%	Observe that all the events in the union are disjoint, and thus
%	$$\P(X =k) = \sum_{I \subseteq \{1, \dots, n\},  |I| = k}\P(\cap_{i \in I}E_i\cap_{i \notin I}E_i^c).$$
%	As there are exactly $\{n \choose k\}$ subsets of size $k$, and events $E_i$ are mutually independent, we deduce
%	$$\P(X = k) = {n \choose k}\Pi_{i \in I}\P(E_i)\Pi_{i \notin I}\P(E_i^c).$$
%	Plugging now in the fact that for all $E_i$ we have that $\P(E_i) = p$, the result follows.
%\end{proof}


\noindent \textbf{Geometric random variable}\\
A random variable that takes values in the set $\N$, each value $k$ with probability $p (1-p)^{k-1}$ for some $0 < p \leq 1$ is called a geometric random variable of parameter $p$. We denote the law of a geometric random variable by $Geo(p)$. One should again check that this even defines a random variable, by seeing that the probabilities do sum to one.

A geometric random variable describes the following situation: we have independent events $E_1, E_2, \dots $ each of success probability $p$ and we are asking for the smallest index $k$ such that the event $E_k$ happens. For example, $Geo(1/2)$ describes the number of tosses needed to get a first heads. This will be made precise on the exercise sheet.

There is also a nice property that characterizes the geometric r.v.:

\begin{lemma}[Geometric r.v. is the only memoryless random variable]
We say that a random variable $X$ with values in $\N$ is memoryless if for every $k, l \in \N$ we have that $\P_X(X > k+l| X > k) = \P_X(X>l)$. Every geometric random variable is memoryless, and in fact these are the only examples of memoryless random variables on $\N$.
\end{lemma}

\begin{proof}
Let us start by proving that the geometric random variable satisfies the memoryless property. First, notice that if $\P(X = 1) = 1$, then $X$ is a degenerate geometric random variable with $p = 1$. So we can suppose that we work in the case $\P(X > 1) > 0$.

Let us check that a geometric r.v. is memoryless. First, it is easy to check that for a geometric random variable $X$, we have that $\P(X > l) = (1-p)^l$ for some $p \in (0,1]$. As by the definition of conditional probability $$\P(X > k+l|X > k) = \frac{\P(X > k+l)}{\P(X > k)},$$ 
it follows that $\P(X > k+l|X > k) = (1-p)^{k+l-k} = (1-p)^l = \P(X > l)$ as desired.

Now, let us show that each random variable satisfying the memoryless property has the law of a geometric random variable. Again if $\P(1) = 1$, we are done. Otherwise we can write
$$\P(X > 1+ l| X > 1)\P(X > 1) = \P(X > 1+l).$$
As for a memoryless random variable $\P(X > l) = \P(X > 1+ l| X > 1)$, we obtain
$$\P(X > l)\P(X>1) = \P(X > l+1).$$
Thus inductively $\P(X > l) = \P(X > 1)^l$ and hence $X$ is a geometric random variable of parameter $p = 1 - \P(X > 1)$.
\end{proof}~\\

\noindent \textbf{Poisson random variable}\\

Poisson was a French mathematician who has famously said that the life is good for only two things - mathematics and teaching mathematics. His random variables come up quite often. 

The Poisson random variable is a discrete random variable with values in $\{0\} \cup \N$ and taking the value $k$ with probability $$e^{-\lambda}\frac{\lambda^k}{k!}$$
for some $\lambda > 0$. We denote this distribution by $Poi(\lambda)$. Poisson random variables describe occurrences of rare events over some time period, where events happening in any two consecutive time periods are independent. For example, it has been used to model
\begin{itemize}
	\item The number of visitors at a small off-road museum.
	\item More widely, the number of stars in a unit of the space.
	\item Or more darkly, it was used to also model the number of soldiers killed by horse kicks in the Prussian army.
\end{itemize}

One way we see the Poisson r.v. appearing is via a limit of the Binomial distribution if the success probability $p$ scales like $1/n$:

\begin{lemma}[Poisson random variable as the limit of Binomials]
Consider the Binomial distribution $Bin(n,\lambda/n)$. Prove that as $n \to \infty$ it converges to $Poi(\lambda)$ in the sense that for every $k\in \{0\} \cup \N$, we have that $$\P(Bin(n,\lambda/n) = k) \to e^{-\lambda}\frac{\lambda^k}{k!}.$$
\end{lemma}

\begin{proof}
By definition, for any fixed $n \in \N$ and $k\in \{0\} \cup \N$, we have
$$\P(Bin(n,\lambda/n) = k) = {n \choose k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}.$$
Using
$${n \choose k} = \frac{n!}{(n-k)!k!} = \frac{n(n-1)\cdots (n-k+1)}{k!}.$$
we can write
$$\P(Bin(n,\lambda/n) = k) = \frac{\lambda^k}{k!}\left(1-\frac{\lambda}{n}\right)^n \frac{n(n-1)\cdots (n-k+1)}{n^k}\left(1-\frac{\lambda}{n}\right)^{-k}.$$
But now as $n \to \infty$
$$\left(1-\frac{\lambda}{n}\right)^n \to e^{-\lambda}.$$
Moreover, for any fixed $t > 0$ also $\frac{n-t}{n} \to 1$ as $n \to \infty$ and hence
$$\frac{n(n-1)\cdots (n-k+1)}{n^k} \to 1$$
and
$$\left(1-\frac{\lambda}{n}\right)^{-k} = \left(\frac{n-\lambda}{n}\right)^{-k}\to 1,$$
proving the lemma.\end{proof}

To connect this to the occurrences of rare events described before, one could think as follows. Suppose we try to model the number of arrivals over time window $[0,1]$, say one year in a distant location. We then cut a time-window $[0,1]$ into $n$ equal time-segments of length $1/n$ with $n$ large, say into 365 days, so that we can suppose that at each time-segment, say each day, there is at most one arrival. In this case we can describe the arrival or non-arrival using $Ber(p)$ or $1_E$ for some event $E$. If we further suppose that all days are alike, we can take this parameter $p$ to be the same for all time-segments of the same length, e.g. for all days. Moreover, if we suppose that an arrival in one time-segment does not influence arrivals in other time-intervals, we can assume that all events $E$ corresponding to different time intervals are mutually independent. Hence the total number of arrivals is the number of independent events happening, when the event probability is $p$ - we saw above that this gives a $Bin(n,p)$ random variable. But now, if you check carefully the proof above, you see that if $p$ is not of the form $\lambda/n$ for some $\lambda > 0$, then in fact the number of events will either go to infinity or go to zero - i.e. to have a non-trivial random variable in the limit $n \to \infty$, we are forced to set $p = \lambda/n$.

Poisson random variables also behave very well under taking independent copies. In particular, the related Poisson point processes is a very interesting random process:

\begin{exo}[Poisson random variables]
	Let $X_1 \sim Poi(\lambda_1)$ and $X_2 \sim Poi(\lambda_2)$ be two independent random variables defined on the same probability space. 
	\begin{itemize}
		\item Prove that then $X_1 + X_2$ is also a Poisson random variable with parameter $\lambda_1 + \lambda_2$. 
		\item Let now $Y_1, Y_2, \dots$ be independent $Ber(p)$ random variables defined on the same probability space. Prove that $X := \sum_{i = 1}^{X_1} Y_i$ also has the law of $Poi(p\lambda)$ and $X_1 - X$ has the law of $Poi((1-p)\lambda)$ and is independent of $X$.
	\end{itemize}
	Now, we consider what is called a Poisson point process on $\N$: This is a collection of i.i.d. random variables $(X_i)_{i \in \N}$ where each $X_i  \sim Poi(\lambda)$. For example you can think that some Newtonian apples fall on each integer. What is the law of the total number of apples on a finite set $S \subseteq \N$? Now colour every apple independently red with probability $p$ and green with probability $1-p$ - i.e. every apple is ripe with probability $p$. Prove that restricting to only ripe / green apples also gives a Poisson point process on $\N$ and that moreover these processes are independent.
	
	Finally, let $i_1$ be the first index of $\N$, which contains at least one apples, let $i_2$ be the second index that contains at least one apple etc. What is the distribution of the vector $(i_1, i_2-i_1, i_3-i_2, \dots)$?
\end{exo}
%Finally, taking the limit is a natural thing to do, as cutting it to exactly $n$ pieces is quite arbitrary, and if the assumptions above hold for $n$, they coul also hold for smaller time-segments. 
%But now why did we choose exactly to cut time into $n$ pieces? Maybe it is reasonable to expect that you could cut into arbitrarily small time intervals and the number of arrivals still behaves independently on each interval, and the probability of an arrival scales linearly with time-length. This would correspond to taking the limit $n \to \infty$ in the description, and hence by the previous lemma we see that the Poisson distribution $Poi(\lambda)$ describes the number of events that occurs in the whole time-interval $[0,1]$. 


\end{document}

